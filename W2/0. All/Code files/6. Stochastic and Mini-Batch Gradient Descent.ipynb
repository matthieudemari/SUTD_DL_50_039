{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Stochastic and Mini-Batch Gradient Descent\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (17/12/2022)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Removing unecessary warnings (optional, just makes notebook outputs more readable)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock dataset, with nonlinearity\n",
    "\n",
    "As in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All helper functions\n",
    "eps = 1e-5\n",
    "min_val = -1 + eps\n",
    "max_val = 1 - eps\n",
    "def val(min_val, max_val):\n",
    "    return round(np.random.uniform(min_val, max_val), 2)\n",
    "def class_for_val(val1, val2):\n",
    "    k = np.pi\n",
    "    return int(val2 >= -1/4 + 3/4*np.sin(val1*k))\n",
    "n_points = 1000\n",
    "def create_dataset(n_points, min_val, max_val):\n",
    "    val1_list = np.array([val(min_val, max_val) for _ in range(n_points)])\n",
    "    val2_list = np.array([val(min_val, max_val) for _ in range(n_points)])\n",
    "    inputs = np.array([[v1, v2] for v1, v2 in zip(val1_list, val2_list)])\n",
    "    outputs = np.array([class_for_val(v1, v2) for v1, v2 in zip(val1_list, val2_list)]).reshape(n_points, 1)\n",
    "    return val1_list, val2_list, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(47)\n",
    "val1_list, val2_list, inputs, outputs = create_dataset(n_points, min_val, max_val)\n",
    "# Check a few entries of the dataset\n",
    "print(val1_list.shape)\n",
    "print(val2_list.shape)\n",
    "print(inputs.shape)\n",
    "print(outputs.shape)\n",
    "print(inputs[0:10, :])\n",
    "print(outputs[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected and observed in the plots below, the dataset now longer exhibits linearity. In fact the function is no longer polynomial either due to the presence of a logarithmic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# Scatter plot\n",
    "markers = {0: \"x\", 1: \"o\"}\n",
    "colors = {0: \"r\", 1: \"g\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "    \n",
    "# Display true boundary\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_true = [-1/4 + 3/4*np.sin(v1*np.pi) for v1 in x1]\n",
    "plt.plot(x1, x2_true, \"k\", label = \"True boundary\")\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Neural Net with Sigmoid Activations, with Batch Gradient Descent \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_Batch():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_y)*0.1\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        self.loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, G_list, iteration_number, alpha = 1e-5, beta1 = 0.9, beta2 = 0.999):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "    \n",
    "        # Compute error term\n",
    "        epsilon = y_pred - outputs\n",
    "        gradient_output = epsilon*y_pred*(1 - y_pred)\n",
    "        error_hidden = np.dot(gradient_output, self.W2.T)\n",
    "        gradient_hidden = error_hidden*A1*(1 - A1)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_W2 = -(1/m)*np.dot(A1.T, gradient_output)\n",
    "        grad_W1 = -(1/m)*np.dot(inputs.T, gradient_hidden)\n",
    "        grad_b2 = -(1/m)*np.sum(gradient_output, axis = 0, keepdims = True)\n",
    "        grad_b1 = -(1/m)*np.sum(gradient_hidden, axis = 0, keepdims = True)\n",
    "        \n",
    "        # Momentum and gradient decay/normalization for each parameter\n",
    "        V_W2, V_W1, V_b2, V_b1, S_W2, S_W1, S_b2, S_b1 = G_list\n",
    "        V_W2 = beta1*V_W2 + (1 - beta1)*grad_W2\n",
    "        V_W1 = beta1*V_W1 + (1 - beta1)*grad_W1\n",
    "        V_b2 = beta1*V_b2 + (1 - beta1)*grad_b2\n",
    "        V_b1 = beta1*V_b1 + (1 - beta1)*grad_b1\n",
    "        V_W2_norm = V_W2/(1 - beta1**iteration_number)\n",
    "        V_W1_norm = V_W1/(1 - beta1**iteration_number)\n",
    "        V_b2_norm = V_b2/(1 - beta1**iteration_number)\n",
    "        V_b1_norm = V_b1/(1 - beta1**iteration_number)\n",
    "        S_W2 = beta2*S_W2 + (1 - beta2)*grad_W2**2\n",
    "        S_W1 = beta2*S_W1 + (1 - beta2)*grad_W1**2\n",
    "        S_b2 = beta2*S_b2 + (1 - beta2)*grad_b2**2\n",
    "        S_b1 = beta2*S_b1 + (1 - beta2)*grad_b1**2\n",
    "        S_W2_norm = S_W2/(1 - beta2**iteration_number)\n",
    "        S_W1_norm = S_W1/(1 - beta2**iteration_number)\n",
    "        S_b2_norm = S_b2/(1 - beta2**iteration_number)\n",
    "        S_b1_norm = S_b1/(1 - beta2**iteration_number)\n",
    "        G_list = [V_W2, V_W1, V_b2, V_b1, S_W2, S_W1, S_b2, S_b1]\n",
    "        \n",
    "        # Gradient descent update rules\n",
    "        eps = 1e-6\n",
    "        self.W2 += alpha*V_W2_norm/(np.sqrt(S_W2_norm) + eps)\n",
    "        self.W1 += alpha*V_W1_norm/(np.sqrt(S_W1_norm) + eps)\n",
    "        self.b2 += alpha*V_b2_norm/(np.sqrt(S_b2_norm) + eps)\n",
    "        self.b1 += alpha*V_b1_norm/(np.sqrt(S_b1_norm) + eps)\n",
    "        \n",
    "        # Update loss\n",
    "        self.CE_loss(inputs, outputs)\n",
    "        return G_list\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1e-5, beta1 = 0.1, beta2 = 0.1, delta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Initialize G_list\n",
    "        G_list = [0*self.W2, 0*self.W1, 0*self.b2, 0*self.b1, \\\n",
    "                  0*self.W2, 0*self.W1, 0*self.b2, 0*self.b1]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            G_list = self.backward(inputs, outputs, G_list, iteration_number, alpha, beta1, beta2)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max//100) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for delta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < delta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than delta on iteration {}.\".format(iteration_number))\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network structure (random normal initialization)\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_batch = ShallowNeuralNet_Batch(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_batch.train(inputs, outputs, N_max = 100000, alpha = 1e-1, beta1 = 0.1, \\\n",
    "                               beta2 = 0.1, delta = 1e-10, display = True)\n",
    "print(shallow_neural_net_batch.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_batch.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Neural Net with Sigmoid Activations, with Stochastic Gradient Descent \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_Stochastic():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_y)*0.1\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, G_list, iteration_number, alpha = 1e-5, beta1 = 0.9, beta2 = 0.999):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "    \n",
    "        # Compute error term\n",
    "        epsilon = y_pred - outputs\n",
    "        gradient_output = epsilon*y_pred*(1 - y_pred)\n",
    "        error_hidden = np.dot(gradient_output, self.W2.T)\n",
    "        gradient_hidden = error_hidden*A1*(1 - A1)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_W2 = -(1/m)*np.dot(A1.T, gradient_output)\n",
    "        grad_W1 = -(1/m)*np.dot(inputs.T, gradient_hidden)\n",
    "        grad_b2 = -(1/m)*np.sum(gradient_output, axis = 0, keepdims = True)\n",
    "        grad_b1 = -(1/m)*np.sum(gradient_hidden, axis = 0, keepdims = True)\n",
    "        \n",
    "        # Momentum and gradient decay/normalization for each parameter\n",
    "        V_W2, V_W1, V_b2, V_b1, S_W2, S_W1, S_b2, S_b1 = G_list\n",
    "        V_W2 = beta1*V_W2 + (1 - beta1)*grad_W2\n",
    "        V_W1 = beta1*V_W1 + (1 - beta1)*grad_W1\n",
    "        V_b2 = beta1*V_b2 + (1 - beta1)*grad_b2\n",
    "        V_b1 = beta1*V_b1 + (1 - beta1)*grad_b1\n",
    "        V_W2_norm = V_W2/(1 - beta1**iteration_number)\n",
    "        V_W1_norm = V_W1/(1 - beta1**iteration_number)\n",
    "        V_b2_norm = V_b2/(1 - beta1**iteration_number)\n",
    "        V_b1_norm = V_b1/(1 - beta1**iteration_number)\n",
    "        S_W2 = beta2*S_W2 + (1 - beta2)*grad_W2**2\n",
    "        S_W1 = beta2*S_W1 + (1 - beta2)*grad_W1**2\n",
    "        S_b2 = beta2*S_b2 + (1 - beta2)*grad_b2**2\n",
    "        S_b1 = beta2*S_b1 + (1 - beta2)*grad_b1**2\n",
    "        S_W2_norm = S_W2/(1 - beta2**iteration_number)\n",
    "        S_W1_norm = S_W1/(1 - beta2**iteration_number)\n",
    "        S_b2_norm = S_b2/(1 - beta2**iteration_number)\n",
    "        S_b1_norm = S_b1/(1 - beta2**iteration_number)\n",
    "        G_list = [V_W2, V_W1, V_b2, V_b1, S_W2, S_W1, S_b2, S_b1]\n",
    "        \n",
    "        # Gradient descent update rules\n",
    "        eps = 1e-6\n",
    "        self.W2 += alpha*V_W2_norm/(np.sqrt(S_W2_norm) + eps)\n",
    "        self.W1 += alpha*V_W1_norm/(np.sqrt(S_W1_norm) + eps)\n",
    "        self.b2 += alpha*V_b2_norm/(np.sqrt(S_b2_norm) + eps)\n",
    "        self.b1 += alpha*V_b1_norm/(np.sqrt(S_b1_norm) + eps)\n",
    "        \n",
    "        # Update loss\n",
    "        loss = self.CE_loss(inputs, outputs)\n",
    "        return G_list, loss\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1e-5, beta1 = 0.1, beta2 = 0.1, delta = 1e-5, display = True):\n",
    "        # Get number of samples\n",
    "        M = inputs.shape[0]\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Initialize G_list\n",
    "        G_list = [0*self.W2, 0*self.W1, 0*self.b2, 0*self.b1, \\\n",
    "                  0*self.W2, 0*self.W1, 0*self.b2, 0*self.b1]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            for i in range(M):\n",
    "                # Select a random sample in inputs\n",
    "                indexes = np.random.randint(0, M)\n",
    "                inputs_sub = inputs[indexes, :]\n",
    "                outputs_sub = outputs[indexes]\n",
    "\n",
    "                # Backpropagate\n",
    "                G_list, loss = self.backward(inputs_sub, outputs, G_list, iteration_number, alpha, beta1, beta2)\n",
    "                \n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            \n",
    "            # Update loss attribute on all losses\n",
    "            self.loss = self.CE_loss(inputs, outputs)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max//100) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for delta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < delta):\n",
    "                if(display):\n",
    "                    message = \"Stopping early - loss evolution was less than delta on\"\n",
    "                    message += \"iteration {}.\".format(iteration_number)\n",
    "                    print(message)\n",
    "                    break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "\n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network structure (random normal initialization)\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_batch = ShallowNeuralNet_Batch(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_batch.train(inputs, outputs, N_max = 100000, alpha = 1e-1, beta1 = 0.1, \\\n",
    "                               beta2 = 0.1, delta = 1e-10, display = True)\n",
    "print(shallow_neural_net_batch.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_batch.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Neural Net with Sigmoid Activations, with Batch Gradient Descent \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_Batch():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_y)*0.1\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        self.loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, G_list, iteration_number, alpha = 1e-5, beta1 = 0.9, beta2 = 0.999):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "    \n",
    "        # Compute error term\n",
    "        epsilon = y_pred - outputs\n",
    "        gradient_output = epsilon*y_pred*(1 - y_pred)\n",
    "        error_hidden = np.dot(gradient_output, self.W2.T)\n",
    "        gradient_hidden = error_hidden*A1*(1 - A1)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_W2 = -(1/m)*np.dot(A1.T, gradient_output)\n",
    "        grad_W1 = -(1/m)*np.dot(inputs.T, gradient_hidden)\n",
    "        grad_b2 = -(1/m)*np.sum(gradient_output, axis = 0, keepdims = True)\n",
    "        grad_b1 = -(1/m)*np.sum(gradient_hidden, axis = 0, keepdims = True)\n",
    "        \n",
    "        # Momentum and gradient decay/normalization for each parameter\n",
    "        V_W2, V_W1, V_b2, V_b1, S_W2, S_W1, S_b2, S_b1 = G_list\n",
    "        V_W2 = beta1*V_W2 + (1 - beta1)*grad_W2\n",
    "        V_W1 = beta1*V_W1 + (1 - beta1)*grad_W1\n",
    "        V_b2 = beta1*V_b2 + (1 - beta1)*grad_b2\n",
    "        V_b1 = beta1*V_b1 + (1 - beta1)*grad_b1\n",
    "        V_W2_norm = V_W2/(1 - beta1**iteration_number)\n",
    "        V_W1_norm = V_W1/(1 - beta1**iteration_number)\n",
    "        V_b2_norm = V_b2/(1 - beta1**iteration_number)\n",
    "        V_b1_norm = V_b1/(1 - beta1**iteration_number)\n",
    "        S_W2 = beta2*S_W2 + (1 - beta2)*grad_W2**2\n",
    "        S_W1 = beta2*S_W1 + (1 - beta2)*grad_W1**2\n",
    "        S_b2 = beta2*S_b2 + (1 - beta2)*grad_b2**2\n",
    "        S_b1 = beta2*S_b1 + (1 - beta2)*grad_b1**2\n",
    "        S_W2_norm = S_W2/(1 - beta2**iteration_number)\n",
    "        S_W1_norm = S_W1/(1 - beta2**iteration_number)\n",
    "        S_b2_norm = S_b2/(1 - beta2**iteration_number)\n",
    "        S_b1_norm = S_b1/(1 - beta2**iteration_number)\n",
    "        G_list = [V_W2, V_W1, V_b2, V_b1, S_W2, S_W1, S_b2, S_b1]\n",
    "        \n",
    "        # Gradient descent update rules\n",
    "        eps = 1e-6\n",
    "        self.W2 += alpha*V_W2_norm/(np.sqrt(S_W2_norm) + eps)\n",
    "        self.W1 += alpha*V_W1_norm/(np.sqrt(S_W1_norm) + eps)\n",
    "        self.b2 += alpha*V_b2_norm/(np.sqrt(S_b2_norm) + eps)\n",
    "        self.b1 += alpha*V_b1_norm/(np.sqrt(S_b1_norm) + eps)\n",
    "        \n",
    "        # Update loss\n",
    "        self.CE_loss(inputs, outputs)\n",
    "        return G_list\n",
    "    \n",
    "    def train(self, N_max = 1000, alpha = 1e-5, beta1 = 0.1, beta2 = 0.1, delta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Initialize G_list\n",
    "        G_list = [0*self.W2, 0*self.W1, 0*self.b2, 0*self.b1, \\\n",
    "                  0*self.W2, 0*self.W1, 0*self.b2, 0*self.b1]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            G_list = self.backward(inputs, outputs, G_list, iteration_number, alpha, beta1, beta2)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max//100) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for delta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < delta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than delta on iteration {}.\".format(iteration_number))\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network structure (random normal initialization)\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_batch = ShallowNeuralNet_Batch(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_batch.train(N_max = 100000, alpha = 1e-1, beta1 = 0.1, beta2 = 0.1, beta = 1e-10, display = True)\n",
    "print(shallow_neural_net_batch.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_batch.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Neural Net with Sigmoid Activations, with Batch Gradient Descent \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_Batch():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_y)*0.1\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        self.loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, G_list, iteration_number, alpha = 1e-5, beta1 = 0.9, beta2 = 0.999):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "    \n",
    "        # Compute error term\n",
    "        epsilon = y_pred - outputs\n",
    "        gradient_output = epsilon*y_pred*(1 - y_pred)\n",
    "        error_hidden = np.dot(gradient_output, self.W2.T)\n",
    "        gradient_hidden = error_hidden*A1*(1 - A1)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_W2 = -(1/m)*np.dot(A1.T, gradient_output)\n",
    "        grad_W1 = -(1/m)*np.dot(inputs.T, gradient_hidden)\n",
    "        grad_b2 = -(1/m)*np.sum(gradient_output, axis = 0, keepdims = True)\n",
    "        grad_b1 = -(1/m)*np.sum(gradient_hidden, axis = 0, keepdims = True)\n",
    "        \n",
    "        # Momentum and gradient decay/normalization for each parameter\n",
    "        V_W2, V_W1, V_b2, V_b1, S_W2, S_W1, S_b2, S_b1 = G_list\n",
    "        V_W2 = beta1*V_W2 + (1 - beta1)*grad_W2\n",
    "        V_W1 = beta1*V_W1 + (1 - beta1)*grad_W1\n",
    "        V_b2 = beta1*V_b2 + (1 - beta1)*grad_b2\n",
    "        V_b1 = beta1*V_b1 + (1 - beta1)*grad_b1\n",
    "        V_W2_norm = V_W2/(1 - beta1**iteration_number)\n",
    "        V_W1_norm = V_W1/(1 - beta1**iteration_number)\n",
    "        V_b2_norm = V_b2/(1 - beta1**iteration_number)\n",
    "        V_b1_norm = V_b1/(1 - beta1**iteration_number)\n",
    "        S_W2 = beta2*S_W2 + (1 - beta2)*grad_W2**2\n",
    "        S_W1 = beta2*S_W1 + (1 - beta2)*grad_W1**2\n",
    "        S_b2 = beta2*S_b2 + (1 - beta2)*grad_b2**2\n",
    "        S_b1 = beta2*S_b1 + (1 - beta2)*grad_b1**2\n",
    "        S_W2_norm = S_W2/(1 - beta2**iteration_number)\n",
    "        S_W1_norm = S_W1/(1 - beta2**iteration_number)\n",
    "        S_b2_norm = S_b2/(1 - beta2**iteration_number)\n",
    "        S_b1_norm = S_b1/(1 - beta2**iteration_number)\n",
    "        G_list = [V_W2, V_W1, V_b2, V_b1, S_W2, S_W1, S_b2, S_b1]\n",
    "        \n",
    "        # Gradient descent update rules\n",
    "        eps = 1e-6\n",
    "        self.W2 += alpha*V_W2_norm/(np.sqrt(S_W2_norm) + eps)\n",
    "        self.W1 += alpha*V_W1_norm/(np.sqrt(S_W1_norm) + eps)\n",
    "        self.b2 += alpha*V_b2_norm/(np.sqrt(S_b2_norm) + eps)\n",
    "        self.b1 += alpha*V_b1_norm/(np.sqrt(S_b1_norm) + eps)\n",
    "        \n",
    "        # Update loss\n",
    "        self.CE_loss(inputs, outputs)\n",
    "        return G_list\n",
    "    \n",
    "    def train(self, inbputs, outputs, N_max = 1000, alpha = 1e-5, beta1 = 0.1, beta2 = 0.1, delta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Initialize G_list\n",
    "        G_list = [0*self.W2, 0*self.W1, 0*self.b2, 0*self.b1, \\\n",
    "                  0*self.W2, 0*self.W1, 0*self.b2, 0*self.b1]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            G_list = self.backward(inputs, outputs, G_list, iteration_number, alpha, beta1, beta2)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max//100) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for delta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < delta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than delta on iteration {}.\".format(iteration_number))\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network structure (random normal initialization)\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_batch = ShallowNeuralNet_Batch(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_batch.train(inputs, outputs, N_max = 100000, alpha = 1e-1, beta1 = 0.1, \\\n",
    "                               beta2 = 0.1, beta = 1e-10, display = True)\n",
    "print(shallow_neural_net_batch.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_batch.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final visualizations and discussion\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training curves\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "plt.plot(list(range(0, len(shallow_neural_net_batch.losses_list), 100)), \\\n",
    "         shallow_neural_net_batch.losses_list[::100], \"b--\", label = \"Training curve - Batch GD\")\n",
    "plt.plot(list(range(0, len(shallow_neural_net_batch.losses_list), 100)), \\\n",
    "         shallow_neural_net_batch.losses_list[::100], \"r--\", label = \"Training curve - Stochastic GD \")\n",
    "plt.plot(list(range(0, len(shallow_neural_net_batch.losses_list), 100)), \\\n",
    "         shallow_neural_net_batch.losses_list[::100], \"g--\", label = \"Training curve - Mini-batch GD\")\n",
    "plt.plot(list(range(0, len(shallow_neural_net_batch.losses_list), 100)), \\\n",
    "         shallow_neural_net_batch.losses_list[::100], \"m--\", label = \"Training curve - Mini-batch Stochastic GD\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Loss (in logarithmic scale)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc = \"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_v2(v1, model, min_val, max_val, thr = 1e-3):\n",
    "    # Dichotomic search, to be used for boundary drawing later\n",
    "    v2_min = min_val\n",
    "    v2_max = max_val\n",
    "    while((v2_max - v2_min) > thr):\n",
    "        v2_avg = (v2_max + v2_min)/2\n",
    "        out = model.forward([v1, v2_avg])\n",
    "        if(out > 0.5):\n",
    "            v2_max = v2_avg\n",
    "        else:\n",
    "            v2_min = v2_avg\n",
    "    return (v2_max + v2_min)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# Scatter plot\n",
    "markers = {0: \"x\", 1: \"o\"}\n",
    "colors = {0: \"c\", 1: \"c\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# Display true boundary\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_true = [-1/4 + 3/4*np.sin(v1*np.pi) for v1 in x1]\n",
    "plt.plot(x1, x2_true, \"k\", label = \"True boundary - used in mock dataset generation\")\n",
    "\n",
    "# Display model boundary (sigmoid model, vanilla GD)\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_model_vanilla = [find_v2(v1, shallow_neural_net_vanilla, min_val, max_val, thr = 1e-3) for v1 in x1]\n",
    "plt.plot(x1, x2_model_vanilla, \"b--\", label = \"Model boundary - sigmoid activation model \\n with Vanilla GD\")\n",
    "\n",
    "# Show\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# Scatter plot\n",
    "markers = {0: \"x\", 1: \"o\"}\n",
    "colors = {0: \"c\", 1: \"c\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# Display true boundary\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_true = [-1/4 + 3/4*np.sin(v1*np.pi) for v1 in x1]\n",
    "plt.plot(x1, x2_true, \"k\", label = \"True boundary - used in mock dataset generation\")\n",
    "\n",
    "# Display model boundary (sigmoid model, Adagrad GD)\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_model_adagrad = [find_v2(v1, shallow_neural_net_adagrad, min_val, max_val, thr = 1e-3) for v1 in x1]\n",
    "plt.plot(x1, x2_model_adagrad, \"r--\", label = \"Model boundary - sigmoid activation model \\n with Adagrad\")\n",
    "\n",
    "# Show\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# Scatter plot\n",
    "markers = {0: \"x\", 1: \"o\"}\n",
    "colors = {0: \"c\", 1: \"c\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# Display true boundary\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_true = [-1/4 + 3/4*np.sin(v1*np.pi) for v1 in x1]\n",
    "plt.plot(x1, x2_true, \"k\", label = \"True boundary - used in mock dataset generation\")\n",
    "\n",
    "# Display model boundary (sigmoid model, RMSProp GD)\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_model_rmsprop = [find_v2(v1, shallow_neural_net_rmsprop, min_val, max_val, thr = 1e-3) for v1 in x1]\n",
    "plt.plot(x1, x2_model_rmsprop, \"g--\", label = \"Model boundary - sigmoid activation model \\n with RMSProp\")\n",
    "\n",
    "# Show\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# Scatter plot\n",
    "markers = {0: \"x\", 1: \"o\"}\n",
    "colors = {0: \"c\", 1: \"c\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# Display true boundary\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_true = [-1/4 + 3/4*np.sin(v1*np.pi) for v1 in x1]\n",
    "plt.plot(x1, x2_true, \"k\", label = \"True boundary - used in mock dataset generation\")\n",
    "\n",
    "# Display model boundary (sigmoid model, Adam GD)\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_model_adam = [find_v2(v1, shallow_neural_net_adam, min_val, max_val, thr = 1e-3) for v1 in x1]\n",
    "plt.plot(x1, x2_model_adam, \"m--\", label = \"Model boundary - sigmoid activation model \\n with Adam\")\n",
    "\n",
    "# Show\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# Scatter plot\n",
    "markers = {0: \"x\", 1: \"o\"}\n",
    "colors = {0: \"c\", 1: \"c\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# Display true boundary\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_true = [-1/4 + 3/4*np.sin(v1*np.pi) for v1 in x1]\n",
    "plt.plot(x1, x2_true, \"k\", label = \"True boundary - used in mock dataset generation\")\n",
    "\n",
    "# Display model boundary (sigmoid model, vanilla GD)\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_model_vanilla = [find_v2(v1, shallow_neural_net_vanilla, min_val, max_val, thr = 1e-3) for v1 in x1]\n",
    "plt.plot(x1, x2_model_vanilla, \"b--\", label = \"Model boundary - sigmoid activation model \\n with Vanilla GD\")\n",
    "\n",
    "# Display model boundary (sigmoid model, Adagrad GD)\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_model_adagrad = [find_v2(v1, shallow_neural_net_adagrad, min_val, max_val, thr = 1e-3) for v1 in x1]\n",
    "plt.plot(x1, x2_model_adagrad, \"r--\", label = \"Model boundary - sigmoid activation model \\n with Adagrad\")\n",
    "\n",
    "# Display model boundary (sigmoid model, RMSProp GD)\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_model_rmsprop = [find_v2(v1, shallow_neural_net_rmsprop, min_val, max_val, thr = 1e-3) for v1 in x1]\n",
    "plt.plot(x1, x2_model_rmsprop, \"g--\", label = \"Model boundary - sigmoid activation model \\n with RMSProp\")\n",
    "\n",
    "# Display model boundary (sigmoid model, Adam GD)\n",
    "x1 = [v1 for v1 in np.linspace(min_val, max_val, 50)]\n",
    "x2_model_adam = [find_v2(v1, shallow_neural_net_adam, min_val, max_val, thr = 1e-3) for v1 in x1]\n",
    "plt.plot(x1, x2_model_adam, \"m--\", label = \"Model boundary - sigmoid activation model \\n with Adam\")\n",
    "\n",
    "# Show\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "In the next notebook, we will investigate some additional variations on these optimizers, along with the mini-batch optimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[i, i**2] for i in range(10)])\n",
    "print(inputs)\n",
    "outputs = np.array([[i**3] for i in range(10)])\n",
    "print(outputs)\n",
    "shuffler = np.random.permutation(len(inputs))\n",
    "print(shuffler)\n",
    "inputs_shuffled = inputs[shuffler, :]\n",
    "outputs_shuffled = outputs[shuffler, :]\n",
    "print(inputs_shuffled)\n",
    "print(outputs_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
