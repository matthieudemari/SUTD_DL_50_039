{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Symmetries in NN and how to break them - Initialization\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (17/12/2022)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Removing unecessary warnings (optional, just makes notebook outputs more readable)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blast from the past - Mock dataset generation\n",
    "\n",
    "As in Notebook 7.a., from W1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All helper functions\n",
    "min_surf = 40\n",
    "max_surf = 200\n",
    "def surface(min_surf, max_surf):\n",
    "    return round(np.random.uniform(min_surf, max_surf), 2)\n",
    "min_dist = 50\n",
    "max_dist = 1000\n",
    "def distance(min_dist, max_dist):\n",
    "    return round(np.random.uniform(min_dist, max_dist), 2)\n",
    "def price(surface, distance):\n",
    "    return round((100000 + 14373*surface + (1000 - distance)*1286)*(1 + np.random.uniform(-0.1, 0.1)))/1000000\n",
    "n_points = 100\n",
    "def create_dataset(n_points, min_surf, max_surf, min_dist, max_dist):\n",
    "    surfaces_list = np.array([surface(min_surf, max_surf) for _ in range(n_points)])\n",
    "    distances_list = np.array([distance(min_dist, max_dist) for _ in range(n_points)])\n",
    "    inputs = np.array([[s, d] for s, d in zip(surfaces_list, distances_list)])\n",
    "    outputs = np.array([price(s, d) for s, d in zip(surfaces_list, distances_list)]).reshape(n_points, 1)\n",
    "    return surfaces_list, distances_list, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n",
      "(100, 2)\n",
      "(100, 1)\n",
      "[[ 58.16 572.97]\n",
      " [195.92 809.8 ]\n",
      " [156.6  349.04]\n",
      " [ 96.23  86.82]\n",
      " [153.22 817.92]\n",
      " [167.94 806.25]\n",
      " [143.29 315.92]\n",
      " [106.34 482.67]\n",
      " [152.96 427.77]\n",
      " [ 79.46 955.76]]\n",
      "[[1.581913]\n",
      " [3.450274]\n",
      " [2.978769]\n",
      " [2.808258]\n",
      " [2.556398]\n",
      " [3.023983]\n",
      " [3.099523]\n",
      " [2.121069]\n",
      " [3.136544]\n",
      " [1.273443]]\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(47)\n",
    "surfaces_list, distances_list, inputs, outputs = create_dataset(n_points, min_surf, max_surf, min_dist, max_dist)\n",
    "# Check a few entries of the dataset\n",
    "print(surfaces_list.shape)\n",
    "print(distances_list.shape)\n",
    "print(inputs.shape)\n",
    "print(outputs.shape)\n",
    "print(inputs[0:10, :])\n",
    "print(outputs[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blast from the past - ShallowNeuralNet class\n",
    "\n",
    "As in Notebook 7.b., from W1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(n_x, n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, n_h)*0.1\n",
    "        self.W2 = np.random.randn(n_h, n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, n_y)*0.1\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(Z1_b, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        return Z2_b\n",
    "    \n",
    "    def MSE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        losses = (pred - outputs_re)**2\n",
    "        self.loss = np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, alpha = 1e-5):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        Z2 = np.matmul(Z1_b, self.W2)\n",
    "        y_pred = Z2 + self.b2\n",
    "    \n",
    "        # Compute error term\n",
    "        epsilon = y_pred - outputs\n",
    "    \n",
    "        # Compute the gradient for W2 and b2\n",
    "        dL_dW2 = (2/m)*np.matmul(Z1_b.T, epsilon)\n",
    "        dL_db2 = (2/m)*np.sum(epsilon, axis = 0, keepdims = True)\n",
    "\n",
    "        # Compute the loss derivative with respect to the first layer\n",
    "        dL_dZ1 = np.matmul(epsilon, self.W2.T)\n",
    "\n",
    "        # Compute the gradient for W1 and b1\n",
    "        dL_dW1 = (2/m)*np.matmul(inputs.T, dL_dZ1)\n",
    "        dL_db1 = (2/m)*np.sum(dL_dZ1, axis = 0, keepdims = True)\n",
    "\n",
    "        # Update the weights and biases using gradient descent\n",
    "        self.W1 -= alpha*dL_dW1\n",
    "        self.b1 -= alpha*dL_db1\n",
    "        self.W2 -= alpha*dL_dW2\n",
    "        self.b2 -= alpha*dL_db2\n",
    "        \n",
    "        # Update loss\n",
    "        self.MSE_loss(inputs, outputs)\n",
    "    \n",
    "    def train(self, N_max = 1000, alpha = 1e-5, beta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            self.backward(inputs, outputs, alpha)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max//100) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for beta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < beta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than beta.\")\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A constant initialization ShallowNeuralNet class\n",
    "\n",
    "We will reuse and amend the **ShallowNeuralNet** class from Noteook 7.b. (Week 1) to produce a **ShallowNeuralNet_ConstantInit** class.\n",
    "\n",
    "In the **__init__()** method of the class, we initialized the weights and biases matrices using some random generators. Let us now assume that the matrices are intialized as matrices filled with constant values, e.g. constant matrices filled with the same values 0.1.\n",
    "\n",
    "We will therefore change the lines:\n",
    "```\n",
    "self.W1 = np.random.randn(n_x, n_h)*0.1\n",
    "self.b1 = np.random.randn(1, n_h)*0.1\n",
    "self.W2 = np.random.randn(n_h, n_y)*0.1\n",
    "self.b2 = np.random.randn(1, n_y)*0.1\n",
    "```\n",
    "We will replace them with:\n",
    "```\n",
    "self.W1 = np.ones(shape = (n_x, n_h))*const_val\n",
    "self.b1 = np.ones(shape = (1, n_h))*const_val\n",
    "self.W2 = np.ones(shape = (n_h, n_y))*const_val\n",
    "self.b2 = np.ones(shape = (1, n_y))*const_val\n",
    "```\n",
    "Where **const_val** is a new parameter required for the class initialization.\n",
    "\n",
    "In addition, our **train()** method will have additional lines to display for the values of $ W1 $, $ b_1 $, $ W_2 $ and $ b_2 $ after each iteration:\n",
    "```\n",
    "print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "print(\"W1, b1, W2, b2: \")\n",
    "print(self.W1)\n",
    "print(self.b1)\n",
    "print(self.W2)\n",
    "print(self.b2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_ConstantInit():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y, const_val):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        # Weights and biases matrices (all initialized as constant\n",
    "        # matrices filled with 0.1 values)\n",
    "        self.W1 = np.ones(shape = (n_x, n_h))*const_val\n",
    "        self.b1 = np.ones(shape = (1, n_h))*const_val\n",
    "        self.W2 = np.ones(shape = (n_h, n_y))*const_val\n",
    "        self.b2 = np.ones(shape = (1, n_y))*const_val\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(Z1_b, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        return Z2_b\n",
    "    \n",
    "    def MSE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        losses = (pred - outputs_re)**2\n",
    "        self.loss = np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, alpha = 1e-5):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        Z2 = np.matmul(Z1_b, self.W2)\n",
    "        y_pred = Z2 + self.b2\n",
    "    \n",
    "        # Compute error term\n",
    "        epsilon = y_pred - outputs\n",
    "    \n",
    "        # Compute the gradient for W2 and b2\n",
    "        dL_dW2 = (2/m)*np.matmul(Z1_b.T, epsilon)\n",
    "        dL_db2 = (2/m)*np.sum(epsilon, axis = 0, keepdims = True)\n",
    "\n",
    "        # Compute the loss derivative with respect to the first layer\n",
    "        dL_dZ1 = np.matmul(epsilon, self.W2.T)\n",
    "\n",
    "        # Compute the gradient for W1 and b1\n",
    "        dL_dW1 = (2/m)*np.matmul(inputs.T, dL_dZ1)\n",
    "        dL_db1 = (2/m)*np.sum(dL_dZ1, axis = 0, keepdims = True)\n",
    "\n",
    "        # Update the weights and biases using gradient descent\n",
    "        self.W1 -= alpha*dL_dW1\n",
    "        self.b1 -= alpha*dL_db1\n",
    "        self.W2 -= alpha*dL_dW2\n",
    "        self.b2 -= alpha*dL_db2\n",
    "        \n",
    "        # Update loss\n",
    "        self.MSE_loss(inputs, outputs)\n",
    "    \n",
    "    def train(self, N_max = 1000, alpha = 1e-5, beta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            self.backward(inputs, outputs, alpha)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max//100) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "                print(\"W1, b1, W2, b2: \")\n",
    "                print(self.W1)\n",
    "                print(self.b1)\n",
    "                print(self.W2)\n",
    "                print(self.b2)\n",
    "            # Check for beta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < beta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than beta.\")\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What needs to be observed here, is that the parameters $ W1 $, $ b_1 $, $ W_2 $ and $ b_2 $ are getting updated by the gradient descent algorithm as before, however, the values of the elements inside of these matrices will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Loss = 618.4538740049891\n",
      "W1, b1, W2, b2: \n",
      "[[0.09938446 0.09938446 0.09938446 0.09938446]\n",
      " [0.09676805 0.09676805 0.09676805 0.09676805]]\n",
      "[[0.09999521 0.09999521 0.09999521 0.09999521]]\n",
      "[[0.09614772]\n",
      " [0.09614772]\n",
      " [0.09614772]\n",
      " [0.09614772]]\n",
      "[[0.09995206]]\n",
      "Iteration 101 - Loss = 5.230545005396098\n",
      "W1, b1, W2, b2: \n",
      "[[0.09062651 0.09062651 0.09062651 0.09062651]\n",
      " [0.04908605 0.04908605 0.04908605 0.04908605]]\n",
      "[[0.0999269 0.0999269 0.0999269 0.0999269]]\n",
      "[[0.02554049]\n",
      " [0.02554049]\n",
      " [0.02554049]\n",
      " [0.02554049]]\n",
      "[[0.09879108]]\n",
      "Iteration 201 - Loss = 1.6294758115353987\n",
      "W1, b1, W2, b2: \n",
      "[[0.09030592 0.09030592 0.09030592 0.09030592]\n",
      " [0.04599115 0.04599115 0.04599115 0.04599115]]\n",
      "[[0.0999242 0.0999242 0.0999242 0.0999242]]\n",
      "[[0.01732601]\n",
      " [0.01732601]\n",
      " [0.01732601]\n",
      " [0.01732601]]\n",
      "[[0.09866814]]\n",
      "Iteration 301 - Loss = 1.401048162989756\n",
      "W1, b1, W2, b2: \n",
      "[[0.09036768 0.09036768 0.09036768 0.09036768]\n",
      " [0.04512412 0.04512412 0.04512412 0.04512412]]\n",
      "[[0.09992449 0.09992449 0.09992449 0.09992449]]\n",
      "[[0.01524626]\n",
      " [0.01524626]\n",
      " [0.01524626]\n",
      " [0.01524626]]\n",
      "[[0.09868702]]\n",
      "Iteration 401 - Loss = 1.3787474398943562\n",
      "W1, b1, W2, b2: \n",
      "[[0.0904973  0.0904973  0.0904973  0.0904973 ]\n",
      " [0.04467982 0.04467982 0.04467982 0.04467982]]\n",
      "[[0.09992532 0.09992532 0.09992532 0.09992532]]\n",
      "[[0.01470233]\n",
      " [0.01470233]\n",
      " [0.01470233]\n",
      " [0.01470233]]\n",
      "[[0.09874258]]\n",
      "Iteration 501 - Loss = 1.3726340052732087\n",
      "W1, b1, W2, b2: \n",
      "[[0.09064391 0.09064391 0.09064391 0.09064391]\n",
      " [0.04434246 0.04434246 0.04434246 0.04434246]]\n",
      "[[0.09992629 0.09992629 0.09992629 0.09992629]]\n",
      "[[0.0145902]\n",
      " [0.0145902]\n",
      " [0.0145902]\n",
      " [0.0145902]]\n",
      "[[0.09880839]]\n",
      "Iteration 601 - Loss = 1.3678384301186657\n",
      "W1, b1, W2, b2: \n",
      "[[0.09079537 0.09079537 0.09079537 0.09079537]\n",
      " [0.04403386 0.04403386 0.04403386 0.04403386]]\n",
      "[[0.09992729 0.09992729 0.09992729 0.09992729]]\n",
      "[[0.0146042]\n",
      " [0.0146042]\n",
      " [0.0146042]\n",
      " [0.0146042]]\n",
      "[[0.09887701]]\n",
      "Iteration 701 - Loss = 1.3631031164974499\n",
      "W1, b1, W2, b2: \n",
      "[[0.09094835 0.09094835 0.09094835 0.09094835]\n",
      " [0.04373213 0.04373213 0.04373213 0.04373213]]\n",
      "[[0.0999283 0.0999283 0.0999283 0.0999283]]\n",
      "[[0.0146562]\n",
      " [0.0146562]\n",
      " [0.0146562]\n",
      " [0.0146562]]\n",
      "[[0.09894628]]\n",
      "Iteration 801 - Loss = 1.3583164025206327\n",
      "W1, b1, W2, b2: \n",
      "[[0.09110185 0.09110185 0.09110185 0.09110185]\n",
      " [0.04343083 0.04343083 0.04343083 0.04343083]]\n",
      "[[0.09992932 0.09992932 0.09992932 0.09992932]]\n",
      "[[0.01472035]\n",
      " [0.01472035]\n",
      " [0.01472035]\n",
      " [0.01472035]]\n",
      "[[0.09901558]]\n",
      "Iteration 901 - Loss = 1.3534667607141282\n",
      "W1, b1, W2, b2: \n",
      "[[0.09125558 0.09125558 0.09125558 0.09125558]\n",
      " [0.04312802 0.04312802 0.04312802 0.04312802]]\n",
      "[[0.09993034 0.09993034 0.09993034 0.09993034]]\n",
      "[[0.014789]\n",
      " [0.014789]\n",
      " [0.014789]\n",
      " [0.014789]]\n",
      "[[0.0990847]]\n",
      "Iteration 1001 - Loss = 1.3485516721070123\n",
      "W1, b1, W2, b2: \n",
      "[[0.09140945 0.09140945 0.09140945 0.09140945]\n",
      " [0.04282309 0.04282309 0.04282309 0.04282309]]\n",
      "[[0.09993136 0.09993136 0.09993136 0.09993136]]\n",
      "[[0.01485989]\n",
      " [0.01485989]\n",
      " [0.01485989]\n",
      " [0.01485989]]\n",
      "[[0.09915359]]\n",
      "Iteration 1101 - Loss = 1.3435694340545283\n",
      "W1, b1, W2, b2: \n",
      "[[0.09156345 0.09156345 0.09156345 0.09156345]\n",
      " [0.04251583 0.04251583 0.04251583 0.04251583]]\n",
      "[[0.09993238 0.09993238 0.09993238 0.09993238]]\n",
      "[[0.01493234]\n",
      " [0.01493234]\n",
      " [0.01493234]\n",
      " [0.01493234]]\n",
      "[[0.09922223]]\n",
      "Iteration 1201 - Loss = 1.3385183875586977\n",
      "W1, b1, W2, b2: \n",
      "[[0.09171755 0.09171755 0.09171755 0.09171755]\n",
      " [0.04220615 0.04220615 0.04220615 0.04220615]]\n",
      "[[0.0999334 0.0999334 0.0999334 0.0999334]]\n",
      "[[0.01500618]\n",
      " [0.01500618]\n",
      " [0.01500618]\n",
      " [0.01500618]]\n",
      "[[0.0992906]]\n",
      "Iteration 1301 - Loss = 1.333396835393547\n",
      "W1, b1, W2, b2: \n",
      "[[0.09187175 0.09187175 0.09187175 0.09187175]\n",
      " [0.041894   0.041894   0.041894   0.041894  ]]\n",
      "[[0.09993443 0.09993443 0.09993443 0.09993443]]\n",
      "[[0.01508137]\n",
      " [0.01508137]\n",
      " [0.01508137]\n",
      " [0.01508137]]\n",
      "[[0.09935871]]\n",
      "Iteration 1401 - Loss = 1.328203030009523\n",
      "W1, b1, W2, b2: \n",
      "[[0.09202607 0.09202607 0.09202607 0.09202607]\n",
      " [0.04157933 0.04157933 0.04157933 0.04157933]]\n",
      "[[0.09993545 0.09993545 0.09993545 0.09993545]]\n",
      "[[0.01515794]\n",
      " [0.01515794]\n",
      " [0.01515794]\n",
      " [0.01515794]]\n",
      "[[0.09942655]]\n",
      "Iteration 1501 - Loss = 1.3229351698778964\n",
      "W1, b1, W2, b2: \n",
      "[[0.09218048 0.09218048 0.09218048 0.09218048]\n",
      " [0.04126212 0.04126212 0.04126212 0.04126212]]\n",
      "[[0.09993648 0.09993648 0.09993648 0.09993648]]\n",
      "[[0.01523591]\n",
      " [0.01523591]\n",
      " [0.01523591]\n",
      " [0.01523591]]\n",
      "[[0.09949412]]\n",
      "Iteration 1601 - Loss = 1.3175913972114985\n",
      "W1, b1, W2, b2: \n",
      "[[0.092335   0.092335   0.092335   0.092335  ]\n",
      " [0.04094232 0.04094232 0.04094232 0.04094232]]\n",
      "[[0.09993751 0.09993751 0.09993751 0.09993751]]\n",
      "[[0.01531532]\n",
      " [0.01531532]\n",
      " [0.01531532]\n",
      " [0.01531532]]\n",
      "[[0.09956142]]\n",
      "Iteration 1701 - Loss = 1.3121697959758434\n",
      "W1, b1, W2, b2: \n",
      "[[0.09248962 0.09248962 0.09248962 0.09248962]\n",
      " [0.04061989 0.04061989 0.04061989 0.04061989]]\n",
      "[[0.09993854 0.09993854 0.09993854 0.09993854]]\n",
      "[[0.01539621]\n",
      " [0.01539621]\n",
      " [0.01539621]\n",
      " [0.01539621]]\n",
      "[[0.09962844]]\n",
      "Iteration 1801 - Loss = 1.3066683899568792\n",
      "W1, b1, W2, b2: \n",
      "[[0.09264434 0.09264434 0.09264434 0.09264434]\n",
      " [0.0402948  0.0402948  0.0402948  0.0402948 ]]\n",
      "[[0.09993957 0.09993957 0.09993957 0.09993957]]\n",
      "[[0.01547863]\n",
      " [0.01547863]\n",
      " [0.01547863]\n",
      " [0.01547863]]\n",
      "[[0.09969518]]\n",
      "Iteration 1901 - Loss = 1.3010851408238018\n",
      "W1, b1, W2, b2: \n",
      "[[0.09279917 0.09279917 0.09279917 0.09279917]\n",
      " [0.03996701 0.03996701 0.03996701 0.03996701]]\n",
      "[[0.0999406 0.0999406 0.0999406 0.0999406]]\n",
      "[[0.01556261]\n",
      " [0.01556261]\n",
      " [0.01556261]\n",
      " [0.01556261]]\n",
      "[[0.09976164]]\n",
      "Iteration 2001 - Loss = 1.2954179461712219\n",
      "W1, b1, W2, b2: \n",
      "[[0.09295409 0.09295409 0.09295409 0.09295409]\n",
      " [0.03963648 0.03963648 0.03963648 0.03963648]]\n",
      "[[0.09994163 0.09994163 0.09994163 0.09994163]]\n",
      "[[0.0156482]\n",
      " [0.0156482]\n",
      " [0.0156482]\n",
      " [0.0156482]]\n",
      "[[0.09982781]]\n",
      "Iteration 2101 - Loss = 1.2896646375394965\n",
      "W1, b1, W2, b2: \n",
      "[[0.09310911 0.09310911 0.09310911 0.09310911]\n",
      " [0.03930316 0.03930316 0.03930316 0.03930316]]\n",
      "[[0.09994267 0.09994267 0.09994267 0.09994267]]\n",
      "[[0.01573544]\n",
      " [0.01573544]\n",
      " [0.01573544]\n",
      " [0.01573544]]\n",
      "[[0.0998937]]\n",
      "Iteration 2201 - Loss = 1.2838229784173538\n",
      "W1, b1, W2, b2: \n",
      "[[0.09326424 0.09326424 0.09326424 0.09326424]\n",
      " [0.03896703 0.03896703 0.03896703 0.03896703]]\n",
      "[[0.0999437 0.0999437 0.0999437 0.0999437]]\n",
      "[[0.01582439]\n",
      " [0.01582439]\n",
      " [0.01582439]\n",
      " [0.01582439]]\n",
      "[[0.0999593]]\n",
      "Iteration 2301 - Loss = 1.2778906622335235\n",
      "W1, b1, W2, b2: \n",
      "[[0.09341946 0.09341946 0.09341946 0.09341946]\n",
      " [0.03862804 0.03862804 0.03862804 0.03862804]]\n",
      "[[0.09994474 0.09994474 0.09994474 0.09994474]]\n",
      "[[0.01591509]\n",
      " [0.01591509]\n",
      " [0.01591509]\n",
      " [0.01591509]]\n",
      "[[0.1000246]]\n",
      "Iteration 2401 - Loss = 1.2718653103459294\n",
      "W1, b1, W2, b2: \n",
      "[[0.09357477 0.09357477 0.09357477 0.09357477]\n",
      " [0.03828615 0.03828615 0.03828615 0.03828615]]\n",
      "[[0.09994578 0.09994578 0.09994578 0.09994578]]\n",
      "[[0.0160076]\n",
      " [0.0160076]\n",
      " [0.0160076]\n",
      " [0.0160076]]\n",
      "[[0.1000896]]\n",
      "Iteration 2501 - Loss = 1.2657444700386984\n",
      "W1, b1, W2, b2: \n",
      "[[0.09373018 0.09373018 0.09373018 0.09373018]\n",
      " [0.03794132 0.03794132 0.03794132 0.03794132]]\n",
      "[[0.09994681 0.09994681 0.09994681 0.09994681]]\n",
      "[[0.01610196]\n",
      " [0.01610196]\n",
      " [0.01610196]\n",
      " [0.01610196]]\n",
      "[[0.10015431]]\n",
      "Iteration 2601 - Loss = 1.2595256125390701\n",
      "W1, b1, W2, b2: \n",
      "[[0.09388569 0.09388569 0.09388569 0.09388569]\n",
      " [0.03759351 0.03759351 0.03759351 0.03759351]]\n",
      "[[0.09994785 0.09994785 0.09994785 0.09994785]]\n",
      "[[0.01619824]\n",
      " [0.01619824]\n",
      " [0.01619824]\n",
      " [0.01619824]]\n",
      "[[0.10021871]]\n",
      "Iteration 2701 - Loss = 1.2532061310682818\n",
      "W1, b1, W2, b2: \n",
      "[[0.09404129 0.09404129 0.09404129 0.09404129]\n",
      " [0.03724267 0.03724267 0.03724267 0.03724267]]\n",
      "[[0.0999489 0.0999489 0.0999489 0.0999489]]\n",
      "[[0.01629648]\n",
      " [0.01629648]\n",
      " [0.01629648]\n",
      " [0.01629648]]\n",
      "[[0.10028281]]\n",
      "Iteration 2801 - Loss = 1.2467833389428187\n",
      "W1, b1, W2, b2: \n",
      "[[0.09419698 0.09419698 0.09419698 0.09419698]\n",
      " [0.03688876 0.03688876 0.03688876 0.03688876]]\n",
      "[[0.09994994 0.09994994 0.09994994 0.09994994]]\n",
      "[[0.01639676]\n",
      " [0.01639676]\n",
      " [0.01639676]\n",
      " [0.01639676]]\n",
      "[[0.1003466]]\n",
      "Iteration 2901 - Loss = 1.2402544677449543\n",
      "W1, b1, W2, b2: \n",
      "[[0.09435276 0.09435276 0.09435276 0.09435276]\n",
      " [0.03653175 0.03653175 0.03653175 0.03653175]]\n",
      "[[0.09995098 0.09995098 0.09995098 0.09995098]]\n",
      "[[0.01649913]\n",
      " [0.01649913]\n",
      " [0.01649913]\n",
      " [0.01649913]]\n",
      "[[0.10041008]]\n",
      "Iteration 3001 - Loss = 1.2336166655844076\n",
      "W1, b1, W2, b2: \n",
      "[[0.09450863 0.09450863 0.09450863 0.09450863]\n",
      " [0.03617159 0.03617159 0.03617159 0.03617159]]\n",
      "[[0.09995203 0.09995203 0.09995203 0.09995203]]\n",
      "[[0.01660365]\n",
      " [0.01660365]\n",
      " [0.01660365]\n",
      " [0.01660365]]\n",
      "[[0.10047325]]\n",
      "Iteration 3101 - Loss = 1.2268669954762028\n",
      "W1, b1, W2, b2: \n",
      "[[0.09466458 0.09466458 0.09466458 0.09466458]\n",
      " [0.03580823 0.03580823 0.03580823 0.03580823]]\n",
      "[[0.09995308 0.09995308 0.09995308 0.09995308]]\n",
      "[[0.0167104]\n",
      " [0.0167104]\n",
      " [0.0167104]\n",
      " [0.0167104]]\n",
      "[[0.1005361]]\n",
      "Iteration 3201 - Loss = 1.2200024338634807\n",
      "W1, b1, W2, b2: \n",
      "[[0.09482062 0.09482062 0.09482062 0.09482062]\n",
      " [0.03544164 0.03544164 0.03544164 0.03544164]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09995412 0.09995412 0.09995412 0.09995412]]\n",
      "[[0.01681943]\n",
      " [0.01681943]\n",
      " [0.01681943]\n",
      " [0.01681943]]\n",
      "[[0.10059862]]\n",
      "Iteration 3301 - Loss = 1.2130198693181269\n",
      "W1, b1, W2, b2: \n",
      "[[0.09497675 0.09497675 0.09497675 0.09497675]\n",
      " [0.03507177 0.03507177 0.03507177 0.03507177]]\n",
      "[[0.09995517 0.09995517 0.09995517 0.09995517]]\n",
      "[[0.01693083]\n",
      " [0.01693083]\n",
      " [0.01693083]\n",
      " [0.01693083]]\n",
      "[[0.10066083]]\n",
      "Iteration 3401 - Loss = 1.2059161014566668\n",
      "W1, b1, W2, b2: \n",
      "[[0.09513295 0.09513295 0.09513295 0.09513295]\n",
      " [0.03469858 0.03469858 0.03469858 0.03469858]]\n",
      "[[0.09995622 0.09995622 0.09995622 0.09995622]]\n",
      "[[0.01704466]\n",
      " [0.01704466]\n",
      " [0.01704466]\n",
      " [0.01704466]]\n",
      "[[0.1007227]]\n",
      "Iteration 3501 - Loss = 1.1986878401140677\n",
      "W1, b1, W2, b2: \n",
      "[[0.09528924 0.09528924 0.09528924 0.09528924]\n",
      " [0.03432203 0.03432203 0.03432203 0.03432203]]\n",
      "[[0.09995728 0.09995728 0.09995728 0.09995728]]\n",
      "[[0.017161]\n",
      " [0.017161]\n",
      " [0.017161]\n",
      " [0.017161]]\n",
      "[[0.10078425]]\n",
      "Iteration 3601 - Loss = 1.1913317048237786\n",
      "W1, b1, W2, b2: \n",
      "[[0.0954456  0.0954456  0.0954456  0.0954456 ]\n",
      " [0.03394208 0.03394208 0.03394208 0.03394208]]\n",
      "[[0.09995833 0.09995833 0.09995833 0.09995833]]\n",
      "[[0.01727993]\n",
      " [0.01727993]\n",
      " [0.01727993]\n",
      " [0.01727993]]\n",
      "[[0.10084547]]\n",
      "Iteration 3701 - Loss = 1.183844224658793\n",
      "W1, b1, W2, b2: \n",
      "[[0.09560204 0.09560204 0.09560204 0.09560204]\n",
      " [0.03355869 0.03355869 0.03355869 0.03355869]]\n",
      "[[0.09995939 0.09995939 0.09995939 0.09995939]]\n",
      "[[0.01740153]\n",
      " [0.01740153]\n",
      " [0.01740153]\n",
      " [0.01740153]]\n",
      "[[0.10090635]]\n",
      "Iteration 3801 - Loss = 1.1762218384955587\n",
      "W1, b1, W2, b2: \n",
      "[[0.09575855 0.09575855 0.09575855 0.09575855]\n",
      " [0.03317181 0.03317181 0.03317181 0.03317181]]\n",
      "[[0.09996044 0.09996044 0.09996044 0.09996044]]\n",
      "[[0.01752588]\n",
      " [0.01752588]\n",
      " [0.01752588]\n",
      " [0.01752588]]\n",
      "[[0.10096689]]\n",
      "Iteration 3901 - Loss = 1.1684608957704512\n",
      "W1, b1, W2, b2: \n",
      "[[0.09591513 0.09591513 0.09591513 0.09591513]\n",
      " [0.03278142 0.03278142 0.03278142 0.03278142]]\n",
      "[[0.0999615 0.0999615 0.0999615 0.0999615]]\n",
      "[[0.01765308]\n",
      " [0.01765308]\n",
      " [0.01765308]\n",
      " [0.01765308]]\n",
      "[[0.10102709]]\n",
      "Iteration 4001 - Loss = 1.1605576578071666\n",
      "W1, b1, W2, b2: \n",
      "[[0.09607178 0.09607178 0.09607178 0.09607178]\n",
      " [0.03238746 0.03238746 0.03238746 0.03238746]]\n",
      "[[0.09996256 0.09996256 0.09996256 0.09996256]]\n",
      "[[0.01778321]\n",
      " [0.01778321]\n",
      " [0.01778321]\n",
      " [0.01778321]]\n",
      "[[0.10108694]]\n",
      "Iteration 4101 - Loss = 1.1525082998029412\n",
      "W1, b1, W2, b2: \n",
      "[[0.09622849 0.09622849 0.09622849 0.09622849]\n",
      " [0.03198991 0.03198991 0.03198991 0.03198991]]\n",
      "[[0.09996363 0.09996363 0.09996363 0.09996363]]\n",
      "[[0.01791637]\n",
      " [0.01791637]\n",
      " [0.01791637]\n",
      " [0.01791637]]\n",
      "[[0.10114645]]\n",
      "Iteration 4201 - Loss = 1.1443089135719116\n",
      "W1, b1, W2, b2: \n",
      "[[0.09638527 0.09638527 0.09638527 0.09638527]\n",
      " [0.03158873 0.03158873 0.03158873 0.03158873]]\n",
      "[[0.09996469 0.09996469 0.09996469 0.09996469]]\n",
      "[[0.01805265]\n",
      " [0.01805265]\n",
      " [0.01805265]\n",
      " [0.01805265]]\n",
      "[[0.10120561]]\n",
      "Iteration 4301 - Loss = 1.1359555111552988\n",
      "W1, b1, W2, b2: \n",
      "[[0.09654211 0.09654211 0.09654211 0.09654211]\n",
      " [0.03118389 0.03118389 0.03118389 0.03118389]]\n",
      "[[0.09996575 0.09996575 0.09996575 0.09996575]]\n",
      "[[0.01819216]\n",
      " [0.01819216]\n",
      " [0.01819216]\n",
      " [0.01819216]]\n",
      "[[0.10126442]]\n",
      "Iteration 4401 - Loss = 1.1274440294204515\n",
      "W1, b1, W2, b2: \n",
      "[[0.096699   0.096699   0.096699   0.096699  ]\n",
      " [0.03077535 0.03077535 0.03077535 0.03077535]]\n",
      "[[0.09996682 0.09996682 0.09996682 0.09996682]]\n",
      "[[0.01833499]\n",
      " [0.01833499]\n",
      " [0.01833499]\n",
      " [0.01833499]]\n",
      "[[0.10132287]]\n",
      "Iteration 4501 - Loss = 1.1187703357840604\n",
      "W1, b1, W2, b2: \n",
      "[[0.09685595 0.09685595 0.09685595 0.09685595]\n",
      " [0.03036309 0.03036309 0.03036309 0.03036309]]\n",
      "[[0.09996789 0.09996789 0.09996789 0.09996789]]\n",
      "[[0.01848126]\n",
      " [0.01848126]\n",
      " [0.01848126]\n",
      " [0.01848126]]\n",
      "[[0.10138096]]\n",
      "Iteration 4601 - Loss = 1.1099302352090832\n",
      "W1, b1, W2, b2: \n",
      "[[0.09701295 0.09701295 0.09701295 0.09701295]\n",
      " [0.02994708 0.02994708 0.02994708 0.02994708]]\n",
      "[[0.09996896 0.09996896 0.09996896 0.09996896]]\n",
      "[[0.01863107]\n",
      " [0.01863107]\n",
      " [0.01863107]\n",
      " [0.01863107]]\n",
      "[[0.10143869]]\n",
      "Iteration 4701 - Loss = 1.1009194786400491\n",
      "W1, b1, W2, b2: \n",
      "[[0.09716999 0.09716999 0.09716999 0.09716999]\n",
      " [0.0295273  0.0295273  0.0295273  0.0295273 ]]\n",
      "[[0.09997004 0.09997004 0.09997004 0.09997004]]\n",
      "[[0.01878454]\n",
      " [0.01878454]\n",
      " [0.01878454]\n",
      " [0.01878454]]\n",
      "[[0.10149606]]\n",
      "Iteration 4801 - Loss = 1.0917337730573167\n",
      "W1, b1, W2, b2: \n",
      "[[0.09732708 0.09732708 0.09732708 0.09732708]\n",
      " [0.02910373 0.02910373 0.02910373 0.02910373]]\n",
      "[[0.09997111 0.09997111 0.09997111 0.09997111]]\n",
      "[[0.01894179]\n",
      " [0.01894179]\n",
      " [0.01894179]\n",
      " [0.01894179]]\n",
      "[[0.10155307]]\n",
      "Iteration 4901 - Loss = 1.0823687933473791\n",
      "W1, b1, W2, b2: \n",
      "[[0.09748421 0.09748421 0.09748421 0.09748421]\n",
      " [0.02867636 0.02867636 0.02867636 0.02867636]]\n",
      "[[0.09997219 0.09997219 0.09997219 0.09997219]]\n",
      "[[0.01910293]\n",
      " [0.01910293]\n",
      " [0.01910293]\n",
      " [0.01910293]]\n",
      "[[0.1016097]]\n",
      "Iteration 5001 - Loss = 1.0728201962032846\n",
      "W1, b1, W2, b2: \n",
      "[[0.09764138 0.09764138 0.09764138 0.09764138]\n",
      " [0.02824516 0.02824516 0.02824516 0.02824516]]\n",
      "[[0.09997327 0.09997327 0.09997327 0.09997327]]\n",
      "[[0.01926809]\n",
      " [0.01926809]\n",
      " [0.01926809]\n",
      " [0.01926809]]\n",
      "[[0.10166597]]\n",
      "Iteration 5101 - Loss = 1.0630836362862883\n",
      "W1, b1, W2, b2: \n",
      "[[0.09779858 0.09779858 0.09779858 0.09779858]\n",
      " [0.02781014 0.02781014 0.02781014 0.02781014]]\n",
      "[[0.09997435 0.09997435 0.09997435 0.09997435]]\n",
      "[[0.01943739]\n",
      " [0.01943739]\n",
      " [0.01943739]\n",
      " [0.01943739]]\n",
      "[[0.10172187]]\n",
      "Iteration 5201 - Loss = 1.0531547848966438\n",
      "W1, b1, W2, b2: \n",
      "[[0.09795582 0.09795582 0.09795582 0.09795582]\n",
      " [0.0273713  0.0273713  0.0273713  0.0273713 ]]\n",
      "[[0.09997543 0.09997543 0.09997543 0.09997543]]\n",
      "[[0.01961097]\n",
      " [0.01961097]\n",
      " [0.01961097]\n",
      " [0.01961097]]\n",
      "[[0.10177739]]\n",
      "Iteration 5301 - Loss = 1.0430293514173945\n",
      "W1, b1, W2, b2: \n",
      "[[0.09811308 0.09811308 0.09811308 0.09811308]\n",
      " [0.02692864 0.02692864 0.02692864 0.02692864]]\n",
      "[[0.09997652 0.09997652 0.09997652 0.09997652]]\n",
      "[[0.01978895]\n",
      " [0.01978895]\n",
      " [0.01978895]\n",
      " [0.01978895]]\n",
      "[[0.10183254]]\n",
      "Iteration 5401 - Loss = 1.0327031078095221\n",
      "W1, b1, W2, b2: \n",
      "[[0.09827036 0.09827036 0.09827036 0.09827036]\n",
      " [0.02648217 0.02648217 0.02648217 0.02648217]]\n",
      "[[0.09997761 0.09997761 0.09997761 0.09997761]]\n",
      "[[0.01997148]\n",
      " [0.01997148]\n",
      " [0.01997148]\n",
      " [0.01997148]]\n",
      "[[0.10188732]]\n",
      "Iteration 5501 - Loss = 1.0221719164490013\n",
      "W1, b1, W2, b2: \n",
      "[[0.09842766 0.09842766 0.09842766 0.09842766]\n",
      " [0.02603191 0.02603191 0.02603191 0.02603191]]\n",
      "[[0.0999787 0.0999787 0.0999787 0.0999787]]\n",
      "[[0.02015869]\n",
      " [0.02015869]\n",
      " [0.02015869]\n",
      " [0.02015869]]\n",
      "[[0.10194172]]\n",
      "Iteration 5601 - Loss = 1.0114317616051673\n",
      "W1, b1, W2, b2: \n",
      "[[0.09858498 0.09858498 0.09858498 0.09858498]\n",
      " [0.02557789 0.02557789 0.02557789 0.02557789]]\n",
      "[[0.09997979 0.09997979 0.09997979 0.09997979]]\n",
      "[[0.02035071]\n",
      " [0.02035071]\n",
      " [0.02035071]\n",
      " [0.02035071]]\n",
      "[[0.10199574]]\n",
      "Iteration 5701 - Loss = 1.0004787848641084\n",
      "W1, b1, W2, b2: \n",
      "[[0.09874231 0.09874231 0.09874231 0.09874231]\n",
      " [0.02512015 0.02512015 0.02512015 0.02512015]]\n",
      "[[0.09998089 0.09998089 0.09998089 0.09998089]]\n",
      "[[0.0205477]\n",
      " [0.0205477]\n",
      " [0.0205477]\n",
      " [0.0205477]]\n",
      "[[0.10204939]]\n",
      "Iteration 5801 - Loss = 0.9893093247990996\n",
      "W1, b1, W2, b2: \n",
      "[[0.09889965 0.09889965 0.09889965 0.09889965]\n",
      " [0.02465873 0.02465873 0.02465873 0.02465873]]\n",
      "[[0.09998199 0.09998199 0.09998199 0.09998199]]\n",
      "[[0.0207498]\n",
      " [0.0207498]\n",
      " [0.0207498]\n",
      " [0.0207498]]\n",
      "[[0.10210266]]\n",
      "Iteration 5901 - Loss = 0.9779199611806006\n",
      "W1, b1, W2, b2: \n",
      "[[0.099057   0.099057   0.099057   0.099057  ]\n",
      " [0.02419368 0.02419368 0.02419368 0.02419368]]\n",
      "[[0.09998309 0.09998309 0.09998309 0.09998309]]\n",
      "[[0.02095715]\n",
      " [0.02095715]\n",
      " [0.02095715]\n",
      " [0.02095715]]\n",
      "[[0.10215555]]\n",
      "Iteration 6001 - Loss = 0.9663075639991904\n",
      "W1, b1, W2, b2: \n",
      "[[0.09921435 0.09921435 0.09921435 0.09921435]\n",
      " [0.02372509 0.02372509 0.02372509 0.02372509]]\n",
      "[[0.0999842 0.0999842 0.0999842 0.0999842]]\n",
      "[[0.02116989]\n",
      " [0.02116989]\n",
      " [0.02116989]\n",
      " [0.02116989]]\n",
      "[[0.10220806]]\n",
      "Iteration 6101 - Loss = 0.9544693475436083\n",
      "W1, b1, W2, b2: \n",
      "[[0.0993717  0.0993717  0.0993717  0.0993717 ]\n",
      " [0.02325302 0.02325302 0.02325302 0.02325302]]\n",
      "[[0.09998531 0.09998531 0.09998531 0.09998531]]\n",
      "[[0.02138817]\n",
      " [0.02138817]\n",
      " [0.02138817]\n",
      " [0.02138817]]\n",
      "[[0.1022602]]\n",
      "Iteration 6201 - Loss = 0.9424029297304771\n",
      "W1, b1, W2, b2: \n",
      "[[0.09952904 0.09952904 0.09952904 0.09952904]\n",
      " [0.02277757 0.02277757 0.02277757 0.02277757]]\n",
      "[[0.09998642 0.09998642 0.09998642 0.09998642]]\n",
      "[[0.02161214]\n",
      " [0.02161214]\n",
      " [0.02161214]\n",
      " [0.02161214]]\n",
      "[[0.10231197]]\n",
      "Iteration 6301 - Loss = 0.9301063968194557\n",
      "W1, b1, W2, b2: \n",
      "[[0.09968637 0.09968637 0.09968637 0.09968637]\n",
      " [0.02229884 0.02229884 0.02229884 0.02229884]]\n",
      "[[0.09998754 0.09998754 0.09998754 0.09998754]]\n",
      "[[0.02184194]\n",
      " [0.02184194]\n",
      " [0.02184194]\n",
      " [0.02184194]]\n",
      "[[0.10236336]]\n",
      "Iteration 6401 - Loss = 0.9175783735648134\n",
      "W1, b1, W2, b2: \n",
      "[[0.09984369 0.09984369 0.09984369 0.09984369]\n",
      " [0.02181698 0.02181698 0.02181698 0.02181698]]\n",
      "[[0.09998866 0.09998866 0.09998866 0.09998866]]\n",
      "[[0.0220777]\n",
      " [0.0220777]\n",
      " [0.0220777]\n",
      " [0.0220777]]\n",
      "[[0.10241438]]\n",
      "Iteration 6501 - Loss = 0.9048180987487804\n",
      "W1, b1, W2, b2: \n",
      "[[0.100001  0.100001  0.100001  0.100001 ]\n",
      " [0.0213321 0.0213321 0.0213321 0.0213321]]\n",
      "[[0.09998978 0.09998978 0.09998978 0.09998978]]\n",
      "[[0.02231956]\n",
      " [0.02231956]\n",
      " [0.02231956]\n",
      " [0.02231956]]\n",
      "[[0.10246504]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6601 - Loss = 0.8918255059108229\n",
      "W1, b1, W2, b2: \n",
      "[[0.10015829 0.10015829 0.10015829 0.10015829]\n",
      " [0.02084439 0.02084439 0.02084439 0.02084439]]\n",
      "[[0.09999091 0.09999091 0.09999091 0.09999091]]\n",
      "[[0.02256766]\n",
      " [0.02256766]\n",
      " [0.02256766]\n",
      " [0.02256766]]\n",
      "[[0.10251533]]\n",
      "Iteration 6701 - Loss = 0.8786013089278188\n",
      "W1, b1, W2, b2: \n",
      "[[0.10031556 0.10031556 0.10031556 0.10031556]\n",
      " [0.020354   0.020354   0.020354   0.020354  ]]\n",
      "[[0.09999205 0.09999205 0.09999205 0.09999205]]\n",
      "[[0.02282212]\n",
      " [0.02282212]\n",
      " [0.02282212]\n",
      " [0.02282212]]\n",
      "[[0.10256525]]\n",
      "Iteration 6801 - Loss = 0.8651470919111628\n",
      "W1, b1, W2, b2: \n",
      "[[0.10047281 0.10047281 0.10047281 0.10047281]\n",
      " [0.01986114 0.01986114 0.01986114 0.01986114]]\n",
      "[[0.09999318 0.09999318 0.09999318 0.09999318]]\n",
      "[[0.02308306]\n",
      " [0.02308306]\n",
      " [0.02308306]\n",
      " [0.02308306]]\n",
      "[[0.10261482]]\n",
      "Iteration 6901 - Loss = 0.8514654026671935\n",
      "W1, b1, W2, b2: \n",
      "[[0.10063002 0.10063002 0.10063002 0.10063002]\n",
      " [0.01936603 0.01936603 0.01936603 0.01936603]]\n",
      "[[0.09999433 0.09999433 0.09999433 0.09999433]]\n",
      "[[0.02335058]\n",
      " [0.02335058]\n",
      " [0.02335058]\n",
      " [0.02335058]]\n",
      "[[0.10266404]]\n",
      "Iteration 7001 - Loss = 0.8375598487174993\n",
      "W1, b1, W2, b2: \n",
      "[[0.10078721 0.10078721 0.10078721 0.10078721]\n",
      " [0.01886892 0.01886892 0.01886892 0.01886892]]\n",
      "[[0.09999547 0.09999547 0.09999547 0.09999547]]\n",
      "[[0.02362478]\n",
      " [0.02362478]\n",
      " [0.02362478]\n",
      " [0.02362478]]\n",
      "[[0.10271291]]\n",
      "Iteration 7101 - Loss = 0.8234351945977353\n",
      "W1, b1, W2, b2: \n",
      "[[0.10094437 0.10094437 0.10094437 0.10094437]\n",
      " [0.01837005 0.01837005 0.01837005 0.01837005]]\n",
      "[[0.09999663 0.09999663 0.09999663 0.09999663]]\n",
      "[[0.02390576]\n",
      " [0.02390576]\n",
      " [0.02390576]\n",
      " [0.02390576]]\n",
      "[[0.10276144]]\n",
      "Iteration 7201 - Loss = 0.8090974588519544\n",
      "W1, b1, W2, b2: \n",
      "[[0.10110149 0.10110149 0.10110149 0.10110149]\n",
      " [0.01786973 0.01786973 0.01786973 0.01786973]]\n",
      "[[0.09999779 0.09999779 0.09999779 0.09999779]]\n",
      "[[0.02419357]\n",
      " [0.02419357]\n",
      " [0.02419357]\n",
      " [0.02419357]]\n",
      "[[0.10280963]]\n",
      "Iteration 7301 - Loss = 0.7945540088212276\n",
      "W1, b1, W2, b2: \n",
      "[[0.10125857 0.10125857 0.10125857 0.10125857]\n",
      " [0.01736827 0.01736827 0.01736827 0.01736827]]\n",
      "[[0.09999895 0.09999895 0.09999895 0.09999895]]\n",
      "[[0.02448828]\n",
      " [0.02448828]\n",
      " [0.02448828]\n",
      " [0.02448828]]\n",
      "[[0.10285748]]\n",
      "Iteration 7401 - Loss = 0.7798136510006224\n",
      "W1, b1, W2, b2: \n",
      "[[0.1014156  0.1014156  0.1014156  0.1014156 ]\n",
      " [0.01686599 0.01686599 0.01686599 0.01686599]]\n",
      "[[0.10000012 0.10000012 0.10000012 0.10000012]]\n",
      "[[0.02478992]\n",
      " [0.02478992]\n",
      " [0.02478992]\n",
      " [0.02478992]]\n",
      "[[0.10290502]]\n",
      "Iteration 7501 - Loss = 0.7648867144209822\n",
      "W1, b1, W2, b2: \n",
      "[[0.10157259 0.10157259 0.10157259 0.10157259]\n",
      " [0.01636327 0.01636327 0.01636327 0.01636327]]\n",
      "[[0.1000013 0.1000013 0.1000013 0.1000013]]\n",
      "[[0.0250985]\n",
      " [0.0250985]\n",
      " [0.0250985]\n",
      " [0.0250985]]\n",
      "[[0.10295223]]\n",
      "Iteration 7601 - Loss = 0.7497851242186486\n",
      "W1, b1, W2, b2: \n",
      "[[0.10172952 0.10172952 0.10172952 0.10172952]\n",
      " [0.01586048 0.01586048 0.01586048 0.01586048]]\n",
      "[[0.10000248 0.10000248 0.10000248 0.10000248]]\n",
      "[[0.02541402]\n",
      " [0.02541402]\n",
      " [0.02541402]\n",
      " [0.02541402]]\n",
      "[[0.10299914]]\n",
      "Iteration 7701 - Loss = 0.7345224623076505\n",
      "W1, b1, W2, b2: \n",
      "[[0.10188639 0.10188639 0.10188639 0.10188639]\n",
      " [0.01535805 0.01535805 0.01535805 0.01535805]]\n",
      "[[0.10000367 0.10000367 0.10000367 0.10000367]]\n",
      "[[0.02573645]\n",
      " [0.02573645]\n",
      " [0.02573645]\n",
      " [0.02573645]]\n",
      "[[0.10304574]]\n",
      "Iteration 7801 - Loss = 0.7191140118886143\n",
      "W1, b1, W2, b2: \n",
      "[[0.10204319 0.10204319 0.10204319 0.10204319]\n",
      " [0.01485639 0.01485639 0.01485639 0.01485639]]\n",
      "[[0.10000487 0.10000487 0.10000487 0.10000487]]\n",
      "[[0.02606572]\n",
      " [0.02606572]\n",
      " [0.02606572]\n",
      " [0.02606572]]\n",
      "[[0.10309205]]\n",
      "Iteration 7901 - Loss = 0.7035767824420041\n",
      "W1, b1, W2, b2: \n",
      "[[0.10219991 0.10219991 0.10219991 0.10219991]\n",
      " [0.01435599 0.01435599 0.01435599 0.01435599]]\n",
      "[[0.10000608 0.10000608 0.10000608 0.10000608]]\n",
      "[[0.02640176]\n",
      " [0.02640176]\n",
      " [0.02640176]\n",
      " [0.02640176]]\n",
      "[[0.10313807]]\n",
      "Iteration 8001 - Loss = 0.687929511886348\n",
      "W1, b1, W2, b2: \n",
      "[[0.10235654 0.10235654 0.10235654 0.10235654]\n",
      " [0.0138573  0.0138573  0.0138573  0.0138573 ]]\n",
      "[[0.1000073 0.1000073 0.1000073 0.1000073]]\n",
      "[[0.02674443]\n",
      " [0.02674443]\n",
      " [0.02674443]\n",
      " [0.02674443]]\n",
      "[[0.10318381]]\n",
      "Iteration 8101 - Loss = 0.6721926427593945\n",
      "W1, b1, W2, b2: \n",
      "[[0.10251306 0.10251306 0.10251306 0.10251306]\n",
      " [0.01336082 0.01336082 0.01336082 0.01336082]]\n",
      "[[0.10000852 0.10000852 0.10000852 0.10000852]]\n",
      "[[0.02709358]\n",
      " [0.02709358]\n",
      " [0.02709358]\n",
      " [0.02709358]]\n",
      "[[0.10322927]]\n",
      "Iteration 8201 - Loss = 0.6563882696224438\n",
      "W1, b1, W2, b2: \n",
      "[[0.10266947 0.10266947 0.10266947 0.10266947]\n",
      " [0.01286708 0.01286708 0.01286708 0.01286708]]\n",
      "[[0.10000975 0.10000975 0.10000975 0.10000975]]\n",
      "[[0.02744904]\n",
      " [0.02744904]\n",
      " [0.02744904]\n",
      " [0.02744904]]\n",
      "[[0.10327448]]\n",
      "Iteration 8301 - Loss = 0.640540055409611\n",
      "W1, b1, W2, b2: \n",
      "[[0.10282574 0.10282574 0.10282574 0.10282574]\n",
      " [0.0123766  0.0123766  0.0123766  0.0123766 ]]\n",
      "[[0.10001099 0.10001099 0.10001099 0.10001099]]\n",
      "[[0.02781056]\n",
      " [0.02781056]\n",
      " [0.02781056]\n",
      " [0.02781056]]\n",
      "[[0.10331942]]\n",
      "Iteration 8401 - Loss = 0.624673115149355\n",
      "W1, b1, W2, b2: \n",
      "[[0.10298185 0.10298185 0.10298185 0.10298185]\n",
      " [0.01188993 0.01188993 0.01188993 0.01188993]]\n",
      "[[0.10001225 0.10001225 0.10001225 0.10001225]]\n",
      "[[0.02817791]\n",
      " [0.02817791]\n",
      " [0.02817791]\n",
      " [0.02817791]]\n",
      "[[0.10336412]]\n",
      "Iteration 8501 - Loss = 0.6088138663680092\n",
      "W1, b1, W2, b2: \n",
      "[[0.10313777 0.10313777 0.10313777 0.10313777]\n",
      " [0.01140759 0.01140759 0.01140759 0.01140759]]\n",
      "[[0.10001351 0.10001351 0.10001351 0.10001351]]\n",
      "[[0.02855077]\n",
      " [0.02855077]\n",
      " [0.02855077]\n",
      " [0.02855077]]\n",
      "[[0.10340857]]\n",
      "Iteration 8601 - Loss = 0.5929898465230384\n",
      "W1, b1, W2, b2: \n",
      "[[0.10329347 0.10329347 0.10329347 0.10329347]\n",
      " [0.01093016 0.01093016 0.01093016 0.01093016]]\n",
      "[[0.10001478 0.10001478 0.10001478 0.10001478]]\n",
      "[[0.02892882]\n",
      " [0.02892882]\n",
      " [0.02892882]\n",
      " [0.02892882]]\n",
      "[[0.10345278]]\n",
      "Iteration 8701 - Loss = 0.577229498971001\n",
      "W1, b1, W2, b2: \n",
      "[[0.10344892 0.10344892 0.10344892 0.10344892]\n",
      " [0.01045817 0.01045817 0.01045817 0.01045817]]\n",
      "[[0.10001606 0.10001606 0.10001606 0.10001606]]\n",
      "[[0.02931168]\n",
      " [0.02931168]\n",
      " [0.02931168]\n",
      " [0.02931168]]\n",
      "[[0.10349676]]\n",
      "Iteration 8801 - Loss = 0.5615619302005896\n",
      "W1, b1, W2, b2: \n",
      "[[0.10360408 0.10360408 0.10360408 0.10360408]\n",
      " [0.00999216 0.00999216 0.00999216 0.00999216]]\n",
      "[[0.10001735 0.10001735 0.10001735 0.10001735]]\n",
      "[[0.02969895]\n",
      " [0.02969895]\n",
      " [0.02969895]\n",
      " [0.02969895]]\n",
      "[[0.10354051]]\n",
      "Iteration 8901 - Loss = 0.5460166422909626\n",
      "W1, b1, W2, b2: \n",
      "[[0.1037589  0.1037589  0.1037589  0.1037589 ]\n",
      " [0.00953268 0.00953268 0.00953268 0.00953268]]\n",
      "[[0.10001865 0.10001865 0.10001865 0.10001865]]\n",
      "[[0.03009019]\n",
      " [0.03009019]\n",
      " [0.03009019]\n",
      " [0.03009019]]\n",
      "[[0.10358404]]\n",
      "Iteration 9001 - Loss = 0.5306232457171808\n",
      "W1, b1, W2, b2: \n",
      "[[0.10391333 0.10391333 0.10391333 0.10391333]\n",
      " [0.00908024 0.00908024 0.00908024 0.00908024]]\n",
      "[[0.10001996 0.10001996 0.10001996 0.10001996]]\n",
      "[[0.03048493]\n",
      " [0.03048493]\n",
      " [0.03048493]\n",
      " [0.03048493]]\n",
      "[[0.10362734]]\n",
      "Iteration 9101 - Loss = 0.5154111586418021\n",
      "W1, b1, W2, b2: \n",
      "[[0.10406731 0.10406731 0.10406731 0.10406731]\n",
      " [0.00863533 0.00863533 0.00863533 0.00863533]]\n",
      "[[0.10002128 0.10002128 0.10002128 0.10002128]]\n",
      "[[0.03088266]\n",
      " [0.03088266]\n",
      " [0.03088266]\n",
      " [0.03088266]]\n",
      "[[0.10367042]]\n",
      "Iteration 9201 - Loss = 0.5004092996314387\n",
      "W1, b1, W2, b2: \n",
      "[[0.10422079 0.10422079 0.10422079 0.10422079]\n",
      " [0.00819843 0.00819843 0.00819843 0.00819843]]\n",
      "[[0.10002261 0.10002261 0.10002261 0.10002261]]\n",
      "[[0.03128286]\n",
      " [0.03128286]\n",
      " [0.03128286]\n",
      " [0.03128286]]\n",
      "[[0.10371328]]\n",
      "Iteration 9301 - Loss = 0.4856457812564877\n",
      "W1, b1, W2, b2: \n",
      "[[0.10437369 0.10437369 0.10437369 0.10437369]\n",
      " [0.00776999 0.00776999 0.00776999 0.00776999]]\n",
      "[[0.10002396 0.10002396 0.10002396 0.10002396]]\n",
      "[[0.03168497]\n",
      " [0.03168497]\n",
      " [0.03168497]\n",
      " [0.03168497]]\n",
      "[[0.10375592]]\n",
      "Iteration 9401 - Loss = 0.471147612225154\n",
      "W1, b1, W2, b2: \n",
      "[[0.10452593 0.10452593 0.10452593 0.10452593]\n",
      " [0.0073504  0.0073504  0.0073504  0.0073504 ]]\n",
      "[[0.10002531 0.10002531 0.10002531 0.10002531]]\n",
      "[[0.03208844]\n",
      " [0.03208844]\n",
      " [0.03208844]\n",
      " [0.03208844]]\n",
      "[[0.10379833]]\n",
      "Iteration 9501 - Loss = 0.45694041554549464\n",
      "W1, b1, W2, b2: \n",
      "[[0.10467745 0.10467745 0.10467745 0.10467745]\n",
      " [0.00694005 0.00694005 0.00694005 0.00694005]]\n",
      "[[0.10002667 0.10002667 0.10002667 0.10002667]]\n",
      "[[0.03249267]\n",
      " [0.03249267]\n",
      " [0.03249267]\n",
      " [0.03249267]]\n",
      "[[0.10384052]]\n",
      "Iteration 9601 - Loss = 0.44304816970321714\n",
      "W1, b1, W2, b2: \n",
      "[[0.10482814 0.10482814 0.10482814 0.10482814]\n",
      " [0.00653927 0.00653927 0.00653927 0.00653927]]\n",
      "[[0.10002804 0.10002804 0.10002804 0.10002804]]\n",
      "[[0.03289707]\n",
      " [0.03289707]\n",
      " [0.03289707]\n",
      " [0.03289707]]\n",
      "[[0.10388248]]\n",
      "Iteration 9701 - Loss = 0.42949297901619415\n",
      "W1, b1, W2, b2: \n",
      "[[0.10497793 0.10497793 0.10497793 0.10497793]\n",
      " [0.00614835 0.00614835 0.00614835 0.00614835]]\n",
      "[[0.10002942 0.10002942 0.10002942 0.10002942]]\n",
      "[[0.03330105]\n",
      " [0.03330105]\n",
      " [0.03330105]\n",
      " [0.03330105]]\n",
      "[[0.1039242]]\n",
      "Iteration 9801 - Loss = 0.41629487823082845\n",
      "W1, b1, W2, b2: \n",
      "[[0.10512672 0.10512672 0.10512672 0.10512672]\n",
      " [0.00576753 0.00576753 0.00576753 0.00576753]]\n",
      "[[0.10003081 0.10003081 0.10003081 0.10003081]]\n",
      "[[0.03370401]\n",
      " [0.03370401]\n",
      " [0.03370401]\n",
      " [0.03370401]]\n",
      "[[0.10396568]]\n",
      "Iteration 9901 - Loss = 0.4034716751312595\n",
      "W1, b1, W2, b2: \n",
      "[[0.1052744  0.1052744  0.1052744  0.1052744 ]\n",
      " [0.00539702 0.00539702 0.00539702 0.00539702]]\n",
      "[[0.10003221 0.10003221 0.10003221 0.10003221]]\n",
      "[[0.03410535]\n",
      " [0.03410535]\n",
      " [0.03410535]\n",
      " [0.03410535]]\n",
      "[[0.10400691]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping - Maximal number of iterations reached.\n"
     ]
    }
   ],
   "source": [
    "# Define neural network structure\n",
    "n_x = 2\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net = ShallowNeuralNet_ConstantInit(n_x, n_h, n_y, 0.1)\n",
    "shallow_neural_net.train(N_max = 10000, alpha = 1e-6, beta = 1e-6, display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, after the first iteration of the trainer, the matrix $ W_1 $ reads as:\n",
    "```\n",
    "[[0.09938446 0.09938446 0.09938446 0.09938446]\n",
    " [0.09676805 0.09676805 0.09676805 0.09676805]]\n",
    "```\n",
    "\n",
    "This is the typical symptom of a symmetry in the Network, which is unfortunately problematic for two reasons\n",
    "\n",
    "First, symmetry can be problematic in Neural Networks because it can cause the network to learn the same features multiple times. This lack of diversity typically causes the network to be less efficient, because it is learning the same information multiple times. This is observed below elements on each line have identical values. This unfortunately leads to the vector $ Z1_b $ calculated by the first layer to have identical values as well, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One input:  [ 58.16 572.97]\n",
      "Z1_b:  [[9.119297 9.119297 9.119297 9.119297]]\n",
      "Z2_b:  [[1.36252994]]\n"
     ]
    }
   ],
   "source": [
    "# Select one input in dataset\n",
    "one_input = inputs[0,:]\n",
    "print(\"One input: \", one_input)\n",
    "# Wx + b operation for the first layer\n",
    "Z1 = np.matmul(one_input, shallow_neural_net.W1)\n",
    "Z1_b = Z1 + shallow_neural_net.b1\n",
    "print(\"Z1_b: \", Z1_b)\n",
    "# Wx + b operation for the second layer\n",
    "Z2 = np.matmul(Z1_b, shallow_neural_net.W2)\n",
    "Z2_b = Z2 + shallow_neural_net.b2\n",
    "print(\"Z2_b: \", Z2_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And second, this symmetry leads to a more difficult training and, eventually, lower generalization capabilities. This is typically observed when comparing the training curves and the final losses obtained for both the original and constant initialization models. The symmetry unfortunately leads to a higher loss after training, as observed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39116118478894846\n"
     ]
    }
   ],
   "source": [
    "# Define neural network structure (constant initialization)\n",
    "n_x = 2\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_const = ShallowNeuralNet_ConstantInit(n_x, n_h, n_y, 0.1)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_const.train(N_max = 10000, alpha = 1e-6, beta = 1e-6, display = False)\n",
    "print(shallow_neural_net_const.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19879750888125622\n"
     ]
    }
   ],
   "source": [
    "# Define neural network structure (random normal initialization)\n",
    "n_x = 2\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_rand = ShallowNeuralNet(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_rand.train(N_max = 10000, alpha = 1e-6, beta = 1e-6, display = False)\n",
    "print(shallow_neural_net_rand.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare training curves\n",
    "fig = plt.figure(figsize = (8, 5))\n",
    "plt.plot(list(range(len(shallow_neural_net_const.losses_list))), \\\n",
    "            shallow_neural_net_const.losses_list, \\\n",
    "            \"b\", label = \"Constant init model\")\n",
    "plt.plot(list(range(len(shallow_neural_net_rand.losses_list))), \\\n",
    "            shallow_neural_net_rand.losses_list, \\\n",
    "            \"r--\", label = \"Random init model\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Loss (in logarithmic scale)\")\n",
    "plt.yscale(\"log\")\n",
    "# Display\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which initialization is best then?\n",
    "\n",
    "We have observed that a constant initialization is usually not a good idea, as it does not break symmetry. For this reason, it is often preferable to have some variance in the parameters and vectors produced during the **forward()** operations?\n",
    "\n",
    "This, however, raises an important question: **which initialization method is best and should be used?**\n",
    "\n",
    "In general, four initialization methods are considered in Neural Networks.\n",
    "\n",
    "1. **The Normal Random initialization:** One simple approach is to initialize the weights of the network to random values drawn from a Gaussian distribution with a mean of 0 and a standard deviation of 0.01 or 0.1. This is a simple and effective method, but it can be sensitive to the choice of standard deviation, and was typically implemented in our original design \n",
    "\n",
    "```\n",
    "self.W1 = np.random.randn(n_x, n_h)*0.1\n",
    "self.b1 = np.random.randn(1, n_h)*0.1\n",
    "self.W2 = np.random.randn(n_h, n_y)*0.1\n",
    "self.b2 = np.random.randn(1, n_y)*0.1\n",
    "```\n",
    "\n",
    "2. **Xavier initialization:** Another popular method is Xavier initialization, which is designed to preserve the variance of the activations of the network as the weights are updated during training. This initialization technique is generally used for large networks with sigmoid or tanh activation functions and can help improve the convergence of the network. This would typically be implemented using a uniform distribution, as shown below:\n",
    "\n",
    "```\n",
    "# To ensure the variance is 1, we divide the range by the square root of the number of inputs\n",
    "init_val = np.sqrt(6.0/(n_x + n_y))\n",
    "self.W1 = np.random.uniform(-init_val, init_val, (n_x, n_h))\n",
    "self.b1 = np.random.uniform(-init_val, init_val, (1, n_h))\n",
    "self.W2 = np.random.uniform(-init_val, init_val, (n_h, n_y))\n",
    "self.b2 = np.random.uniform(-init_val, init_val, (1, n_y))\n",
    "```\n",
    "\n",
    "It can also be implemented using a normal distribution, as shown below:\n",
    "```\n",
    "# To ensure the variance is 1, we divide the standard deviation by the square root of the number of inputs\n",
    "std_dev = np.sqrt(3.0 / (input_size + output_size))\n",
    "self.W1 = np.random.normal(0, std_dev, (n_x, n_h))\n",
    "self.b1 = np.random.normal(0, std_dev, (1, n_h))\n",
    "self.W2 = np.random.normal(0, std_dev, (n_h, n_y))\n",
    "self.b2 = np.random.normal(0, std_dev, (1, n_y))\n",
    "```\n",
    "\n",
    "3. **He initialization:** Similar to Xavier initialization, He initialization is designed to preserve the variance of the activations, but it is specifically designed for large networks with ReLU activation functions. This would typically be implemented using a random normal for weights and zeroes for biases, as shown below:\n",
    "```\n",
    "range1 = np.sqrt(2/n_x)\n",
    "self.W1 = np.random.randn(n_x, n_h)*range1\n",
    "self.b1 = np.zeros((1, n_h))\n",
    "range2 = np.sqrt(2/n_h)\n",
    "self.W2 = np.random.randn(n_h, n_y)*range2\n",
    "self.b2 = np.zeros((1, n_y))\n",
    "```\n",
    "\n",
    "4. **LeCun initialization:** This initialization technique is similar to Xavier initialization, but it is specifically designed for networks with sigmoid activation functions. It can be a good choice for networks with sigmoid activations and small networks. This would typically be implemented as:\n",
    "\n",
    "```\n",
    "range1 = np.sqrt(1/n_x)\n",
    "self.W1 = np.random.randn(n_x, n_h)*range1\n",
    "self.b1 = np.zeros((1, n_h))\n",
    "range2 = np.sqrt(1/n_h)\n",
    "self.W2 = np.random.randn(n_h, n_y)*range2\n",
    "self.b2 = np.zeros((1, n_y))\n",
    "```\n",
    "\n",
    "**Important note:** these are empirical observations only. For this reason, it is a good idea to try a few different initialization techniques and see which one works best for your particular network architecture and dataset. Some recent research has also suggested that using a combination of different initialization techniques (e.g., random initialization for some layers and He initialization for others) can improve performance.\n",
    "\n",
    "We will conclude this notebook by amending the **ShallowNeuralNet** class and bring in five additional methods for each possible initialization method and leave it to the reader to play with the different types of initializations to see their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y, init_type = \"Normal\"):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_type = init_type\n",
    "        self.init_parameters()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        if(self.init_type == \"Normal\"):\n",
    "            self.init_parameters_normal()\n",
    "        elif(self.init_type == \"Xavier\"):\n",
    "            self.init_parameters_xavier()\n",
    "        elif(self.init_type == \"He\"):\n",
    "            self.init_parameters_he()\n",
    "        elif(self.init_type == \"LeCun\"):\n",
    "            self.init_parameters_lecun()\n",
    "        elif(self.init_type == \"Constant\"):\n",
    "            self.init_parameters_const()\n",
    "        else:\n",
    "            assert False, \"Invalid initialization of parameters, check your init_type.\"\n",
    "            \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_y)*0.1\n",
    "        \n",
    "    def init_parameters_xavier(self):\n",
    "        # Weights and biases matrices (Xavier initialized)\n",
    "        init_val = np.sqrt(6.0/(self.n_x + self.n_y))\n",
    "        self.W1 = np.random.uniform(-init_val, init_val, (self.n_x, self.n_h))\n",
    "        self.b1 = np.random.uniform(-init_val, init_val, (1, self.n_h))\n",
    "        self.W2 = np.random.uniform(-init_val, init_val, (self.n_h, self.n_y))\n",
    "        self.b2 = np.random.uniform(-init_val, init_val, (1, self.n_y))\n",
    "        \n",
    "    def init_parameters_he(self):\n",
    "        # Weights and biases matrices (He initialized)\n",
    "        range1 = np.sqrt(2/self.n_x)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*range1\n",
    "        self.b1 = np.zeros((1, self.n_h))\n",
    "        range2 = np.sqrt(2/self.n_h)\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*range2\n",
    "        self.b2 = np.zeros((1, self.n_y))\n",
    "        \n",
    "    def init_parameters_lecun(self):\n",
    "        # Weights and biases matrices (LeCun initialized)\n",
    "        range1 = np.sqrt(1/self.n_x)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*range1\n",
    "        self.b1 = np.zeros((1, self.n_h))\n",
    "        range2 = np.sqrt(1/self.n_h)\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*range2\n",
    "        self.b2 = np.zeros((1, self.n_y))\n",
    "        \n",
    "    def init_parameters_const(self):\n",
    "        # Weights and biases matrices (Constant initialized)\n",
    "        const_val = 0.1\n",
    "        self.W1 = np.ones(shape = (n_x, n_h))*const_val\n",
    "        self.b1 = np.ones(shape = (1, n_h))*const_val\n",
    "        self.W2 = np.ones(shape = (n_h, n_y))*const_val\n",
    "        self.b2 = np.ones(shape = (1, n_y))*const_val\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(Z1_b, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        return Z2_b\n",
    "    \n",
    "    def MSE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        losses = (pred - outputs_re)**2\n",
    "        self.loss = np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, alpha = 1e-5):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        Z2 = np.matmul(Z1_b, self.W2)\n",
    "        y_pred = Z2 + self.b2\n",
    "    \n",
    "        # Compute error term\n",
    "        epsilon = y_pred - outputs\n",
    "    \n",
    "        # Compute the gradient for W2 and b2\n",
    "        dL_dW2 = (2/m)*np.matmul(Z1_b.T, epsilon)\n",
    "        dL_db2 = (2/m)*np.sum(epsilon, axis = 0, keepdims = True)\n",
    "\n",
    "        # Compute the loss derivative with respect to the first layer\n",
    "        dL_dZ1 = np.matmul(epsilon, self.W2.T)\n",
    "\n",
    "        # Compute the gradient for W1 and b1\n",
    "        dL_dW1 = (2/m)*np.matmul(inputs.T, dL_dZ1)\n",
    "        dL_db1 = (2/m)*np.sum(dL_dZ1, axis = 0, keepdims = True)\n",
    "\n",
    "        # Update the weights and biases using gradient descent\n",
    "        self.W1 -= alpha*dL_dW1\n",
    "        self.b1 -= alpha*dL_db1\n",
    "        self.W2 -= alpha*dL_dW2\n",
    "        self.b2 -= alpha*dL_db2\n",
    "        \n",
    "        # Update loss\n",
    "        self.MSE_loss(inputs, outputs)\n",
    "    \n",
    "    def train(self, N_max = 1000, alpha = 1e-5, beta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            self.backward(inputs, outputs, alpha)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max//100) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for beta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < beta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than beta.\")\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "Below, we are trying the different initializations we have implemented and check their final loss values.\n",
    "\n",
    "While it seems that the Normal and Lecun behave nicely, we end up with NaN values for Xavier and He initializations. This NaN value is a typical symptom for a phenomenon called the **exploding gradient syndrom**, which we will investigate in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19879750888125622\n"
     ]
    }
   ],
   "source": [
    "# Define neural network structure (random normal initialization)\n",
    "n_x = 2\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "init_type = \"Normal\"\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_rand = ShallowNeuralNet(n_x, n_h, n_y, init_type)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_rand.train(N_max = 10000, alpha = 1e-6, beta = 1e-6, display = False)\n",
    "print(shallow_neural_net_rand.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Define neural network structure (Xavier initialization)\n",
    "n_x = 2\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "init_type = \"Xavier\"\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_rand = ShallowNeuralNet(n_x, n_h, n_y, init_type)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_rand.train(N_max = 10000, alpha = 1e-6, beta = 1e-6, display = False)\n",
    "print(shallow_neural_net_rand.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Define neural network structure (He initialization)\n",
    "n_x = 2\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "init_type = \"He\"\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_rand = ShallowNeuralNet(n_x, n_h, n_y, init_type)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_rand.train(N_max = 10000, alpha = 1e-6, beta = 1e-6, display = False)\n",
    "print(shallow_neural_net_rand.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19672435697959575\n"
     ]
    }
   ],
   "source": [
    "# Define neural network structure (LeCun initialization)\n",
    "n_x = 2\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "init_type = \"LeCun\"\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_rand = ShallowNeuralNet(n_x, n_h, n_y, init_type)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_rand.train(N_max = 10000, alpha = 1e-6, beta = 1e-6, display = False)\n",
    "print(shallow_neural_net_rand.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "In the next notebook, we will investigate the exploding gradient problem that we have observed in the Xavier and He initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
