{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Adding more layers, larger layers, or more features\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (27/01/2022)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Removing unecessary warnings (optional, just makes notebook outputs more readable)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. New mock dataset generation - Nonlinearity with mysterious equation\n",
    "\n",
    "As before, we will generate a dataset with some non-linearity, produced by a mysterious equation.\n",
    "\n",
    "We do not, however, provide the exact equation of the boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All helper functions\n",
    "eps = 1e-5\n",
    "min_val = -1 + eps\n",
    "max_val = 1 - eps\n",
    "def val(min_val, max_val):\n",
    "    return round(np.random.uniform(min_val, max_val), 2)\n",
    "def misterious_equation(val1, val2):\n",
    "    return 2*val1**2 + 3*val2**2 + 0.75*val1 + 0.25*val2\n",
    "def class_for_val(val1, val2):\n",
    "    return int(misterious_equation(val1, val2) >= 1)\n",
    "n_points = 1000\n",
    "def create_dataset(n_points, min_val, max_val):\n",
    "    val1_list = np.array([val(min_val, max_val) for _ in range(n_points)])\n",
    "    val2_list = np.array([val(min_val, max_val) for _ in range(n_points)])\n",
    "    inputs = np.array([[v1, v2] for v1, v2 in zip(val1_list, val2_list)])\n",
    "    outputs = np.array([class_for_val(v1, v2) for v1, v2 in zip(val1_list, val2_list)]).reshape(n_points, 1)\n",
    "    return val1_list, val2_list, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(27)\n",
    "val1_list, val2_list, inputs, outputs = create_dataset(n_points, min_val, max_val)\n",
    "# Check a few entries of the dataset\n",
    "print(val1_list.shape)\n",
    "print(val2_list.shape)\n",
    "print(inputs.shape)\n",
    "print(outputs.shape)\n",
    "print(inputs[0:5, :])\n",
    "print(outputs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected and observed in the plots below, the dataset does not exhibit linearity, due to the presence of a mysterious function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# Scatter plot\n",
    "markers = {0: \"x\", 1: \"o\"}\n",
    "colors = {0: \"r\", 1: \"g\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Template for our Shallow Neural Network with Activation functions from Notebook 4\n",
    "\n",
    "As in Notebook 4, below is the template class for a neural network consisting of a single linear layer, with sigmoid activation function. Let us try to train it and see its capabilities on the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_WithAct_OneLayer():\n",
    "    \n",
    "    def __init__(self, n_x, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W = np.random.randn(self.n_x, self.n_y)*0.1\n",
    "        self.b = np.random.randn(1, self.n_y)*0.1\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the second layer\n",
    "        Z = np.matmul(inputs, self.W)\n",
    "        Z_b = Z + self.b\n",
    "        y_pred = self.sigmoid(Z_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        self.loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, alpha = 1e-5):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z = np.matmul(inputs, self.W)\n",
    "        Z_b = Z + self.b\n",
    "        A = self.sigmoid(Z_b)\n",
    "    \n",
    "        # Compute error term\n",
    "        dL_dA = -outputs/A + (1 - outputs)/(1 - A)\n",
    "        dL_dZ = dL_dA*A*(1 - A)\n",
    "        \n",
    "        # Gradient descent update rules\n",
    "        self.W -= (1/m)*alpha*np.dot(A.T, dL_dZ)\n",
    "        self.b -= (1/m)*alpha*np.sum(dL_dZ, axis = 0, keepdims = True)\n",
    "        \n",
    "        # Update loss\n",
    "        self.CE_loss(inputs, outputs)\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1e-5, delta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            self.backward(inputs, outputs, alpha)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max*0.05) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for delta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < delta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than delta on iteration {}.\".format(iteration_number))\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "n_x = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act = ShallowNeuralNet_WithAct_OneLayer(n_x, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act.train(inputs, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_neural_net_act.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained, but we are not surprised to see that it struggles to classify... The model is too simple for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# --- Scatter dataset plot\n",
    "markers = {0: \"s\", 1: \"o\"}\n",
    "colors = {0: \"r\", 1: \"g\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# --- Make predictions \n",
    "outputs_pred = np.round(shallow_neural_net_act.forward(inputs))\n",
    "\n",
    "# --- Scatter prediction plot\n",
    "markers_pred = {0: \"P\", 1: \"*\"}\n",
    "colors_pred = {0: \"b\", 1: \"k\"}\n",
    "indexes_0_pred = np.where(outputs_pred == 0)[0]\n",
    "v1_0_pred = val1_list[indexes_0_pred]\n",
    "v2_0_pred = val2_list[indexes_0_pred]\n",
    "indexes_1_pred = np.where(outputs_pred == 1)[0]\n",
    "v1_1_pred = val1_list[indexes_1_pred]\n",
    "v2_1_pred = val2_list[indexes_1_pred]\n",
    "plt.scatter(v1_0_pred, v2_0_pred, c = colors_pred[0], marker = markers_pred[0])\n",
    "plt.scatter(v1_1_pred, v2_1_pred, c = colors_pred[1], marker = markers_pred[1])\n",
    "\n",
    "# --- Show legend\n",
    "legend_elements = [Line2D([0], [0], marker = 's', color = 'w', label = 'True 0', \\\n",
    "                          markerfacecolor = 'r', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'o', color = 'w', label = 'True 1', \\\n",
    "                          markerfacecolor = 'g', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'P', color = 'w', label = 'Predicted 0', \\\n",
    "                          markerfacecolor = 'b', markersize = 10),\n",
    "                   Line2D([0], [0], marker = '*', color = 'w', label = 'Predicted 1', \\\n",
    "                          markerfacecolor = 'k', markersize = 10)]\n",
    "plt.legend(handles = legend_elements, loc = 'best')\n",
    "acc = accuracy_score(outputs_pred, outputs)\n",
    "plt.title(\"Model 1 (single layer + sigmoid) - Accuracy Score = {}\".format(acc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How about two layers then?\n",
    "\n",
    "How about two layers then? (As in Notebook 4 also)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_WithAct_TwoLayers():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_y)*0.1\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        self.loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, alpha = 1e-5):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        A2 = self.sigmoid(Z2_b)\n",
    "    \n",
    "        # Compute error term\n",
    "        dL_dA2 = -outputs/A2 + (1 - outputs)/(1 - A2)\n",
    "        dL_dZ2 = dL_dA2*A2*(1 - A2)\n",
    "        dL_dA1 = np.dot(dL_dZ2, self.W2.T)\n",
    "        dL_dZ1 = dL_dA1*A1*(1 - A1)\n",
    "        \n",
    "        # Gradient descent update rules\n",
    "        self.W2 -= (1/m)*alpha*np.dot(A1.T, dL_dZ2)\n",
    "        self.W1 -= (1/m)*alpha*np.dot(inputs.T, dL_dZ1)\n",
    "        self.b2 -= (1/m)*alpha*np.sum(dL_dZ2, axis = 0, keepdims = True)\n",
    "        self.b1 -= (1/m)*alpha*np.sum(dL_dZ1, axis = 0, keepdims = True)\n",
    "        \n",
    "        # Update loss\n",
    "        self.CE_loss(inputs, outputs)\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1e-5, delta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            self.backward(inputs, outputs, alpha)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max*0.05) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for delta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < delta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than delta on iteration {}.\".format(iteration_number))\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "n_x = 2\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act2 = ShallowNeuralNet_WithAct_TwoLayers(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act2.train(inputs, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act2.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_act2.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained, but it stil struggles to classify... Is the model still too simple for the task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# --- Scatter dataset plot\n",
    "markers = {0: \"s\", 1: \"o\"}\n",
    "colors = {0: \"r\", 1: \"g\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# --- Make predictions \n",
    "outputs_pred = np.round(shallow_neural_net_act2.forward(inputs))\n",
    "\n",
    "# --- Scatter prediction plot\n",
    "markers_pred = {0: \"P\", 1: \"*\"}\n",
    "colors_pred = {0: \"b\", 1: \"k\"}\n",
    "indexes_0_pred = np.where(outputs_pred == 0)[0]\n",
    "v1_0_pred = val1_list[indexes_0_pred]\n",
    "v2_0_pred = val2_list[indexes_0_pred]\n",
    "indexes_1_pred = np.where(outputs_pred == 1)[0]\n",
    "v1_1_pred = val1_list[indexes_1_pred]\n",
    "v2_1_pred = val2_list[indexes_1_pred]\n",
    "plt.scatter(v1_0_pred, v2_0_pred, c = colors_pred[0], marker = markers_pred[0])\n",
    "plt.scatter(v1_1_pred, v2_1_pred, c = colors_pred[1], marker = markers_pred[1])\n",
    "\n",
    "# --- Show legend\n",
    "legend_elements = [Line2D([0], [0], marker = 's', color = 'w', label = 'True 0', \\\n",
    "                          markerfacecolor = 'r', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'o', color = 'w', label = 'True 1', \\\n",
    "                          markerfacecolor = 'g', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'P', color = 'w', label = 'Predicted 0', \\\n",
    "                          markerfacecolor = 'b', markersize = 10),\n",
    "                   Line2D([0], [0], marker = '*', color = 'w', label = 'Predicted 1', \\\n",
    "                          markerfacecolor = 'k', markersize = 10)]\n",
    "plt.legend(handles = legend_elements, loc = 'best')\n",
    "acc = accuracy_score(outputs_pred, outputs)\n",
    "plt.title(\"Model 1 (single layer + sigmoid) - Accuracy Score = {}\".format(acc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How about adding more layers?\n",
    "\n",
    "What if we decided to stick to the original inputs (no feature processing), but added more layers, e.g. a third one?\n",
    "\n",
    "**Practice:** How would you modify the template below to have a third layer with the following successive sizes?\n",
    "\n",
    "$ N_x \\rightarrow N_h \\rightarrow N_{h2} \\rightarrow N_y $\n",
    "\n",
    "We already provide a suggestion for the init method, and parts where the code should probably be amended (we will probably need to amend some methods such as parameters initialization, forward and backward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_WithAct_ThreeLayers():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_h2, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_h2 = n_h2\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_h2)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_h2)*0.1\n",
    "        \"\"\"\n",
    "        If a third layer is added, it will need parameters.\n",
    "        \"\"\"\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "        \"\"\"\n",
    "        If a third layer is added, more operations will occur.\n",
    "        \"\"\"\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        self.loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, alpha = 1e-5):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        A2 = self.sigmoid(Z2_b)\n",
    "        \"\"\"\n",
    "        If a third layer is added, more operations will occur.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Compute error term\n",
    "        dL_dA2 = -outputs/A2 + (1 - outputs)/(1 - A2)\n",
    "        dL_dZ2 = dL_dA2*A2*(1 - A2)\n",
    "        dL_dA1 = np.dot(dL_dZ2, self.W2.T)\n",
    "        dL_dZ1 = dL_dA1*A1*(1 - A1)\n",
    "        \"\"\"\n",
    "        If a third layer is added, more gradients will have to be computed using the chain rule.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Gradient descent update rules\n",
    "        self.W2 -= (1/m)*alpha*np.dot(A1.T, dL_dZ2)\n",
    "        self.W1 -= (1/m)*alpha*np.dot(inputs.T, dL_dZ1)\n",
    "        self.b2 -= (1/m)*alpha*np.sum(dL_dZ2, axis = 0, keepdims = True)\n",
    "        self.b1 -= (1/m)*alpha*np.sum(dL_dZ1, axis = 0, keepdims = True)\n",
    "        \"\"\"\n",
    "        If a third layer is added, more gradient descent update rules will occur.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update loss\n",
    "        self.CE_loss(inputs, outputs)\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1e-5, delta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            self.backward(inputs, outputs, alpha)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max*0.05) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for delta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < delta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than delta on iteration {}.\".format(iteration_number))\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "n_x = 2\n",
    "n_h = 2\n",
    "n_h2 = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act3 = ShallowNeuralNet_WithAct_ThreeLayers(n_x, n_h, n_h2, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act3.train(inputs, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act3.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_act3.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained, but it still struggles to classify... It seems adding more layers will not do the trick here. It was still good practice nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# --- Scatter dataset plot\n",
    "markers = {0: \"s\", 1: \"o\"}\n",
    "colors = {0: \"r\", 1: \"g\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# --- Make predictions \n",
    "outputs_pred = np.round(shallow_neural_net_act3.forward(inputs))\n",
    "\n",
    "# --- Scatter prediction plot\n",
    "markers_pred = {0: \"P\", 1: \"*\"}\n",
    "colors_pred = {0: \"b\", 1: \"k\"}\n",
    "indexes_0_pred = np.where(outputs_pred == 0)[0]\n",
    "v1_0_pred = val1_list[indexes_0_pred]\n",
    "v2_0_pred = val2_list[indexes_0_pred]\n",
    "indexes_1_pred = np.where(outputs_pred == 1)[0]\n",
    "v1_1_pred = val1_list[indexes_1_pred]\n",
    "v2_1_pred = val2_list[indexes_1_pred]\n",
    "plt.scatter(v1_0_pred, v2_0_pred, c = colors_pred[0], marker = markers_pred[0])\n",
    "plt.scatter(v1_1_pred, v2_1_pred, c = colors_pred[1], marker = markers_pred[1])\n",
    "\n",
    "# --- Show legend\n",
    "legend_elements = [Line2D([0], [0], marker = 's', color = 'w', label = 'True 0', \\\n",
    "                          markerfacecolor = 'r', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'o', color = 'w', label = 'True 1', \\\n",
    "                          markerfacecolor = 'g', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'P', color = 'w', label = 'Predicted 0', \\\n",
    "                          markerfacecolor = 'b', markersize = 10),\n",
    "                   Line2D([0], [0], marker = '*', color = 'w', label = 'Predicted 1', \\\n",
    "                          markerfacecolor = 'k', markersize = 10)]\n",
    "plt.legend(handles = legend_elements, loc = 'best')\n",
    "acc = accuracy_score(outputs_pred, outputs)\n",
    "plt.title(\"Model 1 (single layer + sigmoid) - Accuracy Score = {}\".format(acc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How about some feature engineering instead?\n",
    "\n",
    "**Practice:** The mysterious equation seems to be using the squared values of the inputs. How about reworking the inputs so each sample consists of 4 values instead of 2, by adding the squared values of each original inputs?\n",
    "\n",
    "In other words, let us process the inputs so that each input sample $ i $ in the dataset is transformed as:\n",
    "\n",
    "$$ (val_1, val_2) \\rightarrow (val_1, val_2, val_1^2, val_2^2) $$\n",
    "\n",
    "Do note, that after this input transformation we will have to replace $ N_x = 2 $ with $ N_x = 4 $.\n",
    "\n",
    "And let us go back to only using two layers instead of three as using more layers did not solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing feature so they have squared values on top of the previous values\n",
    "\"\"\"\n",
    "How would we rework the inputs so each sample has 4 features instead of 2.\n",
    "Basically adding squared values along with the previous values.\n",
    "After the transformation inputs_processing will be an array with 4 columns instead of 2.\n",
    "\"\"\"\n",
    "inputs_processing = None\n",
    "print(inputs_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "n_x = 4 # Instead of 2\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act4 = ShallowNeuralNet_WithAct_TwoLayers(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act4.train(inputs_processing, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act4.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_neural_net_act4.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should train, and classify correctly!\n",
    "\n",
    "That is an important lesson for us: more often than not, adding more layers to the model will not solve the problem (it is a rookie mistake that will only cost youy more training time with very little gains). \n",
    "\n",
    "When possible, features engineering (that is, reworking your inputs to add relevant information that could hep train a model) is best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# --- Scatter dataset plot\n",
    "markers = {0: \"s\", 1: \"o\"}\n",
    "colors = {0: \"r\", 1: \"g\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# --- Make predictions \n",
    "outputs_pred = np.round(shallow_neural_net_act4.forward(inputs_processing))\n",
    "\n",
    "# --- Scatter prediction plot\n",
    "markers_pred = {0: \"P\", 1: \"*\"}\n",
    "colors_pred = {0: \"b\", 1: \"k\"}\n",
    "indexes_0_pred = np.where(outputs_pred == 0)[0]\n",
    "v1_0_pred = val1_list[indexes_0_pred]\n",
    "v2_0_pred = val2_list[indexes_0_pred]\n",
    "indexes_1_pred = np.where(outputs_pred == 1)[0]\n",
    "v1_1_pred = val1_list[indexes_1_pred]\n",
    "v2_1_pred = val2_list[indexes_1_pred]\n",
    "plt.scatter(v1_0_pred, v2_0_pred, c = colors_pred[0], marker = markers_pred[0])\n",
    "plt.scatter(v1_1_pred, v2_1_pred, c = colors_pred[1], marker = markers_pred[1])\n",
    "\n",
    "# --- Show legend\n",
    "legend_elements = [Line2D([0], [0], marker = 's', color = 'w', label = 'True 0', \\\n",
    "                          markerfacecolor = 'r', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'o', color = 'w', label = 'True 1', \\\n",
    "                          markerfacecolor = 'g', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'P', color = 'w', label = 'Predicted 0', \\\n",
    "                          markerfacecolor = 'b', markersize = 10),\n",
    "                   Line2D([0], [0], marker = '*', color = 'w', label = 'Predicted 1', \\\n",
    "                          markerfacecolor = 'k', markersize = 10)]\n",
    "plt.legend(handles = legend_elements, loc = 'best')\n",
    "acc = accuracy_score(outputs_pred, outputs)\n",
    "plt.title(\"Model 1 (single layer + sigmoid) - Accuracy Score = {}\".format(acc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How about two layers, but more neurons?\n",
    "\n",
    "A thing that might help however, would be adding more neurons.\n",
    "\n",
    "**Practice:** Let us go back to our original two layers model, but this time, let us increase the size of the first layer, for instance, let us try using $ n_h = 10 $ instead of 2. Is that going to help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act5 = ShallowNeuralNet_WithAct_TwoLayers(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act5.train(inputs, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act5.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_act5.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should train and classify almost perfectly!\n",
    "\n",
    "That is another important lesson: having larger layers is often better than having more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "# --- Scatter dataset plot\n",
    "markers = {0: \"s\", 1: \"o\"}\n",
    "colors = {0: \"r\", 1: \"g\"}\n",
    "indexes_0 = np.where(outputs == 0)[0]\n",
    "v1_0 = val1_list[indexes_0]\n",
    "v2_0 = val2_list[indexes_0]\n",
    "indexes_1 = np.where(outputs == 1)[0]\n",
    "v1_1 = val1_list[indexes_1]\n",
    "v2_1 = val2_list[indexes_1]\n",
    "plt.scatter(v1_0, v2_0, c = colors[0], marker = markers[0])\n",
    "plt.scatter(v1_1, v2_1, c = colors[1], marker = markers[1])\n",
    "\n",
    "# --- Make predictions \n",
    "outputs_pred = np.round(shallow_neural_net_act5.forward(inputs))\n",
    "\n",
    "# --- Scatter prediction plot\n",
    "markers_pred = {0: \"P\", 1: \"*\"}\n",
    "colors_pred = {0: \"b\", 1: \"k\"}\n",
    "indexes_0_pred = np.where(outputs_pred == 0)[0]\n",
    "v1_0_pred = val1_list[indexes_0_pred]\n",
    "v2_0_pred = val2_list[indexes_0_pred]\n",
    "indexes_1_pred = np.where(outputs_pred == 1)[0]\n",
    "v1_1_pred = val1_list[indexes_1_pred]\n",
    "v2_1_pred = val2_list[indexes_1_pred]\n",
    "plt.scatter(v1_0_pred, v2_0_pred, c = colors_pred[0], marker = markers_pred[0])\n",
    "plt.scatter(v1_1_pred, v2_1_pred, c = colors_pred[1], marker = markers_pred[1])\n",
    "\n",
    "# --- Show legend\n",
    "legend_elements = [Line2D([0], [0], marker = 's', color = 'w', label = 'True 0', \\\n",
    "                          markerfacecolor = 'r', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'o', color = 'w', label = 'True 1', \\\n",
    "                          markerfacecolor = 'g', markersize = 10),\n",
    "                   Line2D([0], [0], marker = 'P', color = 'w', label = 'Predicted 0', \\\n",
    "                          markerfacecolor = 'b', markersize = 10),\n",
    "                   Line2D([0], [0], marker = '*', color = 'w', label = 'Predicted 1', \\\n",
    "                          markerfacecolor = 'k', markersize = 10)]\n",
    "plt.legend(handles = legend_elements, loc = 'best')\n",
    "acc = accuracy_score(outputs_pred, outputs)\n",
    "plt.title(\"Model 1 (single layer + sigmoid) - Accuracy Score = {}\".format(acc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
