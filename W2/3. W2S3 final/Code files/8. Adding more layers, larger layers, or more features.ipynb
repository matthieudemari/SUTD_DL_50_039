{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Adding more layers, using larger layers, or crafting more features?\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.1 (16/06/2023)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.11.4)\n",
    "- Matplotlib (tested on v3.7.1)\n",
    "- Numpy (tested on v1.24.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Removing unecessary warnings (optional, just makes notebook outputs more readable)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also prepared a few functions to help you, in the utils.py file, which is given along with this notebook. Feel free to have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. New mock dataset generation - Nonlinearity with a mysterious equation\n",
    "\n",
    "As before, we will generate a dataset with some non-linearity, whose boundary follows a mysterious equation.\n",
    "\n",
    "We do not, however, provide the exact equation of the boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Generation values\n",
    "eps = 1e-5\n",
    "min_val = -1 + eps\n",
    "max_val = 1 - eps\n",
    "n_points = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(27)\n",
    "val1_list, val2_list, inputs, outputs = create_dataset(n_points, min_val, max_val)\n",
    "# Check a few entries of the dataset\n",
    "print(val1_list.shape)\n",
    "print(val2_list.shape)\n",
    "print(inputs.shape)\n",
    "print(outputs.shape)\n",
    "print(inputs[0:5, :])\n",
    "print(outputs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected and observed in the plots below, the dataset does not exhibit linearity, due to the presence of a mysterious function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dataset (no classification)\n",
    "show_dataset(val1_list, val2_list, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Template for our Shallow Neural Network with Activation functions from Notebook 4\n",
    "\n",
    "As in Notebook 4, below is the template class for a neural network consisting of a single linear layer, with sigmoid activation function. The class has been coded for you in the utils file and it ready to be used, as shown below.\n",
    "\n",
    "Let us try to train it and see its capabilities on the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure\n",
    "n_x = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act = ShallowNeuralNet_WithAct_OneLayer(n_x, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act.train(inputs, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "shallow_neural_net_act.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained, but we are not surprised to see that it struggles to classify... The model is too simple for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset and predictions made by model\n",
    "show_dataset_and_predictions(inputs, val1_list, val2_list, outputs, shallow_neural_net_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How about two layers then?\n",
    "\n",
    "How about two layers then? (As in Notebook 4 also)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "n_x = 2\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act2 = ShallowNeuralNet_WithAct_TwoLayers(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act2.train(inputs, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act2.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "shallow_neural_net_act2.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained, but it stil struggles to classify... Is the model still too simple for the task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset and predictions made by model\n",
    "show_dataset_and_predictions(inputs, val1_list, val2_list, outputs, shallow_neural_net_act2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How about two layers, but more neurons?\n",
    "\n",
    "A thing that might help would be adding more neurons to the first layer, because using only two neurons might not be enough.\n",
    "\n",
    "**Practice #1:** Let us go back to our original two layers model, but this time, let us increase the size of the first layer, for instance, let us try using $ n_h = 10 $ instead of $ 2 $. Is that going to help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "\"\"\"\n",
    "What needs to be modified here to make 10 neurons in the first layer?\n",
    "\"\"\"\n",
    "n_x = 2\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act3 = ShallowNeuralNet_WithAct_TwoLayers(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act3.train(inputs, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act3.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_act3.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It trains and almost classifies perfectly!\n",
    "\n",
    "That is another important lesson: you should always ensure that your layers have enough neurons in them.\n",
    "\n",
    "What is a good number of neurons to use then? That is not an easy question to answer, we will discuss it in class together at the end of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset_and_predictions(inputs, val1_list, val2_list, outputs, shallow_neural_net_act3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How about some feature engineering instead?\n",
    "\n",
    "**Practice #2:** The mysterious equation, in the utils file, seems to be using the squared values of the inputs. How about reworking the inputs so each sample consists of 4 values instead of 2, by adding the squared values of each original inputs?\n",
    "\n",
    "In other words, what we are suggesting here is to process the inputs and generate polynomial features, as in the polynomial regression, so that each input sample $ i $ in the dataset is transformed as:\n",
    "\n",
    "$$ (val_1, val_2) \\rightarrow (val_1, val_2, val_1^2, val_2^2) $$\n",
    "\n",
    "Do note that after this input transformation we will call our two-layers model, but will have to replace $ N_x = 2 $ with $ N_x = 4 $.\n",
    "\n",
    "And let us go back to only using two layers instead of three as using more layers did not solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing feature so they have squared values instead\n",
    "def rework_inputs(inputs):\n",
    "    \"\"\"\n",
    "    How would you rework your inputs to make add some polynomial features of order 2 to the dataset?\n",
    "    \"\"\"\n",
    "    inputs_processing = None\n",
    "    return inputs_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing feature so they have squared values instead\n",
    "inputs_processing = rework_inputs(inputs)\n",
    "print(inputs_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "n_x = 4\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act4 = ShallowNeuralNet_WithAct_TwoLayers(n_x, n_h, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act4.train(inputs_processing, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act4.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_neural_net_act4.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It trains, and this time, it seems to classify correctly!\n",
    "\n",
    "That is an important lesson for us: when possible, features engineering (that is, reworking your inputs to add relevant information that could help train a model) is best! It is, however, difficult, as it requires to have insights about the dataset, which relies on human expertise/intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset_and_predictions(inputs_processing, val1_list, val2_list, outputs, shallow_neural_net_act4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How about adding more layers?\n",
    "\n",
    "What if we decided to stick to the original inputs (no feature processing), but added more layers, e.g. a third one?\n",
    "\n",
    "**Practice #3: How would you modify the template below to have a third layer with the following successive sizes?**\n",
    "\n",
    "$ N_x \\rightarrow N_h \\rightarrow N_{h2} \\rightarrow N_y $\n",
    "\n",
    "We already provide a suggestion for the init method, and parts where the code should probably be amended (we will probably need to amend some methods such as parameters initialization, forward and backward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet_WithAct_ThreeLayers():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_h2, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_h2 = n_h2\n",
    "        self.n_y = n_y\n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "        # Loss, initialized as infinity before first calculation is made\n",
    "        self.loss = float(\"Inf\")\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        \"\"\"\n",
    "        This will have to be updated as new parameters will be used for the third layer.\n",
    "        \"\"\"\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_y)*0.1\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "        \"\"\"\n",
    "        This will probably have new operations implemented, as a third layer will be added.\n",
    "        \"\"\"\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # CE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        self.loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, inputs, outputs, alpha = 1e-5):\n",
    "        # Get the number of samples in dataset\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        # Forward propagate\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        A2 = self.sigmoid(Z2_b)\n",
    "        \"\"\"\n",
    "        More stuff happens here too\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute error term\n",
    "        dL_dA2 = -outputs/A2 + (1 - outputs)/(1 - A2)\n",
    "        dL_dZ2 = dL_dA2*A2*(1 - A2)\n",
    "        dL_dA1 = np.dot(dL_dZ2, self.W2.T)\n",
    "        dL_dZ1 = dL_dA1*A1*(1 - A1)\n",
    "        \"\"\"\n",
    "        More partial derivatives need to be calculated for A3 and Z3.\n",
    "        The derivatives above might no longer be correct and should probably be adjusted.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Gradient descent update rules\n",
    "        self.W2 -= (1/m)*alpha*np.dot(A1.T, dL_dZ2)\n",
    "        self.W1 -= (1/m)*alpha*np.dot(inputs.T, dL_dZ1)\n",
    "        self.b2 -= (1/m)*alpha*np.sum(dL_dZ2, axis = 0, keepdims = True)\n",
    "        self.b1 -= (1/m)*alpha*np.sum(dL_dZ1, axis = 0, keepdims = True)\n",
    "        \"\"\"\n",
    "        More gradient descent rules need to be calculated for W3 and b3.\n",
    "        The formulas above might no longer be correct and should probably be adjusted.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update loss\n",
    "        self.CE_loss(inputs, outputs)\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1e-5, delta = 1e-5, display = True):\n",
    "        # List of losses, starts with the current loss\n",
    "        self.losses_list = [self.loss]\n",
    "        # Repeat iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Backpropagate\n",
    "            self.backward(inputs, outputs, alpha)\n",
    "            new_loss = self.loss\n",
    "            # Update losses list\n",
    "            self.losses_list.append(new_loss)\n",
    "            # Display\n",
    "            if(display and iteration_number % (N_max*0.05) == 1):\n",
    "                print(\"Iteration {} - Loss = {}\".format(iteration_number, new_loss))\n",
    "            # Check for delta value and early stop criterion\n",
    "            difference = abs(self.losses_list[-1] - self.losses_list[-2])\n",
    "            if(difference < delta):\n",
    "                if(display):\n",
    "                    print(\"Stopping early - loss evolution was less than delta on iteration {}.\".format(iteration_number))\n",
    "                break\n",
    "        else:\n",
    "            # Else on for loop will execute if break did not trigger\n",
    "            if(display):\n",
    "                print(\"Stopping - Maximal number of iterations reached.\")\n",
    "    \n",
    "    def show_losses_over_training(self):\n",
    "        # Initialize matplotlib\n",
    "        fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        axs[0].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[0].set_xlabel(\"Iteration number\")\n",
    "        axs[0].set_ylabel(\"Loss\")\n",
    "        axs[1].plot(list(range(len(self.losses_list))), self.losses_list)\n",
    "        axs[1].set_xlabel(\"Iteration number\")\n",
    "        axs[1].set_ylabel(\"Loss (in logarithmic scale)\")\n",
    "        axs[1].set_yscale(\"log\")\n",
    "        # Display\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train neural network structure (with activation)\n",
    "n_x = 2\n",
    "n_h = 2\n",
    "n_h2 = 2\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_act5 = ShallowNeuralNet_WithAct_ThreeLayers(n_x, n_h, n_h2, n_y)\n",
    "# Train and show final loss\n",
    "shallow_neural_net_act5.train(inputs, outputs, N_max = 10000, alpha = 1, delta = 1e-8, display = True)\n",
    "print(shallow_neural_net_act5.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_neural_net_act5.show_losses_over_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trains, but it still struggles to classify... (We obtain an accuracy of 66% or so)\n",
    "\n",
    "It seems adding more layers will not do the trick here, as the problem was elsewhere: we needed more neurons in the first layer.\n",
    "\n",
    "It was still good practice nonetheless, and a good lesson for us: adding a layer to a neural network that is malfunctioning is most likely not going to fix its issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset_and_predictions(inputs, val1_list, val2_list, outputs, shallow_neural_net_act5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical homework questions would be\n",
    "\n",
    "1. Show the ShallowNeuralNet_WithAct_ThreeLayers class you coded and explain the changes you made to the code.\n",
    "\n",
    "2. Is the performance of the model improving when adding a third layer as suggested in Task 3?\n",
    "\n",
    "3. Show the rework_inputs function you produced on Task #2 and explain your logic in terms of code.\n",
    "\n",
    "4. Is the performance of the model improving after feature engineering? If so, how could we explain this performance improvement?\n",
    "\n",
    "5. Task #1 suggests using two layers model with more neurons is sufficient to address our underfitting issue.\n",
    "Why this be a good approach? How could we explain this intuitively?\n",
    "\n",
    "6. Challenge: If the issue was indeed more neurons needed for our model, does that mean I could make the 1-layer model work by simply adding more neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
