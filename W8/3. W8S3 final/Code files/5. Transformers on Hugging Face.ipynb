{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09900065-b62c-4481-a902-356e73f4827a",
   "metadata": {},
   "source": [
    "# 5. Transformers on Hugging Face\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (19/01/2025)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.11.4)\n",
    "- Numpy (tested on v1.25.2)\n",
    "- Torch (tested on v2.0.1+cu118)\n",
    "- Torchvision (tested on v0.15.2+cu118)\n",
    "- Transformers (tested on v4.48)\n",
    "- We also strongly recommend setting up CUDA on your machine! (At this point, honestly, it is almost mandatory).\n",
    "\n",
    "### Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d13d5a0-001f-40b2-a088-cdc81bbc4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, logging\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfe4cc-300a-404a-bda3-20b1583a7120",
   "metadata": {},
   "source": [
    "### Downloading a pre-trained transformer model\n",
    "\n",
    "We will be using a pre-trained bert model to make embeddings. Note that, in practice, we often decompose the entire word embedding procedure into a tokenizer and a model, as shown below.\n",
    "\n",
    "The tokenizer is responsible for preprocessing text input so it can be understood by the model. The tokenizer will:\n",
    "- Split the input text into smaller units called \"tokens.\" For BERT, this includes breaking down words into subwords or word pieces, called tokens (e.g., \"unhappiness\" will decompse into the tokens [\"un\", \"happy\", \"ness\"]).\n",
    "- Add special tokens such as separators (to separate sentences or mark the end of a sequence).\n",
    "- Convert tokens into numerical IDs based on the model's vocabulary.\n",
    "\n",
    "This breakdown will be explored more in detail in the NLP Term 7 course.\n",
    "\n",
    "The model is the neural network itself. It processes the tokenized input and produces meaningful numerical representations (our word embeddings) of the text. The model will\n",
    "- receive the tokenized output from the tokenizer,\n",
    "- produce contextual embeddings for each token, capturing semantic and syntactic meaning,\n",
    "- and as a result produce vector representations of the entire input.\n",
    "\n",
    "Both are necessary, as the tokenizer prepares raw text in the correct format and numerical representation for the model. And, the model processes this representation to generate embeddings or predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db65bf5-ca1a-4ea3-b143-bdda4837b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\matth\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\matth\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\matth\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\matth\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\matth\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\matth\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Create and load pre-trained BERT model and tokenizer\n",
    "# (Ignore the warning, if any.)\n",
    "# (This might take a while if you run it for the first time,\n",
    "# as we need to download the model from huggingface and it is 400MB!)\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0c065-51d4-4b20-b4e1-fde0b24b1df3",
   "metadata": {},
   "source": [
    "### Using our pre-trained model to generate some word embeddings\n",
    "\n",
    "Below, we will simply use our pre-trained model to generate word embeddings for three words: cat, kitten and university."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76455bbb-f4b5-4c66-b42a-9ce993a03a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the words to get embeddings for\n",
    "words = [\"cat\", \"kitten\", \"university\"]\n",
    "\n",
    "# Process each word individually\n",
    "for word in words:\n",
    "    # Tokenize first\n",
    "    inputs = tokenizer(word, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    # Compute embedding second\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Extract the embedding for the word (BERT's output includes embeddings for all tokens)\n",
    "    embedding = outputs.last_hidden_state.mean(dim = 1)\n",
    "    embeddings.append(embedding.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e7ffe-12a2-4689-8511-69291b092a4a",
   "metadata": {},
   "source": [
    "### Checking the embeddings and their similarities\n",
    "\n",
    "Later on, we can check the embeddings (by printing the vectors), and more importantly, confirm that the word embedsdings preserve similarity. We do so, by checking the cosine similarity between all pair of words and observe that there is a high similarity between the words \"cat\" and \"kitten\", as opposed to the other pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f22769e0-b666-4022-a325-a348d434f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to check similarity between the embeddings\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/(norm(vec1)*norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5b7a761-da54-42cf-9cf6-fb0ee9fac790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'cat' and 'kitten': 0.8756\n",
      "Cosine similarity between 'cat' and 'university': 0.4423\n",
      "Cosine similarity between 'kitten' and 'university': 0.4332\n"
     ]
    }
   ],
   "source": [
    "# Compute the cosine similarities between some pairs of words to check embedding similarities\n",
    "similarity_cat_kitten = cosine_similarity(embeddings[0], embeddings[1])\n",
    "similarity_cat_university = cosine_similarity(embeddings[0], embeddings[2])\n",
    "similarity_kitten_university = cosine_similarity(embeddings[1], embeddings[2])\n",
    "print(f\"Cosine similarity between 'cat' and 'kitten': {similarity_cat_kitten:.4f}\")\n",
    "print(f\"Cosine similarity between 'cat' and 'university': {similarity_cat_university:.4f}\")\n",
    "print(f\"Cosine similarity between 'kitten' and 'university': {similarity_kitten_university:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780394d9-4d84-477b-98c2-cb5816a37fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
