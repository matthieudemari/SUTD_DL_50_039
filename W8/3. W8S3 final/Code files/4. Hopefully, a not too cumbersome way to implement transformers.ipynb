{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98cf42ca",
   "metadata": {},
   "source": [
    "# 4. Hopefully, a not too cumbersome way to implement transformers\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.1 (22/03/2022)\n",
    "\n",
    "This notebook demonstrates how we could technically implement attention layers and transformers, as described in Vaswani et al., “Attention Is All You Need”, 2017;\n",
    "\n",
    "https://arxiv.org/abs/1706.03762\n",
    "\n",
    "and Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, 2019.\n",
    "\n",
    "https://arxiv.org/abs/1810.04805\n",
    "\n",
    "This code is not designed for running (did not even bother coming with imports for this), and training code (dataset, loss, trainer functions, etc.) is also not provided.\n",
    "\n",
    "This is just for illustrative purposes, as we do not expect students to train transformers (costly in time and resources). The purpose of this notebook is also to show how we may design multiple classes of nn.Module and combine them together in a very sophisticated architecture.\n",
    "\n",
    "More importantly, we no longer have to implement this difficult architecture manually, as a recent PyTorch release now includes a nn.Transformer! (Feel free to check https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html and https://pytorch.org/tutorials/beginner/transformer_tutorial.html for details!)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)\n",
    "- Torch (tested on v1.10.1)\n",
    "- Torchvision (tested on v0.11.2)\n",
    "- We also strongly recommend setting up CUDA on your machine!\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ad17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e91c2",
   "metadata": {},
   "source": [
    "### 1. The Self-attention mechanism and the MultiHeadAttentionLayer layers\n",
    "\n",
    "Let us start with the attention mechanism. As seen in class, it consists of a rather simple mathematical formula involving three matrices Q, K and V.\n",
    "\n",
    "We implement it below as the SelfAttention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2388d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: SelfAttention layer Class, describing the attention layers to\n",
    "    be used in the transformer.\n",
    "    Attributes list:\n",
    "    - d_model: An integer, defining the dimension of the attention layer\n",
    "    - output_size: An integer, defining the dimension of the output\n",
    "    for the attention layer.\n",
    "    - droupout_rate: A float value between 0 and 1, corresponding to the Dropout rate\n",
    "    used in the Dropout layers of the Feed Forward layers.\n",
    "    - mask: A boolean. If set to True, a triangular mask will be applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, output_size, dropout_rate = 0.3, mask = None):\n",
    "        \"\"\"\n",
    "        Init Method for attention layer, mostly defining attributes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_model, output_size)\n",
    "        self.key = nn.Linear(d_model, output_size)\n",
    "        self.value = nn.Linear(d_model, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.mask = mask\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Forward pass for the attention layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute query, key and value parameters\n",
    "        q_shape = q.shape[0]\n",
    "        y_len = q.shape[1]\n",
    "        seq_len = k.shape[1]\n",
    "        query_out = self.query(q)\n",
    "        key_out = self.key(k)\n",
    "        value_out = self.value(v)\n",
    "        key_dim = key.size(-1)\n",
    "        transposed_key_out = key.transpose(1,2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.bmm(query, transposed_key_out)/np.sqrt(key_dim)\n",
    "        \n",
    "        # Apply masks if needed\n",
    "        if self.mask is None:\n",
    "            continue\n",
    "        elif not self.mask:\n",
    "            temp = torch.ones((y_len, y_len), \\\n",
    "                              device = self.mask.device, \\\n",
    "                              dtype = torch.uint8)\n",
    "            subsequent_mask = 1 - torch.triu(temp, diagonal = 1)\n",
    "            subsequent_mask = subsequent_mask[None, :, :].expand(q_shape, y_len, y_len)\n",
    "            scores = scores.masked_fill(subsequent_mask == 0, -float(\"Inf\"))\n",
    "        elif self.mask:\n",
    "            expanded_mask = self.mask[:, None, :].expand(q_shape, y_len, seq_len)\n",
    "            scores = scores.masked_fill(expanded_mask == 0, -float(\"Inf\"))\n",
    "        \n",
    "        # Final touches\n",
    "        weights = F.softmax(scores, dim = -1)\n",
    "        out = torch.bmm(weights, value_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba981e",
   "metadata": {},
   "source": [
    "We can then use the attention operation multiple times in a row, as suggested in the original paper, and combine it with a few Linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0908b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: MultiHeadAttention layer Class, describing the attention layers to\n",
    "    be used in the transformer.\n",
    "    Attributes list:\n",
    "    - d_model: An integer, defining the dimension of the attention layer\n",
    "    - num_heads: An integer, defining the number of attention heads\n",
    "    to be used in both the encoder and decoder layers.\n",
    "    - droupout_rate: A float value between 0 and 1, corresponding to the Dropout rate\n",
    "    used in the Dropout layers of the Feed Forward layers.\n",
    "    - mask: A boolean. If set to True, a triangular mask will be applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout_rate, mask = None):\n",
    "        \"\"\"\n",
    "        Init method, mostly defining attributes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.droupout_rate = droupout_rate\n",
    "        self.attention_output_size = d_model/num_heads\n",
    "        layers = [SelfAttention(d_model, self.attention_output_size, dropout_rate, mask) \\\n",
    "                  for i in range(num_heads)]\n",
    "        self.attention_layers = nn.Module(layers, )\n",
    "        self.final_layer = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Forward pass, simply going through each attention layer.\n",
    "        Applying one linear for final touch.\n",
    "        \"\"\"\n",
    "        x = torch.cat([layer(q, k, v) for layer in self.attention_layers], dim = -1)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec2f59",
   "metadata": {},
   "source": [
    "### 2. The EncoderLayer and Encoder architecture\n",
    "\n",
    "In this part, we start off by defining the typical layers used in the Encoder part of the transformer.\n",
    "It consists of an attention layer (along with a skip connection), and a feed-forward block (which is simply defined as a combination of several linear layers).\n",
    "\n",
    "We implement it in the EncoderLayer module below, which reuses our previous modules accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: EncoderLayer Class, describing the encoder layers to\n",
    "    be used in the Encoder part of the transformer.\n",
    "    Attributes list:\n",
    "    - d_model: An integer, defining the dimension of the encoder layer.\n",
    "    - num_heads: An integer, defining the number of attention heads\n",
    "    to be used in both the encoder and decoder layers.\n",
    "    - d_inner: An integer, defining the number of neurons in the Feed Forward layers.\n",
    "    - droupout_rate: A float value between 0 and 1, corresponding to the Dropout rate\n",
    "    used in the Dropout layers of the Feed Forward layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_inner = 2048, droupout_rate = 0.3):\n",
    "        \"\"\"\n",
    "        Init Method, which adds up\n",
    "        - one MultiHeadAttentionLayer, with an expanded mask,\n",
    "        - one Feed Forward layer,\n",
    "        - and Normalization Layers after each one of them.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First Multi Head Attention Layer\n",
    "        self.attention = MultiHeadAttentionLayer(d_model, num_heads, droupout_rate)\n",
    "        # Feed Forward Network Layer\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_inner),\n",
    "                                 nn.ReLU(inplace = True),\n",
    "                                 nn.Dropout(droupout_rate),\n",
    "                                 nn.Linear(d_inner, d_model),\n",
    "                                 nn.Dropout(droupout_rate),)\n",
    "        # Normalization Layers\n",
    "        # (Roughly identical to BatchNorm)\n",
    "        self.attention_normalization = nn.LayerNorm(d_model)\n",
    "        self.ffn_normalization = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the encoder layer.\n",
    "        \"\"\"\n",
    "        # Attention Layer forward pass and normalization\n",
    "        out = x + self.attention(q = x,\n",
    "                               k = x,\n",
    "                               v = x)\n",
    "        out = self.attention_normalization(out)\n",
    "        # Feed Forward Network Layer forward pass and normalization\n",
    "        out = out + self.ffn(out)\n",
    "        out = self.ffn_normalization(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1034e59",
   "metadata": {},
   "source": [
    "Later on, we combine several EncoderLayer modules together to obtain the full Encoder part of the transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: Encoder Class, describing the Encoding part of the transformer.\n",
    "    Attributes list:\n",
    "    - d_model: An integer, defining the dimension of the encoder layers.\n",
    "    - num_heads: An integer, defining the number of attention heads\n",
    "    to be used in both the encoder and decoder layers.\n",
    "    - num_encoders: An integer, defining the number of encoder layers\n",
    "    to be used in the transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, num_encoders):\n",
    "        \"\"\"\n",
    "        Init Method, which adds up num_encoders EncoderLayers in a row. \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers_list = [EncoderLayer(d_model, num_heads) for i in range(num_encoders)]\n",
    "        self.enc_layers = nn.ModuleList(layers_list, )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder, simply repeating the EncoderLayers in a row.\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        for layers in self.enc_layers:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60784fc",
   "metadata": {},
   "source": [
    "### 3. The DecoderLayer and Decoder architecture\n",
    "\n",
    "Roughly the same logic as in the Encoder part.\n",
    "\n",
    "Same same, but different (masking must be included in some attention layers and output from Encoder must also be included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4724f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: DecoderLayer Class, describing the decoder layers to\n",
    "    be used in the Decoder part of the transformer.\n",
    "    Attributes list:\n",
    "    - d_model: An integer, defining the dimension of the encoder layers.\n",
    "    - num_heads: An integer, defining the number of attention heads\n",
    "    to be used in the decoder layer.\n",
    "    - d_inner: An integer, defining the number of neurons in the Feed Forward layers.\n",
    "    - droupout_rate: A float value between 0 and 1, corresponding to the Dropout rate\n",
    "    used in the Dropout layers of the Feed Forward layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_inner = 2048, droupout_rate = 0.3):\n",
    "        \"\"\"\n",
    "        Init Method, which adds up\n",
    "        - one MultiHeadAttentionLayer, with a triangular mask,\n",
    "        - one MultiHeadAttentionLayer, with no mask,\n",
    "        - one Feed Forward layer,\n",
    "        - and Normalization Layers after each one of them.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Masked First Attention Layer\n",
    "        self.masked_attention = MultiHeadAttentionLayer(d_model, \\\n",
    "                                                        num_heads, \\\n",
    "                                                        droupout_rate, \\\n",
    "                                                        mask = True)\n",
    "        # Subsequent Attention Layer\n",
    "        self.subsequent_attention = MultiHeadAttentionLayer(d_model, \\\n",
    "                                                            num_heads, \\\n",
    "                                                            droupout_rate)\n",
    "        # Feed Forward Network Layer\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_inner),\n",
    "                                 nn.ReLU(inplace = True),\n",
    "                                 nn.Dropout(droupout_rate),\n",
    "                                 nn.Linear(d_inner, d_model),\n",
    "                                 nn.Dropout(droupout_rate),)\n",
    "        # Normalization Layers\n",
    "        # (Roughly identical to BatchNorm)\n",
    "        self.masked_attention_normalization = nn.LayerNorm(d_model)\n",
    "        self.subsequent_attention_normalization = nn.LayerNorm(d_model)\n",
    "        self.ffn_normalization = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, y, enc_out):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder layer.\n",
    "        \"\"\"\n",
    "        # Masked First Attention Layer\n",
    "        out = y\n",
    "        out = out + self.masked_attention(q = out, k = out, v = out)\n",
    "        out = self.masked_attention_normalization(out)\n",
    "        # Subsequent Attention Layer\n",
    "        out = out + self.subsequent_attention(q = out, k = enc_out, v = enc_out)\n",
    "        out = self.subsequent_attention_normalization(out)\n",
    "        # Feed Forward Network Layer\n",
    "        out = out + self.ffn(out)\n",
    "        out = self.ffn_normalization(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6583b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: Encoder Class, describing the Encoding part of the transformer.\n",
    "    Attributes list:\n",
    "    - d_model: An integer, defining the dimension of the decoder layers.\n",
    "    - num_heads: An integer, defining the number of attention heads\n",
    "    to be used in both the decoder layers.\n",
    "    - num_decoders: An integer, defining the number of decoder layers\n",
    "    to be used in the transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, num_decoders):\n",
    "        \"\"\"\n",
    "        Init Method, which adds up num_decoders DecoderLayers in a row. \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers_list = [DecoderLayer(d_model, num_heads) for i in range(num_decoders)]\n",
    "        self.dec_layers = nn.ModuleList(layers_list, )\n",
    "        \n",
    "    def forward(self, y, enc_out):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder, simply repeating the DeccoderLayers in a row.\n",
    "        \"\"\"\n",
    "        out = y\n",
    "        for layers in self.dec_layers:\n",
    "            out = layer(out, enc_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d3813",
   "metadata": {},
   "source": [
    "### 4. Finally assemble everything into your whole Transformer architecture...\n",
    "\n",
    "And voila!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7abb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: Full Transformer Class, combining the Encoder and Decoder parts.\n",
    "    Attributes list:\n",
    "    - d_model: An integer, defining the dimension of the encoder\n",
    "    and decoder layers.\n",
    "    - num_heads: An integer, defining the number of attention heads\n",
    "    to be used in both the encoder and decoder layers.\n",
    "    - num_encoders: An integer, defining the number of encoder layers\n",
    "    to be used in the transformer.\n",
    "    - num_decoders: An integer, defining the number of decoder layers\n",
    "    to be used in the transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model = 512, num_heads = 8, num_encoders = 6, num_decoders = 6):\n",
    "        \"\"\"\n",
    "        Init Method, establishing the Decoder and Encoder parts as\n",
    "        attributes of the transfomer.\n",
    "        Later on, we will extract the encoder part for our Embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, num_heads, num_encoders)\n",
    "        self.decoder = Decoder(d_model, num_heads, num_decoders)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward pass for transformer.\n",
    "        - Will encoder the input with the Encoder part first.\n",
    "        - Then use the Decoder, combining the encoded input and its target,\n",
    "        along with masks.\n",
    "        \"\"\"\n",
    "        enc_out = self.encoder(x)\n",
    "        dec_out = self.decoder(y, enc_out)\n",
    "        return dec_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
