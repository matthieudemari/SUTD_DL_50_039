{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866b19f8",
   "metadata": {},
   "source": [
    "# 5. Vanilla GAN on MNIST with FC layers\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.1 (05/04/2022)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d28b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2569c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b1473",
   "metadata": {},
   "source": [
    "### Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c9680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transform to be applied to dataset\n",
    "# - Tensor conversion\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95021a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST train dataset\n",
    "mnist = torchvision.datasets.MNIST(root = './data/',\n",
    "                                   train = True,\n",
    "                                   transform = transform,\n",
    "                                   download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0c4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "batch_size = 32\n",
    "data_loader = torch.utils.data.DataLoader(dataset = mnist,\n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a457b",
   "metadata": {},
   "source": [
    "### Discriminator model as a set of FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2ef6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Dicriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, image_size):\n",
    "        # Init from nn.Module\n",
    "        super().__init__()\n",
    "        \n",
    "        # FC layers\n",
    "        self.D = nn.Sequential(nn.Linear(image_size, hidden_size),\n",
    "                               nn.LeakyReLU(0.2),\n",
    "                               nn.Linear(hidden_size, hidden_size),\n",
    "                               nn.LeakyReLU(0.2),\n",
    "                               nn.Linear(hidden_size, 1),\n",
    "                               nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.D(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2b44a",
   "metadata": {},
   "source": [
    "### Generator model as a set of FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04431258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size, hidden_size, image_size):\n",
    "        # Init from nn.Module\n",
    "        super().__init__()\n",
    "        \n",
    "        # FC layers\n",
    "        self.G = nn.Sequential(nn.Linear(latent_size, hidden_size),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(hidden_size, hidden_size),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(hidden_size, image_size),\n",
    "                               nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.G(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c447cc",
   "metadata": {},
   "source": [
    "### Trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "172b145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for model generation and training\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 300\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d2ba48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dicriminator(\n",
       "  (D): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create discriminator model\n",
    "D = Dicriminator(hidden_size, image_size)\n",
    "D.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86d5852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (G): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=784, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create generator model\n",
    "G = Generator(latent_size, hidden_size, image_size)\n",
    "G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a9af55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr = 0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr = 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b99f82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History trackers for training curves\n",
    "# Keeping track of losses and accuracy scores\n",
    "d_losses = np.zeros(num_epochs)\n",
    "g_losses = np.zeros(num_epochs)\n",
    "real_scores = np.zeros(num_epochs)\n",
    "fake_scores = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077c494",
   "metadata": {},
   "source": [
    "**Note: running the cell below (our trainer function) will take a long time!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "069467d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/300], Step [200/1875], d_loss: 0.1433, g_loss: 4.0865, D(x): 0.94, D(G(z)): 0.07\n",
      "Epoch [0/300], Step [400/1875], d_loss: 0.6542, g_loss: 4.9897, D(x): 0.72, D(G(z)): 0.16\n",
      "Epoch [0/300], Step [600/1875], d_loss: 0.1404, g_loss: 5.0244, D(x): 0.90, D(G(z)): 0.03\n",
      "Epoch [0/300], Step [800/1875], d_loss: 0.1928, g_loss: 5.2891, D(x): 0.95, D(G(z)): 0.10\n",
      "Epoch [0/300], Step [1000/1875], d_loss: 0.6924, g_loss: 1.6430, D(x): 0.78, D(G(z)): 0.29\n",
      "Epoch [0/300], Step [1200/1875], d_loss: 0.3639, g_loss: 3.6104, D(x): 0.81, D(G(z)): 0.11\n",
      "Epoch [0/300], Step [1400/1875], d_loss: 0.6465, g_loss: 2.3239, D(x): 0.77, D(G(z)): 0.24\n",
      "Epoch [0/300], Step [1600/1875], d_loss: 0.3685, g_loss: 3.0124, D(x): 0.84, D(G(z)): 0.12\n",
      "Epoch [0/300], Step [1800/1875], d_loss: 0.3489, g_loss: 2.3461, D(x): 0.93, D(G(z)): 0.23\n",
      "Epoch [1/300], Step [200/1875], d_loss: 0.2324, g_loss: 2.8783, D(x): 0.93, D(G(z)): 0.11\n",
      "Epoch [1/300], Step [400/1875], d_loss: 0.3017, g_loss: 2.8461, D(x): 0.91, D(G(z)): 0.15\n",
      "Epoch [1/300], Step [600/1875], d_loss: 0.3020, g_loss: 3.3072, D(x): 0.92, D(G(z)): 0.14\n",
      "Epoch [1/300], Step [800/1875], d_loss: 0.4148, g_loss: 3.6628, D(x): 0.88, D(G(z)): 0.10\n",
      "Epoch [1/300], Step [1000/1875], d_loss: 0.3996, g_loss: 3.9128, D(x): 0.92, D(G(z)): 0.17\n",
      "Epoch [1/300], Step [1200/1875], d_loss: 0.4047, g_loss: 2.9210, D(x): 0.85, D(G(z)): 0.17\n",
      "Epoch [1/300], Step [1400/1875], d_loss: 0.2493, g_loss: 3.1288, D(x): 0.86, D(G(z)): 0.05\n",
      "Epoch [1/300], Step [1600/1875], d_loss: 0.2276, g_loss: 3.0737, D(x): 0.95, D(G(z)): 0.13\n",
      "Epoch [1/300], Step [1800/1875], d_loss: 0.2151, g_loss: 2.9864, D(x): 0.90, D(G(z)): 0.07\n",
      "Epoch [2/300], Step [200/1875], d_loss: 0.1216, g_loss: 5.7480, D(x): 0.97, D(G(z)): 0.06\n",
      "Epoch [2/300], Step [400/1875], d_loss: 0.0784, g_loss: 5.1353, D(x): 0.96, D(G(z)): 0.02\n",
      "Epoch [2/300], Step [600/1875], d_loss: 0.1390, g_loss: 4.5242, D(x): 0.95, D(G(z)): 0.07\n",
      "Epoch [2/300], Step [800/1875], d_loss: 0.1369, g_loss: 3.9988, D(x): 0.96, D(G(z)): 0.08\n",
      "Epoch [2/300], Step [1000/1875], d_loss: 0.2794, g_loss: 4.4670, D(x): 0.90, D(G(z)): 0.08\n",
      "Epoch [2/300], Step [1200/1875], d_loss: 0.0349, g_loss: 4.8950, D(x): 1.00, D(G(z)): 0.03\n",
      "Epoch [2/300], Step [1400/1875], d_loss: 0.0569, g_loss: 5.5962, D(x): 0.97, D(G(z)): 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m outputs \u001b[38;5;241m=\u001b[39m D(fake_images)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 6. We train G to maximize log(D(G(z))\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# instead of minimizing log(1-D(G(z)))\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# (Strictly equivalent but empirically better)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 7. Backprop and optimize G\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Remember to reset gradients for both optimizers!\u001b[39;00m\n\u001b[0;32m     60\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:3030\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3027\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3028\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # 1. Flatten image\n",
    "        images = images.view(batch_size, -1).to(device)\n",
    "        images = Variable(images)\n",
    "        \n",
    "        # 2. Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        real_labels = Variable(real_labels)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        fake_labels = Variable(fake_labels)\n",
    "        \n",
    "        \"\"\"\n",
    "        PART 1: TRAIN THE DISCRIMINATOR\n",
    "        \"\"\"\n",
    "\n",
    "        # 3. Compute BCE_Loss using real images\n",
    "        # Here, BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels = 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # 3.bis. Compute BCELoss using fake images\n",
    "        # Here, BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # First term of the loss is always zero since fake_labels = 0\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        z = Variable(z)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # 4. Backprop and optimize for D\n",
    "        # Remember to reset gradients for both optimizers!\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        PART 2: TRAIN THE GENERATOR\n",
    "        \"\"\"\n",
    "\n",
    "        # 5. Generate fresh noise samples and produce fake images\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        z = Variable(z)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # 6. We train G to maximize log(D(G(z))\n",
    "        # instead of minimizing log(1-D(G(z)))\n",
    "        # (Strictly equivalent but empirically better)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # 7. Backprop and optimize G\n",
    "        # Remember to reset gradients for both optimizers!\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        PART 3: UPDATE STATISTICS FOR VISUALIZATION LATER\n",
    "        \"\"\"\n",
    "        \n",
    "        # 8. Update the losses and scores for mini-batches\n",
    "        d_losses[epoch] = d_losses[epoch]*(i/(i+1.)) \\\n",
    "            + d_loss.item()*(1./(i+1.))\n",
    "        g_losses[epoch] = g_losses[epoch]*(i/(i+1.)) \\\n",
    "            + g_loss.item()*(1./(i+1.))\n",
    "        real_scores[epoch] = real_scores[epoch]*(i/(i+1.)) \\\n",
    "            + real_score.mean().item()*(1./(i+1.))\n",
    "        fake_scores[epoch] = fake_scores[epoch]*(i/(i+1.)) \\\n",
    "            + fake_score.mean().item()*(1./(i+1.))\n",
    "        \n",
    "        # 9. Display\n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e17b8",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2436479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display losses for both the generator and discriminator\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), d_losses, label = 'd loss')\n",
    "plt.plot(range(1, num_epochs + 1), g_losses, label = 'g loss')    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d250f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display accuracy scores for both the generator and discriminator\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), fake_scores, label='fake score')\n",
    "plt.plot(range(1, num_epochs + 1), real_scores, label='real score')    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d301d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a few fake samples (5 of them) for visualization\n",
    "n_samples = 5\n",
    "z = torch.randn(n_samples, latent_size).to(device)\n",
    "z = Variable(z)\n",
    "fake_images = G(z)\n",
    "fake_images = fake_images.cpu().detach().numpy().reshape(n_samples, 28, 28)\n",
    "print(fake_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[0])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[1])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[2])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[3])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03abfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
