{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. PyTorch Basics - Datasets and Dataloaders\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.2 (22/06/2023)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.11.4)\n",
    "- Matplotlib (tested on v3.7.1)\n",
    "- Numpy (tested on v1.24.3)\n",
    "- Pandas (tested on v2.0.2)\n",
    "- Torch (tested on v2.0.1+cu118)\n",
    "- Torchvision (tested on v0.15.2+cu118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in datasets\n",
    "\n",
    "Pytorch has a few built-in datasets, typically the most common ones that have been used to demonstrate concepts, such as MNIST or CIFAR-10. When relying on the built-in datasets, PyTorch will have commands that allow to download the dataset directly, ready to be used on models.\n",
    "\n",
    "For more details on the available built-in datasets, have a look at the following page: https://pytorch.org/vision/stable/datasets.html.\n",
    "\n",
    "Below, we demonstrate how to use built-in datasets, using the FashionMNIST dataset. This dataset consists of 28 by 28 greyscale images, with 10 classes, listed below.\n",
    "\n",
    "It is typically used to design image classification models (i.e. models that receive images as inputs), and attempt to predict what is in the image in question. The 10 classes are index with 0-9 values, corresponding to the 10 types of fashion objects found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of labels and their identification\n",
    "labels_map = {0: \"T-Shirt\",\n",
    "              1: \"Trouser\",\n",
    "              2: \"Pullover\",\n",
    "              3: \"Dress\",\n",
    "              4: \"Coat\",\n",
    "              5: \"Sandal\",\n",
    "              6: \"Shirt\",\n",
    "              7: \"Sneaker\",\n",
    "              8: \"Bag\",\n",
    "              9: \"Ankle Boot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path as string\n",
    "# Dataset will be downloaded and stored there.\n",
    "folder_path = \"./data\"\n",
    "\n",
    "# Download (download = True) training data (train = True)\n",
    "# to folder specified in root parameter.\n",
    "# The transform parameter specifies that the image samples will\n",
    "# be converted to Tensors, ready to be used by models.\n",
    "training_data = datasets.FashionMNIST(root = folder_path, \\\n",
    "                                      train = True, \\\n",
    "                                      download = True, \\\n",
    "                                      transform = ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be downloaded, if not already in folder specified in folder_path.\n",
    "\n",
    "The training_data object we obtain has interesting attributes, such as *data* (which contains the data in a single tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Dataset contains 60000 samples, that are greyscale \n",
    "# images with size 28 by 28 pixels.\n",
    "print(training_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  torch.Size([1, 28, 28])\n",
      "Label:  8\n"
     ]
    }
   ],
   "source": [
    "# We can then fetch a sample using the [] notations\n",
    "sample_index = 894\n",
    "img, label = training_data[sample_index]\n",
    "print(\"Image: \", img.shape)\n",
    "print(\"Label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display sample\n",
    "sample_index = 894\n",
    "img, label = training_data[sample_index]\n",
    "plt.figure(figsize = (3, 3))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"Image {} - Label = {} ({})\".format(sample_index, \\\n",
    "                                              label, \\\n",
    "                                              labels_map[label]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a custom Dataset\n",
    "\n",
    "Most of the time, when demonstrating concepts, we will rely on a \"simple\" dataset, available in the PyTorch library. In practice, however, you will often play with a custom dataset, fitting your project needs.\n",
    "\n",
    "Most datasets can be provided by your future employer, or can be found on dataset search engines, such as Kaggle, Google Dataset Search, etc.\n",
    "\n",
    "Today, we will play with a simplified version of the Ames Housing Dataset, which can be found online, here: https://www.kaggle.com/datasets/prevek18/ames-housing-dataset?resource=download\n",
    "\n",
    "Compiled by Dean De Cock for use in data science education, this dataset includes a variety of features for approximately 2,800 houses in Ames, Iowa, including the size of the house (in square feet), the number of bedrooms and bathrooms, the year the house was built, and more. It also includes the sale price for each house. The Ames Housing Dataset is a popular choice for machine learning projects, and it has been used to build models for predicting house prices based on various features.\n",
    "\n",
    "It consists of an Excel file (AmesHousing.xlsx) stored in the ./ames/ folder. The original dataset can be found in the  AmesHousing.csv file, but we have simplified it by removing some of its features. The features we are interested in are:\n",
    "\n",
    "- Lot Area: The area of the lot in square feet.\n",
    "- Overall Qual: A rating of the overall material and finish of the house (1-10).\n",
    "- Overall Cond: A rating of the overall condition of the house (1-10).\n",
    "- Year Built: The year the house was built.\n",
    "- Year Remod/Add: The year the house was remodeled or had an addition added.\n",
    "- Total Bsmt SF: The total surface of the basement, in square feet.\n",
    "- 1st Flr SF: The first floor surface, in square feet.\n",
    "- 2nd Flr SF: The second floor surface, in square feet.\n",
    "- Gr Liv Area: The above grade (ground) living area, in square feet.\n",
    "- Full Bath: The number of full bathrooms.\n",
    "- Half Bath: The number of half bathrooms.\n",
    "- Bedroom AbvGr: The number of bedrooms.\n",
    "- Kitchen AbvGr: The number of kitchens.\n",
    "- TotRms AbvGrd: The total number of rooms (does not include bathrooms).\n",
    "- Garage Area: The size of the garage, in square feet.\n",
    "- Yr Sold: The year the property was sold.\n",
    "\n",
    "These 16 features will be used as inputs, and the output will consist of just one parameter, in the final column, which is:\n",
    "\n",
    "- SalePrice: The sale price, in dollars.\n",
    "\n",
    "The dataset contains 2928 entries, and can be displayed below using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Lot Area  Overall Qual  Overall Cond  Year Built  Year Remod/Add  \\\n",
      "0     31770             6             5        1960            1960   \n",
      "1     11622             5             6        1961            1961   \n",
      "2     14267             6             6        1958            1958   \n",
      "3     11160             7             5        1968            1968   \n",
      "4     13830             5             5        1997            1998   \n",
      "\n",
      "   Total Bsmt SF  1st Flr SF  2nd Flr SF  Gr Liv Area  Full Bath  Half Bath  \\\n",
      "0           1080        1656           0         1656          1          0   \n",
      "1            882         896           0          896          1          0   \n",
      "2           1329        1329           0         1329          1          1   \n",
      "3           2110        2110           0         2110          2          1   \n",
      "4            928         928         701         1629          2          1   \n",
      "\n",
      "   Bedroom AbvGr  Kitchen AbvGr  TotRms AbvGrd  Garage Area  Yr Sold  \\\n",
      "0              3              1              7          528     2010   \n",
      "1              2              1              5          730     2010   \n",
      "2              3              1              6          312     2010   \n",
      "3              3              1              8          522     2010   \n",
      "4              3              1              6          482     2010   \n",
      "\n",
      "   SalePrice  \n",
      "0     215000  \n",
      "1     105000  \n",
      "2     172000  \n",
      "3     244000  \n",
      "4     189900  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset using pandas, and showing the first five entries\n",
    "ames_dataset = pd.read_excel(\"./ames/AmesHousing.xlsx\")\n",
    "print(ames_dataset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom Dataset class for PyTorch\n",
    "\n",
    "To write a custom dataset class in PyTorch, you should do the following:\n",
    "\n",
    "- Create a class that subclasses torch.utils.data.Dataset.\n",
    "- Define a constructor (**\\_\\_init\\_\\_**) that takes in the required arguments, and stores them as member variables.\n",
    "- Define a method **\\_\\_getitem\\_\\_** that takes an index as input and returns the data and label at that index as an array. This will allow to use the square bracket notation on our dataset object (like we would with lists).\n",
    "- Define a method **\\_\\_len\\_\\_** that returns the length of the dataset (i.e., the number of examples it contains).\n",
    "\n",
    "In the case of our custom dataset, we can do it as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmesHousingDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    #The init method will simply initialize attributes, which consist\n",
    "    # of the details related to the dataset.\n",
    "    def __init__(self, file_path = \"./ames/AmesHousing.xlsx\"):\n",
    "        # Whole data as a pandas array\n",
    "        self.data = pd.read_excel(file_path)\n",
    "        self.dataset_length = len(self.data) #2928\n",
    "        \n",
    "        # Extract inputs\n",
    "        self.input_fetaures_number = 16\n",
    "        self.input_features = self.data.iloc[:, :16]\n",
    "        \n",
    "        # Extract outputs\n",
    "        self.output_fetaures_number = 1\n",
    "        self.output_feature = self.data.iloc[:, 16]\n",
    "    \n",
    "    # The getitem method returns the sample with given index\n",
    "    # x will consist of the 16 input features for the given sample,\n",
    "    # whereas y will consist of the 1 output feature for the given sample.\n",
    "    def __getitem__(self, index):\n",
    "        # Fetch inputs\n",
    "        x = self.input_features.iloc[index].values\n",
    "        # Fetch outputs\n",
    "        y = self.output_feature.iloc[index]\n",
    "        return x, y\n",
    "    \n",
    "    # Finally, the len special method should return the number of samples,\n",
    "    # in the dataset. We could use self.dataset_length, but it is more\n",
    "    # modular to use len(self.data).\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then try it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (16,)\n",
      "[6858    6    4 1915 1950  806  841  806 1647    1    1    4    1    6\n",
      "  216 2010]\n",
      "<class 'numpy.int64'> ()\n",
      "128000\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the dataset\n",
    "ames_dataset = AmesHousingDataset('./ames/AmesHousing.xlsx')\n",
    "\n",
    "# Fetch sample with index 286\n",
    "sample_input, sample_output = ames_dataset[286]\n",
    "# Input is a (16,) numpy array, with the following values\n",
    "print(type(sample_input), sample_input.shape)\n",
    "print(sample_input)\n",
    "# Output is a single value, of type numpy int64\n",
    "print(type(sample_output), sample_output.shape)\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom dataloader\n",
    "\n",
    "Before we can feed this dataset object to Neural Networks, we need to supplement it with a Dataloader, that will shuffle the samples randomly and produce mini-batches of a given size. This Dataloader typically allows for stochastic mini-batches. The dataloader will also transform arrays into tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then generate minibatches using the for loop shown below. Notice how this creates 92 (= 2928/32, rounded up) batches of 32 samples, with the exception of the last batch, that only contains 16 samples (= 2928 % 32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the dataset\n",
    "ames_dataset = AmesHousingDataset('./ames/AmesHousing.xlsx')\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "ames_dataloader = torch.utils.data.DataLoader(ames_dataset, \\\n",
    "                                              batch_size = 32, \\\n",
    "                                              shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Batch number:  0\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  1\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  2\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  3\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  4\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  5\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  6\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  7\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  8\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  9\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  10\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  11\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  12\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  13\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  14\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  15\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  16\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  17\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  18\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  19\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  20\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  21\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  22\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  23\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  24\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  25\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  26\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  27\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  28\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  29\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  30\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  31\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  32\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  33\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  34\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  35\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  36\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  37\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  38\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  39\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  40\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  41\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  42\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  43\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  44\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  45\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  46\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  47\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  48\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  49\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  50\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  51\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  52\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  53\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  54\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  55\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  56\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  57\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  58\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  59\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  60\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  61\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  62\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  63\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  64\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  65\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  66\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  67\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  68\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  69\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  70\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  71\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  72\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  73\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  74\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  75\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  76\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  77\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  78\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  79\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  80\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  81\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  82\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  83\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  84\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  85\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  86\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  87\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  88\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  89\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  90\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "---\n",
      "Batch number:  91\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for batch_number, batch in enumerate(ames_dataloader):\n",
    "    inputs, outputs = batch\n",
    "    print(\"---\")\n",
    "    print(\"Batch number: \", batch_number)\n",
    "    print(inputs.shape)\n",
    "    print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "This concludes our PyTorch basics notebooks on how to implement custom Dataset and Dataloader objects.\n",
    "\n",
    "In the next notebooks, we will investigate a different task, which will introduce more advanced concepts, that are variations of the current ones: Deep Neural Networks and Multi-class binary classification. We will investigate this in a guided project approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
