{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PyTorch Basics - Advanced Backprop & Optimizers, Regularization, Initializers\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (27/12/2022)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)\n",
    "- Time\n",
    "- Torch (tested on v1.13.0)\n",
    "- Torchmetrics (tested on v0.11.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# Numpy\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "# OS\n",
    "import os\n",
    "# Pickle\n",
    "import pickle\n",
    "# Time\n",
    "from time import time\n",
    "# Torch\n",
    "import torch\n",
    "from torchmetrics.classification import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock dataset, with nonlinearity\n",
    "\n",
    "As in the previous notebooks, we will reuse our nonlinear binary classification mock dataset and generate a training set with 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All helper functions\n",
    "eps = 1e-5\n",
    "min_val = -1 + eps\n",
    "max_val = 1 - eps\n",
    "def val(min_val, max_val):\n",
    "    return round(np.random.uniform(min_val, max_val), 2)\n",
    "def class_for_val(val1, val2):\n",
    "    k = np.pi\n",
    "    return int(val2 >= -1/4 + 3/4*np.sin(val1*k))\n",
    "def create_dataset(n_points, min_val, max_val):\n",
    "    val1_list = np.array([val(min_val, max_val) for _ in range(n_points)])\n",
    "    val2_list = np.array([val(min_val, max_val) for _ in range(n_points)])\n",
    "    inputs = np.array([[v1, v2] for v1, v2 in zip(val1_list, val2_list)])\n",
    "    outputs = np.array([class_for_val(v1, v2) for v1, v2 in zip(val1_list, val2_list)]).reshape(n_points, 1)\n",
    "    return val1_list, val2_list, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset (train)\n",
    "np.random.seed(47)\n",
    "n_points = 1000\n",
    "train_val1_list, train_val2_list, train_inputs, train_outputs = create_dataset(n_points, min_val, max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors and send to device (CUDA or CPU)\n",
    "train_inputs_pt = torch.from_numpy(train_inputs).to(device)\n",
    "train_outputs_pt = torch.from_numpy(train_outputs).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Torch loss and accuracy functions for simplicity\n",
    "\n",
    "In order to make our model even simpler, we will use the loss functions and evaluation fucntions from PyTorch.\n",
    "\n",
    "Our **CE_loss()** and **accuracy()** methods will therefore be replaced with the **nn.BCELoss()** function and the **BinaryAccuracy()** functions.\n",
    "\n",
    "Feel free to have a look at the loss functions available in PyTorch, here: https://pytorch.org/docs/stable/nn.html#loss-functions.\n",
    "\n",
    "Torchmetrics also provides a few functions, ready to use with PyTorch: https://torchmetrics.readthedocs.io/en/stable/all-metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our class will inherit from the torch.nn.Module\n",
    "# used to write all model in PyTorch\n",
    "class ShallowNeuralNet_PT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Super __init__ for inheritance\n",
    "        super().__init__()\n",
    "        \n",
    "        # Network dimensions (as before)\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        \n",
    "        # Initialize parameters using the torch.nn.Parameter type (a subclass of Tensors).\n",
    "        # We immediatly initialize the parameters using a random normal.\n",
    "        # The RNG is done using torch.randn instead of the NumPy RNG.\n",
    "        # We add a conversion into float64 (the same float type used by Numpy to generate our data)\n",
    "        # And send them to our GPU/CPU device\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(n_x, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(1, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(n_h, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(1, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W1.retain_grad()\n",
    "        self.b1.retain_grad()\n",
    "        self.W2.retain_grad()\n",
    "        self.b2.retain_grad()\n",
    "        \n",
    "        # Loss and accuracy functions\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Instead of using np.matmul(), we use its equivalent in PyTorch,\n",
    "        # which is torch.matmul()!\n",
    "        # (Most numpy matrix operations ahve their equivalent in torch, check it out!)\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = torch.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Sigmoid is already implemented in PyTorch, feel fre to reuse it!\n",
    "        A1 = torch.sigmoid(Z1_b)\n",
    "        \n",
    "        # Wx + b operation for the second layer\n",
    "        # (Same as first layer)\n",
    "        Z2 = torch.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = torch.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1):\n",
    "        # History of losses\n",
    "        self.loss_history = []\n",
    "        # Repeat gradient descent procedure for N_max iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Forward pass\n",
    "            # This is equivalent to pred = self.forward(inputs)\n",
    "            pred = self(inputs)\n",
    "            # Compute loss\n",
    "            loss_val = self.loss(pred, outputs.to(torch.float64))\n",
    "            self.loss_history.append(loss_val.item())\n",
    "\n",
    "            # Backpropagate\n",
    "            # Compute differentiation of loss with respect to all\n",
    "            # parameters involved in the calculation that have a flag\n",
    "            # requires_grad = True (that is W2, W1, b2 and b1)\n",
    "            loss_val.backward()\n",
    "\n",
    "            # Update all weights\n",
    "            # Note that this operation should not be tracked for gradients,\n",
    "            # hence the torch.no_grad()!\n",
    "            with torch.no_grad():\n",
    "                self.W1 -= alpha*self.W1.grad\n",
    "                self.W2 -= alpha*self.W2.grad\n",
    "                self.b1 -= alpha*self.b1.grad\n",
    "                self.b2 -= alpha*self.b2.grad\n",
    "\n",
    "            # Reset gradients to 0\n",
    "            self.W1.grad.zero_()\n",
    "            self.W2.grad.zero_()\n",
    "            self.b1.grad.zero_()\n",
    "            self.W2.grad.zero_()\n",
    "            \n",
    "            # Display\n",
    "            if(iteration_number % (N_max//20) == 1):\n",
    "                # Compute accuracy for display\n",
    "                acc_val = self.accuracy(pred, outputs)\n",
    "                print(\"Iteration {} - Loss = {} - Accuracy = {}\".format(iteration_number, loss_val.item(), acc_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Loss = 0.6815893678844995 - Accuracy = 0.6259999871253967\n",
      "Iteration 51 - Loss = 0.26856284262640595 - Accuracy = 0.875\n",
      "Iteration 101 - Loss = 0.2554499223809305 - Accuracy = 0.878000020980835\n",
      "Iteration 151 - Loss = 0.2103705015487559 - Accuracy = 0.9020000100135803\n",
      "Iteration 201 - Loss = 0.17958287411376456 - Accuracy = 0.9129999876022339\n",
      "Iteration 251 - Loss = 0.15927524680187605 - Accuracy = 0.9279999732971191\n",
      "Iteration 301 - Loss = 0.14429291254439877 - Accuracy = 0.9380000233650208\n",
      "Iteration 351 - Loss = 0.13263530282019223 - Accuracy = 0.9470000267028809\n",
      "Iteration 401 - Loss = 0.12317784533839293 - Accuracy = 0.9520000219345093\n",
      "Iteration 451 - Loss = 0.11523437134002097 - Accuracy = 0.9599999785423279\n",
      "Iteration 501 - Loss = 0.10832495056501382 - Accuracy = 0.9629999995231628\n",
      "Iteration 551 - Loss = 0.10205194807838043 - Accuracy = 0.9700000286102295\n",
      "Iteration 601 - Loss = 0.09608766375853714 - Accuracy = 0.972000002861023\n",
      "Iteration 651 - Loss = 0.09022613737409999 - Accuracy = 0.9739999771118164\n",
      "Iteration 701 - Loss = 0.0844319584119672 - Accuracy = 0.9739999771118164\n",
      "Iteration 751 - Loss = 0.07881339959056509 - Accuracy = 0.9760000109672546\n",
      "Iteration 801 - Loss = 0.07352591390993116 - Accuracy = 0.9760000109672546\n",
      "Iteration 851 - Loss = 0.06868647548294009 - Accuracy = 0.9789999723434448\n",
      "Iteration 901 - Loss = 0.06434477920221017 - Accuracy = 0.9800000190734863\n",
      "Iteration 951 - Loss = 0.06049534252291416 - Accuracy = 0.9810000061988831\n",
      "Iteration 1001 - Loss = 0.057100707346842375 - Accuracy = 0.9819999933242798\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network structure\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_pt = ShallowNeuralNet_PT(n_x, n_h, n_y).to(device)\n",
    "train_pred = shallow_neural_net_pt.train(train_inputs_pt, train_outputs_pt, N_max = 1001, alpha = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9819999933242798\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy after training\n",
    "acc = shallow_neural_net_pt.accuracy(shallow_neural_net_pt(train_inputs_pt), train_outputs_pt).item()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced backpropagation and optimizers\n",
    "\n",
    "We can also define some advanced optimizers, e.g. Adam, as below.\n",
    "\n",
    "Feel free to have a look at all the available optimizers, here: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "Three modifications are to be considered to use Adam instead of the Vanilla gradient descent rule.\n",
    "\n",
    "1. **Adam** has been added as an optimizer and its parameters can be passed to the train() method.\n",
    "\n",
    "```\n",
    "# Optimizer\n",
    "# You can use self.parameters() to get the list of parameters for the model\n",
    "# self.parameters() is therefore equivalent to [self.W1, self.b1, self.W2, self.b2]\n",
    "optimizer = torch.optim.Adam(self.parameters(), # Parameters to be updated by gradient rule\n",
    "                                 lr = alpha, # Learning rate\n",
    "                                 betas = (beta1, beta2), # Betas used in Adam rules for V and S\n",
    "                                 eps = 1e-08) # Epsilon value used in normalization\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "2. **Optimizer step** must be performed to update the V and S parameters in Adam. This also replaces the gradient rule update entirely (damn!).\n",
    "\n",
    "```\n",
    "# Update all weights and optimizer step (will update the V \n",
    "# and S parameters in Adam) all at once!\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "3. **Reset gradients in optimizer to 0**, like you would in the parameters tensors. This replaces all four self.Xx.grad.zero_() operations!\n",
    "```\n",
    "# Reset gradients to 0\n",
    "optimizer.zero_grad()\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our class will inherit from the torch.nn.Module\n",
    "# used to write all model in PyTorch\n",
    "class ShallowNeuralNet_PT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Super __init__ for inheritance\n",
    "        super().__init__()\n",
    "        \n",
    "        # Network dimensions (as before)\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        \n",
    "        # Initialize parameters using the torch.nn.Parameter type (a subclass of Tensors).\n",
    "        # We immediatly initialize the parameters using a random normal.\n",
    "        # The RNG is done using torch.randn instead of the NumPy RNG.\n",
    "        # We add a conversion into float64 (the same float type used by Numpy to generate our data)\n",
    "        # And send them to our GPU/CPU device\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(n_x, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(1, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(n_h, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(1, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W1.retain_grad()\n",
    "        self.b1.retain_grad()\n",
    "        self.W2.retain_grad()\n",
    "        self.b2.retain_grad()\n",
    "        \n",
    "        # Loss and accuracy functions\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Instead of using np.matmul(), we use its equivalent in PyTorch,\n",
    "        # which is torch.matmul()!\n",
    "        # (Most numpy matrix operations ahve their equivalent in torch, check it out!)\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = torch.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Sigmoid is already implemented in PyTorch, feel fre to reuse it!\n",
    "        A1 = torch.sigmoid(Z1_b)\n",
    "        \n",
    "        # Wx + b operation for the second layer\n",
    "        # (Same as first layer)\n",
    "        Z2 = torch.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = torch.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1, beta1 = 0.9, beta2 = 0.999):\n",
    "        # Optimizer\n",
    "        # You can use self.parameters() to get the list of parameters for the model\n",
    "        # self.parameters() is therefore equivalent to [self.W1, self.b1, self.W2, self.b2]\n",
    "        optimizer = torch.optim.Adam(self.parameters(), # Parameters to be updated by gradient rule\n",
    "                                     lr = alpha, # Learning rate\n",
    "                                     betas = (beta1, beta2), # Betas used in Adam rules for V and S\n",
    "                                     eps = 1e-08) # Epsilon value used in normalization\n",
    "        optimizer.zero_grad()\n",
    "        # History of losses\n",
    "        self.loss_history = []\n",
    "        # Repeat gradient descent procedure for N_max iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Forward pass\n",
    "            # This is equivalent to pred = self.forward(inputs)\n",
    "            pred = self(inputs)\n",
    "            # Compute loss\n",
    "            loss_val = self.loss(pred, outputs.to(torch.float64))\n",
    "            self.loss_history.append(loss_val.item())\n",
    "\n",
    "            # Backpropagate\n",
    "            # Compute differentiation of loss with respect to all\n",
    "            # parameters involved in the calculation that have a flag\n",
    "            # requires_grad = True (that is W2, W1, b2 and b1)\n",
    "            loss_val.backward()\n",
    "\n",
    "            # Update all weights and optimizer step (will update the V \n",
    "            # and S parameters in Adam) all at once!\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Reset gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Display\n",
    "            if(iteration_number % (N_max//20) == 1):\n",
    "                # Compute accuracy for display\n",
    "                acc_val = self.accuracy(pred, outputs)\n",
    "                print(\"Iteration {} - Loss = {} - Accuracy = {}\".format(iteration_number, loss_val.item(), acc_val.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam allows for a much faster convergence during training now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Loss = 0.6963595490783729 - Accuracy = 0.37400001287460327\n",
      "Iteration 51 - Loss = 0.27495575265733646 - Accuracy = 0.8730000257492065\n",
      "Iteration 101 - Loss = 0.25407400607104663 - Accuracy = 0.8659999966621399\n",
      "Iteration 151 - Loss = 0.25298914524563143 - Accuracy = 0.8659999966621399\n",
      "Iteration 201 - Loss = 0.22908243002852788 - Accuracy = 0.8849999904632568\n",
      "Iteration 251 - Loss = 0.05799776142957951 - Accuracy = 0.9789999723434448\n",
      "Iteration 301 - Loss = 0.023912304350808676 - Accuracy = 0.9959999918937683\n",
      "Iteration 351 - Loss = 0.019605317364312973 - Accuracy = 0.9959999918937683\n",
      "Iteration 401 - Loss = 0.01757331994992764 - Accuracy = 0.9959999918937683\n",
      "Iteration 451 - Loss = 0.01629953531239529 - Accuracy = 0.9959999918937683\n",
      "Iteration 501 - Loss = 0.015403979247254736 - Accuracy = 0.9959999918937683\n",
      "Iteration 551 - Loss = 0.014727827297406149 - Accuracy = 0.9959999918937683\n",
      "Iteration 601 - Loss = 0.014191901532099892 - Accuracy = 0.9959999918937683\n",
      "Iteration 651 - Loss = 0.013752175036052902 - Accuracy = 0.9959999918937683\n",
      "Iteration 701 - Loss = 0.013381920880170714 - Accuracy = 0.9959999918937683\n",
      "Iteration 751 - Loss = 0.013063772098980669 - Accuracy = 0.9959999918937683\n",
      "Iteration 801 - Loss = 0.012785907674362686 - Accuracy = 0.9959999918937683\n",
      "Iteration 851 - Loss = 0.012540014739541182 - Accuracy = 0.9959999918937683\n",
      "Iteration 901 - Loss = 0.012320081324915643 - Accuracy = 0.9959999918937683\n",
      "Iteration 951 - Loss = 0.01212162833277035 - Accuracy = 0.9959999918937683\n",
      "Iteration 1001 - Loss = 0.011941197065168975 - Accuracy = 0.9959999918937683\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network structure\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_pt = ShallowNeuralNet_PT(n_x, n_h, n_y).to(device)\n",
    "train_pred = shallow_neural_net_pt.train(train_inputs_pt, train_outputs_pt, N_max = 1001, \\\n",
    "                                         alpha = 1, beta1 = 0.9, beta2 = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9959999918937683\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy after training\n",
    "acc = shallow_neural_net_pt.accuracy(shallow_neural_net_pt(train_inputs_pt), train_outputs_pt).item()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Mini-Batches\n",
    "\n",
    "We can also define Stochastic Mini-Batches as shown below. This is done with two modifications.\n",
    "\n",
    "1. **Create a Dataloader** using the inputs and outputs provided in the train(). We will leanr more about these Dataset and Dataloader objects in the next notebook. For now, just consider that it allows to conveniently zip the data in an object that is able to shuffle and draw randomly mini-batches of data for us.\n",
    "\n",
    "```\n",
    "# Create a PyTorch dataset object from the input and output data\n",
    "dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "# Create a PyTorch DataLoader object from the dataset, with the specified batch size\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "```\n",
    "\n",
    "2. **Loop over the mini-batches of data**, instead of using the entire inputs/outputs at once, like in batch gradient descent.\n",
    "\n",
    "```\n",
    "# Loop over each mini-batch of data\n",
    "for batch in data_loader:\n",
    "    # Unpack the mini-batch data\n",
    "    x_batch, y_batch = batch\n",
    "    # Forward pass\n",
    "    # This is equivalent to pred = self.forward(inputs)\n",
    "    pred = self(x_batch)\n",
    "    # Compute loss\n",
    "    loss_val = self.loss(pred, y_batch.to(torch.float64))\n",
    "    self.loss_history.append(loss_val.item())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our class will inherit from the torch.nn.Module\n",
    "# used to write all model in PyTorch\n",
    "class ShallowNeuralNet_PT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Super __init__ for inheritance\n",
    "        super().__init__()\n",
    "        \n",
    "        # Network dimensions (as before)\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        \n",
    "        # Initialize parameters using the torch.nn.Parameter type (a subclass of Tensors).\n",
    "        # We immediatly initialize the parameters using a random normal.\n",
    "        # The RNG is done using torch.randn instead of the NumPy RNG.\n",
    "        # We add a conversion into float64 (the same float type used by Numpy to generate our data)\n",
    "        # And send them to our GPU/CPU device\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(n_x, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(1, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(n_h, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(1, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W1.retain_grad()\n",
    "        self.b1.retain_grad()\n",
    "        self.W2.retain_grad()\n",
    "        self.b2.retain_grad()\n",
    "        \n",
    "        # Loss and accuracy functions\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Instead of using np.matmul(), we use its equivalent in PyTorch,\n",
    "        # which is torch.matmul()!\n",
    "        # (Most numpy matrix operations ahve their equivalent in torch, check it out!)\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = torch.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Sigmoid is already implemented in PyTorch, feel fre to reuse it!\n",
    "        A1 = torch.sigmoid(Z1_b)\n",
    "        \n",
    "        # Wx + b operation for the second layer\n",
    "        # (Same as first layer)\n",
    "        Z2 = torch.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = torch.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1, beta1 = 0.9, beta2 = 0.999, batch_size = 32):\n",
    "        # Create a PyTorch dataset object from the input and output data\n",
    "        dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "        # Create a PyTorch DataLoader object from the dataset, with the specified batch size\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "        # Optimizer\n",
    "        # You can use self.parameters() to get the list of parameters for the model\n",
    "        # self.parameters() is therefore equivalent to [self.W1, self.b1, self.W2, self.b2]\n",
    "        optimizer = torch.optim.Adam(self.parameters(), # Parameters to be updated by gradient rule\n",
    "                                     lr = alpha, # Learning rate\n",
    "                                     betas = (beta1, beta2), # Betas used in Adam rules for V and S\n",
    "                                     eps = 1e-08) # Epsilon value used in normalization\n",
    "        optimizer.zero_grad()\n",
    "        # History of losses\n",
    "        self.loss_history = []\n",
    "        # Repeat gradient descent procedure for N_max iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Loop over each mini-batch of data\n",
    "            for batch in data_loader:\n",
    "                # Unpack the mini-batch data\n",
    "                inputs_batch, outputs_batch = batch\n",
    "                \n",
    "                # Forward pass\n",
    "                # This is equivalent to pred = self.forward(inputs)\n",
    "                pred = self(inputs_batch)\n",
    "                # Compute loss\n",
    "                loss_val = self.loss(pred, outputs_batch.to(torch.float64))\n",
    "                self.loss_history.append(loss_val.item())\n",
    "\n",
    "                # Backpropagate\n",
    "                # Compute differentiation of loss with respect to all\n",
    "                # parameters involved in the calculation that have a flag\n",
    "                # requires_grad = True (that is W2, W1, b2 and b1)\n",
    "                loss_val.backward()\n",
    "\n",
    "                # Update all weights and optimizer step (will update the V \n",
    "                # and S parameters in Adam) all at once!\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset gradients to 0\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Display\n",
    "            if(iteration_number % (N_max//20) == 1):\n",
    "                # Compute accuracy for display\n",
    "                pred = self(inputs)\n",
    "                acc_val = self.accuracy(pred, outputs)\n",
    "                print(\"Iteration {} - Loss = {} - Accuracy = {}\".format(iteration_number, loss_val.item(), acc_val.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mini-batches allows for a much faster convergence (only 250 iterations needed instead of 1000 before!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Loss = 0.15743604845979645 - Accuracy = 0.8180000185966492\n",
      "Iteration 13 - Loss = 0.02051996240903821 - Accuracy = 0.9729999899864197\n",
      "Iteration 25 - Loss = 0.007127222721020081 - Accuracy = 0.9710000157356262\n",
      "Iteration 37 - Loss = 0.005443105633340545 - Accuracy = 0.972000002861023\n",
      "Iteration 49 - Loss = 0.013696338146403067 - Accuracy = 0.9879999756813049\n",
      "Iteration 61 - Loss = 0.016204086614336637 - Accuracy = 0.9729999899864197\n",
      "Iteration 73 - Loss = 0.008053626944567276 - Accuracy = 0.9769999980926514\n",
      "Iteration 85 - Loss = 0.3933300796055345 - Accuracy = 0.9729999899864197\n",
      "Iteration 97 - Loss = 0.001591107536451724 - Accuracy = 0.9800000190734863\n",
      "Iteration 109 - Loss = 0.035385071610661935 - Accuracy = 0.984000027179718\n",
      "Iteration 121 - Loss = 0.6749003095139646 - Accuracy = 0.9900000095367432\n",
      "Iteration 133 - Loss = 0.0028621269205506275 - Accuracy = 0.9819999933242798\n",
      "Iteration 145 - Loss = 0.004587757293009814 - Accuracy = 0.984000027179718\n",
      "Iteration 157 - Loss = 0.0021766402531936645 - Accuracy = 0.9620000123977661\n",
      "Iteration 169 - Loss = 0.4635701641106967 - Accuracy = 0.9900000095367432\n",
      "Iteration 181 - Loss = 0.29041910556272604 - Accuracy = 0.984000027179718\n",
      "Iteration 193 - Loss = 0.00814629956220964 - Accuracy = 0.9869999885559082\n",
      "Iteration 205 - Loss = 0.015330424595706039 - Accuracy = 0.9929999709129333\n",
      "Iteration 217 - Loss = 0.012632367002025283 - Accuracy = 0.9829999804496765\n",
      "Iteration 229 - Loss = 0.013756329626939016 - Accuracy = 0.984000027179718\n",
      "Iteration 241 - Loss = 0.5454397356642312 - Accuracy = 0.9860000014305115\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network structure\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_pt = ShallowNeuralNet_PT(n_x, n_h, n_y).to(device)\n",
    "train_pred = shallow_neural_net_pt.train(train_inputs_pt, train_outputs_pt, N_max = 250, \\\n",
    "                                         alpha = 1, beta1 = 0.9, beta2 = 0.999, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.953000009059906\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy after training\n",
    "acc = shallow_neural_net_pt.accuracy(shallow_neural_net_pt(train_inputs_pt), train_outputs_pt).item()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a regularization term to the loss function\n",
    "\n",
    "Another interesting concept is that we can add a regularization term to the loss function very easily. Not that it is necessary here, but if we wanted to, here is how we would do it.\n",
    "\n",
    "Our first step would be to simply compute our regularization term by using the PyTorch functions, for instance the L1 loss as follows:\n",
    "```\n",
    "L1_reg = lambda_l1*sum(torch.abs(param).sum() for param in self.parameters())\n",
    "```\n",
    "\n",
    "We would then simply add it to the loss before backpropagating.\n",
    "\n",
    "```\n",
    "# Compute loss and regularization term\n",
    "loss_val = self.loss(pred, outputs_batch.to(torch.float64))\n",
    "L1_reg = lambda_l1*sum(torch.abs(param).sum() for param in self.parameters())\n",
    "total_loss = loss_val + L1_reg\n",
    "self.loss_history.append(total_loss.item())\n",
    "\n",
    "# Backpropagate\n",
    "# Compute differentiation of loss with respect to all\n",
    "# parameters involved in the calculation that have a flag\n",
    "# requires_grad = True (that is W2, W1, b2 and b1)\n",
    "total_loss.backward()\n",
    "```\n",
    "\n",
    "That is it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our class will inherit from the torch.nn.Module\n",
    "# used to write all model in PyTorch\n",
    "class ShallowNeuralNet_PT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Super __init__ for inheritance\n",
    "        super().__init__()\n",
    "        \n",
    "        # Network dimensions (as before)\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        \n",
    "        # Initialize parameters using the torch.nn.Parameter type (a subclass of Tensors).\n",
    "        # We use xavier_uniform_ initialization.\n",
    "        W1 = torch.zeros(size = (n_x, n_h), requires_grad = True, \\\n",
    "                         dtype = torch.float64, device = device)\n",
    "        torch.nn.init.xavier_uniform_(W1.data)\n",
    "        self.W1 = torch.nn.Parameter(W1).retain_grad()\n",
    "        b1 = torch.zeros(size = (1, n_h), requires_grad = True, \\\n",
    "                         dtype = torch.float64, device = device)\n",
    "        torch.nn.init.xavier_uniform_(b1.data)\n",
    "        self.b1 = torch.nn.Parameter(b1).retain_grad()\n",
    "        W2 = torch.zeros(size = (n_h, n_y), requires_grad = True, \\\n",
    "                         dtype = torch.float64, device = device)\n",
    "        torch.nn.init.xavier_uniform_(W2.data)\n",
    "        self.W2 = torch.nn.Parameter(W2).retain_grad()\n",
    "        b2 = torch.zeros(size = (1, n_y), requires_grad = True, \\\n",
    "                         dtype = torch.float64, device = device)\n",
    "        torch.nn.init.xavier_uniform_(b2.data)\n",
    "        self.b2 = torch.nn.Parameter(b2).retain_grad()\n",
    "        \n",
    "        # Loss and accuracy functions\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Instead of using np.matmul(), we use its equivalent in PyTorch,\n",
    "        # which is torch.matmul()!\n",
    "        # (Most numpy matrix operations ahve their equivalent in torch, check it out!)\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = torch.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Sigmoid is already implemented in PyTorch, feel fre to reuse it!\n",
    "        A1 = torch.sigmoid(Z1_b)\n",
    "        \n",
    "        # Wx + b operation for the second layer\n",
    "        # (Same as first layer)\n",
    "        Z2 = torch.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = torch.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1, beta1 = 0.9, beta2 = 0.999, \\\n",
    "              batch_size = 32, lambda_val = 1e-3):\n",
    "        # Create a PyTorch dataset object from the input and output data\n",
    "        dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "        # Create a PyTorch DataLoader object from the dataset, with the specified batch size\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "        # Optimizer\n",
    "        # You can use self.parameters() to get the list of parameters for the model\n",
    "        # self.parameters() is therefore equivalent to [self.W1, self.b1, self.W2, self.b2]\n",
    "        optimizer = torch.optim.Adam(self.parameters(), # Parameters to be updated by gradient rule\n",
    "                                     lr = alpha, # Learning rate\n",
    "                                     betas = (beta1, beta2), # Betas used in Adam rules for V and S\n",
    "                                     eps = 1e-08) # Epsilon value used in normalization\n",
    "        optimizer.zero_grad()\n",
    "        # History of losses\n",
    "        self.loss_history = []\n",
    "        # Repeat gradient descent procedure for N_max iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Loop over each mini-batch of data\n",
    "            for batch in data_loader:\n",
    "                # Unpack the mini-batch data\n",
    "                inputs_batch, outputs_batch = batch\n",
    "                \n",
    "                # Forward pass\n",
    "                # This is equivalent to pred = self.forward(inputs)\n",
    "                pred = self(inputs_batch)\n",
    "                # Compute loss and regularization term\n",
    "                loss_val = self.loss(pred, outputs_batch.to(torch.float64))\n",
    "                L1_reg = lambda_val*sum(torch.abs(param).sum() for param in self.parameters())\n",
    "                total_loss = loss_val + L1_reg\n",
    "                self.loss_history.append(total_loss.item())\n",
    "\n",
    "                # Backpropagate\n",
    "                # Compute differentiation of loss with respect to all\n",
    "                # parameters involved in the calculation that have a flag\n",
    "                # requires_grad = True (that is W2, W1, b2 and b1)\n",
    "                total_loss.backward()\n",
    "\n",
    "                # Update all weights and optimizer step (will update the V \n",
    "                # and S parameters in Adam) all at once!\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset gradients to 0\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Display\n",
    "            if(iteration_number % (N_max//20) == 1):\n",
    "                # Compute accuracy for display\n",
    "                pred = self(inputs)\n",
    "                acc_val = self.accuracy(pred, outputs)\n",
    "                print(\"Iteration {} - Loss = {} - Accuracy = {}\".format(iteration_number, \\\n",
    "                                                                        total_loss.item(), \\\n",
    "                                                                        acc_val.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization has an impact on training. Feel free to play with the value of lambda to see its effect on training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter.__new__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m n_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m37\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m shallow_neural_net_pt \u001b[38;5;241m=\u001b[39m \u001b[43mShallowNeuralNet_PT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_y\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m shallow_neural_net_pt\u001b[38;5;241m.\u001b[39mtrain(train_inputs_pt, train_outputs_pt, N_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m, \\\n\u001b[0;32m      8\u001b[0m                                          alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, beta1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m, beta2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, lambda_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[81], line 26\u001b[0m, in \u001b[0;36mShallowNeuralNet_PT.__init__\u001b[1;34m(self, n_x, n_h, n_y)\u001b[0m\n\u001b[0;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mxavier_uniform_(W1\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(W1)\u001b[38;5;241m.\u001b[39mretain_grad()\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_h\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(size \u001b[38;5;241m=\u001b[39m (n_h, n_y), requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \\\n\u001b[0;32m     29\u001b[0m                              dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat64, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, n_y), requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \\\n\u001b[0;32m     31\u001b[0m                              dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat64, device \u001b[38;5;241m=\u001b[39m device)\n",
      "\u001b[1;31mTypeError\u001b[0m: Parameter.__new__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "# Define a neural network structure\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_pt = ShallowNeuralNet_PT(n_x, n_h, n_y).to(device)\n",
    "train_pred = shallow_neural_net_pt.train(train_inputs_pt, train_outputs_pt, N_max = 250, \\\n",
    "                                         alpha = 1, beta1 = 0.9, beta2 = 0.999, batch_size = 32, lambda_val = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy after training\n",
    "acc = shallow_neural_net_pt.accuracy(shallow_neural_net_pt(train_inputs_pt), train_outputs_pt).item()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding initializers to the model\n",
    "\n",
    "Finally, the part below, is a bit tedious.\n",
    "```\n",
    "# Initialize parameters using the torch.nn.Parameter type (a subclass of Tensors).\n",
    "# We immediatly initialize the parameters using a random normal.\n",
    "# The RNG is done using torch.randn instead of the NumPy RNG.\n",
    "# We add a conversion into float64 (the same float type used by Numpy to generate our data)\n",
    "# And send them to our GPU/CPU device\n",
    "self.W1 = torch.nn.Parameter(torch.randn(n_x, n_h, requires_grad = True, \\\n",
    "                             dtype = torch.float64, device = device)*0.1)\n",
    "self.b1 = torch.nn.Parameter(torch.randn(1, n_h, requires_grad = True, \\\n",
    "                             dtype = torch.float64, device = device)*0.1)\n",
    "self.W2 = torch.nn.Parameter(torch.randn(n_h, n_y, requires_grad = True, \\\n",
    "                             dtype = torch.float64, device = device)*0.1)\n",
    "self.b2 = torch.nn.Parameter(torch.randn(1, n_y, requires_grad = True, \\\n",
    "                             dtype = torch.float64, device = device)*0.1)\n",
    "self.W1.retain_grad()\n",
    "self.b1.retain_grad()\n",
    "self.W2.retain_grad()\n",
    "self.b2.retain_grad()\n",
    "```\n",
    "\n",
    "We would love to replace it with something a bit simpler, which allows the user to choose how to initialize said parameters (by using Xavier, LeCun, He, etc.).\n",
    "\n",
    "This can be done, by using functions from the torch.nn.init, e.g. the xavier_uniform_() one.\n",
    "\n",
    "```\n",
    "# Initialize parameters using the torch.nn.Parameter type (a subclass of Tensors).\n",
    "# We use xavier_uniform_ initialization.\n",
    "W1 = torch.zeros(size = (n_x, n_h), requires_grad = True, \\\n",
    "                 dtype = torch.float64, device = device)\n",
    "torch.nn.init.xavier_uniform_(W1.data)\n",
    "self.W1 = torch.nn.Parameter(W1).retain_grad()\n",
    "b1 = torch.zeros(size = (1, n_h), requires_grad = True, \\\n",
    "                 dtype = torch.float64, device = device)\n",
    "torch.nn.init.xavier_uniform_(b1.data)\n",
    "self.b1 = torch.nn.Parameter(b1).retain_grad()\n",
    "W2 = torch.zeros(size = (n_h, n_y), requires_grad = True, \\\n",
    "                 dtype = torch.float64, device = device)\n",
    "torch.nn.init.xavier_uniform_(W2.data)\n",
    "self.W2 = torch.nn.Parameter(W2).retain_grad()\n",
    "b2 = torch.zeros(size = (1, n_y), requires_grad = True, \\\n",
    "                 dtype = torch.float64, device = device)\n",
    "torch.nn.init.xavier_uniform_(b2.data)\n",
    "self.b2 = torch.nn.Parameter(b2).retain_grad()\n",
    "```\n",
    "\n",
    "Note that this is still a very unstable feature, which might be added in later version of PyTorch, so stay tuned! And have a look at this for additional initializers: https://pytorch.org/cppdocs/api/file_torch_csrc_api_include_torch_nn_init.h.html#file-torch-csrc-api-include-torch-nn-init-h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our class will inherit from the torch.nn.Module\n",
    "# used to write all model in PyTorch\n",
    "class ShallowNeuralNet_PT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Super __init__ for inheritance\n",
    "        super().__init__()\n",
    "        \n",
    "        # Network dimensions (as before)\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        \n",
    "        # Initialize parameters using the torch.nn.Parameter type (a subclass of Tensors).\n",
    "        # We immediatly initialize the parameters using a random normal.\n",
    "        # The RNG is done using torch.randn instead of the NumPy RNG.\n",
    "        # We add a conversion into float64 (the same float type used by Numpy to generate our data)\n",
    "        # And send them to our GPU/CPU device\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(n_x, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(1, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(n_h, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(1, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W1.retain_grad()\n",
    "        self.b1.retain_grad()\n",
    "        self.W2.retain_grad()\n",
    "        self.b2.retain_grad()\n",
    "        \n",
    "        # Loss and accuracy functions\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Instead of using np.matmul(), we use its equivalent in PyTorch,\n",
    "        # which is torch.matmul()!\n",
    "        # (Most numpy matrix operations ahve their equivalent in torch, check it out!)\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = torch.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Sigmoid is already implemented in PyTorch, feel fre to reuse it!\n",
    "        A1 = torch.sigmoid(Z1_b)\n",
    "        \n",
    "        # Wx + b operation for the second layer\n",
    "        # (Same as first layer)\n",
    "        Z2 = torch.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = torch.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def train(self, inputs, outputs, N_max = 1000, alpha = 1, beta1 = 0.9, beta2 = 0.999, \\\n",
    "              batch_size = 32, lambda_val = 1e-3):\n",
    "        # Create a PyTorch dataset object from the input and output data\n",
    "        dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "        # Create a PyTorch DataLoader object from the dataset, with the specified batch size\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "        # Optimizer\n",
    "        # You can use self.parameters() to get the list of parameters for the model\n",
    "        # self.parameters() is therefore equivalent to [self.W1, self.b1, self.W2, self.b2]\n",
    "        optimizer = torch.optim.Adam(self.parameters(), # Parameters to be updated by gradient rule\n",
    "                                     lr = alpha, # Learning rate\n",
    "                                     betas = (beta1, beta2), # Betas used in Adam rules for V and S\n",
    "                                     eps = 1e-08) # Epsilon value used in normalization\n",
    "        optimizer.zero_grad()\n",
    "        # History of losses\n",
    "        self.loss_history = []\n",
    "        # Repeat gradient descent procedure for N_max iterations\n",
    "        for iteration_number in range(1, N_max + 1):\n",
    "            # Loop over each mini-batch of data\n",
    "            for batch in data_loader:\n",
    "                # Unpack the mini-batch data\n",
    "                inputs_batch, outputs_batch = batch\n",
    "                \n",
    "                # Forward pass\n",
    "                # This is equivalent to pred = self.forward(inputs)\n",
    "                pred = self(inputs_batch)\n",
    "                # Compute loss and regularization term\n",
    "                loss_val = self.loss(pred, outputs_batch.to(torch.float64))\n",
    "                L1_reg = lambda_val*sum(torch.abs(param).sum() for param in self.parameters())\n",
    "                total_loss = loss_val + L1_reg\n",
    "                self.loss_history.append(total_loss.item())\n",
    "\n",
    "                # Backpropagate\n",
    "                # Compute differentiation of loss with respect to all\n",
    "                # parameters involved in the calculation that have a flag\n",
    "                # requires_grad = True (that is W2, W1, b2 and b1)\n",
    "                total_loss.backward()\n",
    "\n",
    "                # Update all weights and optimizer step (will update the V \n",
    "                # and S parameters in Adam) all at once!\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset gradients to 0\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Display\n",
    "            if(iteration_number % (N_max//20) == 1):\n",
    "                # Compute accuracy for display\n",
    "                pred = self(inputs)\n",
    "                acc_val = self.accuracy(pred, outputs)\n",
    "                print(\"Iteration {} - Loss = {} - Accuracy = {}\".format(iteration_number, \\\n",
    "                                                                        total_loss.item(), \\\n",
    "                                                                        acc_val.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization has an impact on training. Feel free to play with the value of lambda to see its effect on training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Loss = 0.08475216810090405 - Accuracy = 0.8619999885559082\n",
      "Iteration 13 - Loss = 0.05901462414011047 - Accuracy = 0.9769999980926514\n",
      "Iteration 25 - Loss = 0.016450984651784465 - Accuracy = 0.9829999804496765\n",
      "Iteration 37 - Loss = 0.008222233924491921 - Accuracy = 0.984000027179718\n",
      "Iteration 49 - Loss = 0.13328321891314257 - Accuracy = 0.9800000190734863\n",
      "Iteration 61 - Loss = 0.009147019171303892 - Accuracy = 0.9810000061988831\n",
      "Iteration 73 - Loss = 0.44096735252082253 - Accuracy = 0.9769999980926514\n",
      "Iteration 85 - Loss = 0.009082679657959317 - Accuracy = 0.9819999933242798\n",
      "Iteration 97 - Loss = 0.07022150526787058 - Accuracy = 0.972000002861023\n",
      "Iteration 109 - Loss = 0.007796034371179035 - Accuracy = 0.9729999899864197\n",
      "Iteration 121 - Loss = 0.03185199986167074 - Accuracy = 0.984000027179718\n",
      "Iteration 133 - Loss = 0.10836565844049802 - Accuracy = 0.9269999861717224\n",
      "Iteration 145 - Loss = 0.03817443400090914 - Accuracy = 0.9559999704360962\n",
      "Iteration 157 - Loss = 0.010279585621843343 - Accuracy = 0.984000027179718\n",
      "Iteration 169 - Loss = 0.5916096737236146 - Accuracy = 0.9919999837875366\n",
      "Iteration 181 - Loss = 0.012822819031748301 - Accuracy = 0.9789999723434448\n",
      "Iteration 193 - Loss = 0.9925352465459178 - Accuracy = 0.9800000190734863\n",
      "Iteration 205 - Loss = 1.804276033704204 - Accuracy = 0.9829999804496765\n",
      "Iteration 217 - Loss = 0.013485143009677014 - Accuracy = 0.9739999771118164\n",
      "Iteration 229 - Loss = 0.04515166876518076 - Accuracy = 0.9789999723434448\n",
      "Iteration 241 - Loss = 0.0566910497685566 - Accuracy = 0.9670000076293945\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network structure\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_pt = ShallowNeuralNet_PT(n_x, n_h, n_y).to(device)\n",
    "train_pred = shallow_neural_net_pt.train(train_inputs_pt, train_outputs_pt, N_max = 250, \\\n",
    "                                         alpha = 1, beta1 = 0.9, beta2 = 0.999, batch_size = 32, lambda_val = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy after training\n",
    "acc = shallow_neural_net_pt.accuracy(shallow_neural_net_pt(train_inputs_pt), train_outputs_pt).item()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "This concludes our PyTorch basics notebooks on how to implement a Shallow Neural Network in PyTorch for binar classification.\n",
    "\n",
    "In the next notebooks, we will investigate a different task, which will introduce more advanced concepts, that are variations of the current ones: Deep Neural Networks and Multi-class binary classification. We will investigate this in a guided project approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
