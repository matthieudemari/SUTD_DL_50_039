{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PyTorch Basics - Writing a PyTorch Neural Network with Forward Propagation\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (27/12/2022)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)\n",
    "- Time\n",
    "- Torch (tested on v1.13.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# Numpy\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "# Time\n",
    "from time import time\n",
    "# Torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock dataset, with nonlinearity\n",
    "\n",
    "As in the previous notebooks, we will reuse our nonlinear binary classification mock dataset and generate a training set with 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All helper functions\n",
    "eps = 1e-5\n",
    "min_val = -1 + eps\n",
    "max_val = 1 - eps\n",
    "def val(min_val, max_val):\n",
    "    return round(np.random.uniform(min_val, max_val), 2)\n",
    "def class_for_val(val1, val2):\n",
    "    k = np.pi\n",
    "    return int(val2 >= -1/4 + 3/4*np.sin(val1*k))\n",
    "def create_dataset(n_points, min_val, max_val):\n",
    "    val1_list = np.array([val(min_val, max_val) for _ in range(n_points)])\n",
    "    val2_list = np.array([val(min_val, max_val) for _ in range(n_points)])\n",
    "    inputs = np.array([[v1, v2] for v1, v2 in zip(val1_list, val2_list)])\n",
    "    outputs = np.array([class_for_val(v1, v2) for v1, v2 in zip(val1_list, val2_list)]).reshape(n_points, 1)\n",
    "    return val1_list, val2_list, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset (train)\n",
    "np.random.seed(47)\n",
    "n_points = 1000\n",
    "train_val1_list, train_val2_list, train_inputs, train_outputs = create_dataset(n_points, min_val, max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our previous Shallow Neural Net class, with everything\n",
    "\n",
    "We wil reuse our previous **ShallowNeuralNet** class from Week2 Notebook 7, which:\n",
    "- implements a Shallow neural network using two fully connected layers and sigmoid activation functions,\n",
    "- uses a Stochastic Mini-Batch gradient descent, with Adam as its optimizer,\n",
    "- uses a random normal initialization,\n",
    "- comes with a forward() method for predictions,\n",
    "- comes with a backward() and train() method for backpropagation training,\n",
    "- comes with a cross-entropy loss function and an accuracy calculating loss function,\n",
    "- comes with a display function, to show training curves on both the loss and the accuracy,\n",
    "- comes with save and load functions.\n",
    "\n",
    "For now, we will focus on replicating the init, forward, loss and accuracy methods in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.init_parameters_normal()\n",
    "         \n",
    "    def init_parameters_normal(self):\n",
    "        # Weights and biases matrices (randomly initialized)\n",
    "        self.W1 = np.random.randn(self.n_x, self.n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, self.n_h)*0.1\n",
    "        self.W2 = np.random.randn(self.n_h, self.n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, self.n_y)*0.1\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1/(1 + np.exp(-val))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = np.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        A1 = self.sigmoid(Z1_b)\n",
    "        # Wx + b operation for the second layer\n",
    "        Z2 = np.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = self.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, inputs, outputs):\n",
    "        # MSE loss function as before\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        eps = 1e-10\n",
    "        losses = outputs*np.log(pred + eps) + (1 - outputs)*np.log(1 - pred + eps)\n",
    "        loss = -np.sum(losses)/outputs.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, inputs, outputs):\n",
    "        # Calculate accuracy for given inputs and ouputs\n",
    "        pred = [int(val >= 0.5) for val in self.forward(inputs)]\n",
    "        acc = sum([int(val1 == val2[0]) for val1, val2 in zip(pred, outputs)])/outputs.shape[0]\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then run the model by running the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "(1000, 1)\n",
      "0.626\n",
      "0.6853940202992042\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network structure\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net = ShallowNeuralNet(n_x, n_h, n_y)\n",
    "pred = shallow_neural_net.forward(train_inputs)\n",
    "acc = shallow_neural_net.accuracy(train_inputs, train_outputs)\n",
    "loss = shallow_neural_net.CE_loss(train_inputs, train_outputs)\n",
    "print(pred.shape)\n",
    "print(train_outputs.shape)\n",
    "print(acc)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting our class using PyTorch operations instead of Numpy - Init,  Forward, Loss and Accuracy\n",
    "\n",
    "The main differences between the original class and the PyTorch version in the **ShallowNeuralNet_PT** class are:\n",
    "- The PyTorch version of the class should inherit from torch.nn.Module and call its parent's init method using super(). This is necessary because PyTorch uses classes inherited from Module to keep track of the layers and their parameters in a neural network.\n",
    "- Instead of using NumPy arrays for the weights and biases, the PyTorch version uses torch.nn.Parameter objects, which are tensors that are optimized by PyTorch's optimizers.\n",
    "- The activation function sigmoid is replaced with PyTorch's torch.sigmoid function.\n",
    "- In the CE_loss and accuracy methods, we will reuse the torch functions and methods as much as possible, instead of the Numpy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our class will inherit from the torch.nn.Module\n",
    "# used to write all model in PyTorch\n",
    "class ShallowNeuralNet_PT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y, device):\n",
    "        # Super __init__ for inheritance\n",
    "        super().__init__()\n",
    "        \n",
    "        # Network dimensions (as before)\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        \n",
    "        # Device\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize parameters using the torch.nn.Parameter type (a subclass of Tensors).\n",
    "        # We immediatly initialize the parameters using a random normal.\n",
    "        # The RNG is done using torch.randn instead of the NumPy RNG.\n",
    "        # We add a conversion into float64 (the same float type used by Numpy to generate our data)\n",
    "        # And send them to our GPU/CPU device\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(n_x, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(1, n_h, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(n_h, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(1, n_y, requires_grad = True, \\\n",
    "                                     dtype = torch.float64, device = device)*0.1)\n",
    "        self.W1.retain_grad()\n",
    "        self.b1.retain_grad()\n",
    "        self.W2.retain_grad()\n",
    "        self.b2.retain_grad()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Instead of using np.matmul(), we use its equivalent in PyTorch,\n",
    "        # which is torch.matmul()!\n",
    "        # (Most numpy matrix operations ahve their equivalent in torch, check it out!)\n",
    "        # Wx + b operation for the first layer\n",
    "        Z1 = torch.matmul(inputs, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        # Sigmoid is already implemented in PyTorch, feel fre to reuse it!\n",
    "        A1 = torch.sigmoid(Z1_b)\n",
    "        \n",
    "        # Wx + b operation for the second layer\n",
    "        # (Same as first layer)\n",
    "        Z2 = torch.matmul(A1, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        y_pred = torch.sigmoid(Z2_b)\n",
    "        return y_pred\n",
    "    \n",
    "    def CE_loss(self, pred, outputs):\n",
    "        # We will use an epsilon to avoid NaNs on the log() values\n",
    "        eps = 1e-10\n",
    "        # As before with matmul, most operations in NumPy have their equivalent in torch (e.g. log and sum)\n",
    "        losses = outputs * torch.log(pred + eps) + (1 - outputs) * torch.log(1 - pred + eps)\n",
    "        loss = -torch.sum(losses)/outputs.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, pred, outputs):\n",
    "        # Calculate accuracy for given inputs and ouputs\n",
    "        # We will, again, rely as much as possible on the torch methods and functions. \n",
    "        return ((pred >= 0.5).int() == outputs).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to use this Neural Network on our dataset, we need to convert them to PyTorch Tensor objects and send them to GPU (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_pt = torch.from_numpy(train_inputs).to(device)\n",
    "train_outputs_pt = torch.from_numpy(train_outputs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "tensor(0.6260, device='cuda:0') tensor(0.6881, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "0.6260000467300415 0.688087761172729\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network structure\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "np.random.seed(37)\n",
    "shallow_neural_net_pt = ShallowNeuralNet_PT(n_x, n_h, n_y, device)\n",
    "train_pred = shallow_neural_net_pt.forward(train_inputs_pt)\n",
    "acc = shallow_neural_net_pt.accuracy(train_pred, train_outputs_pt)\n",
    "loss = shallow_neural_net_pt.CE_loss(train_pred, train_outputs_pt)\n",
    "print(train_pred.shape)\n",
    "print(train_outputs_pt.shape)\n",
    "print(acc, loss)\n",
    "print(acc.item(), loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, quick note, using the ShallowNeuralNet_PT object (or any torch.nn.Module object for that matter) as a function will call the forward method we have defined! It MUST therefore use the name forward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "# This is therefore equivalent to train_pred = shallow_neural_net_pt.forward(train_inputs_pt)\n",
    "train_pred = shallow_neural_net_pt(train_inputs_pt)\n",
    "print(train_pred.shape)\n",
    "print(train_outputs_pt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation times comparison\n",
    "\n",
    "Below, we run both models (the NumPy one and the PyTorch one) and ask them to perform 1000 times the accuracy computation.\n",
    "\n",
    "On my machine (which is CUDA enabled and uses an Nvidia GTX 1060), we can observe that the PyTorch model is roughly 30 times faster!\n",
    "\n",
    "This will obviously depend on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0732526779174805\n",
      "0.07289958000183105\n",
      "28.43984393141093\n"
     ]
    }
   ],
   "source": [
    "# Calculate computation times (NumPy NN)\n",
    "start = time()\n",
    "for i in range(1000):\n",
    "    train_acc = shallow_neural_net.accuracy(train_inputs, train_outputs)\n",
    "end = time()\n",
    "time_np = end - start\n",
    "print(time_np)\n",
    "\n",
    "# Calculate computation times (PyTorch NN)\n",
    "start = time()\n",
    "for i in range(1000):\n",
    "    train_acc_pt = shallow_neural_net_pt.accuracy(train_inputs_pt, train_outputs_pt)\n",
    "end = time()\n",
    "time_pt = end - start\n",
    "print(time_pt)\n",
    "\n",
    "# Ratio\n",
    "ratio = time_np/time_pt\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "In the next notebook, we will investigate how to implement the backpropagation mechanism using the PyTorch framework, and eventually use it to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
