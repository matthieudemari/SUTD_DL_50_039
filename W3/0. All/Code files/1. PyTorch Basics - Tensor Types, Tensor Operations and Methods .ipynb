{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PyTorch Basics - Tensor Types, Tensor Operations and Methods \n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (27/12/2022)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)\n",
    "- Torch (tested on v1.13.0)\n",
    "- Time (default Python library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# Numpy\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "# Torch\n",
    "import torch\n",
    "# Time\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the benefits of using PyTorch to implement a Neural Network\n",
    "\n",
    "There are several benefits to using PyTorch over NumPy for implementing neural networks:\n",
    "\n",
    "- PyTorch provides a more intuitive interface for working with tensors and neural networks. NumPy is primarily a numerical computing library, and while it can be used to perform operations on arrays that are similar to those used in neural networks, PyTorch is specifically designed with deep learning in mind and provides a more natural and convenient interface for defining and training neural networks.\n",
    "- PyTorch has better support for GPU acceleration than NumPy. If a GPU is available, it can significantly speed up the training of our neural network by performing the computations on the GPU using PyTorch. This can be especially useful for training large and complex models.\n",
    "- PyTorch includes a number of high-level abstractions for building and training neural networks, such as nn.Module, nn.Sequential, and optim. These abstractions make it easier to write and debug code, and can also improve the performance of our model by allowing PyTorch to apply optimization techniques such as graph fusion and automatic differentiation (which is nice as we will no longer have to worry about the gradient update rules to use!).\n",
    "- PyTorch has a large and active community of users, coming witha wealth of online resources and documentation to help troubleshoot any issues.\n",
    "\n",
    "Overall, while NumPy is a powerful library for numerical computing, but PyTorch is a more effective choice for implementing and training neural networks, especially if when taking advantage of GPU acceleration or when using more advanced features such as automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a GPU/CPU device for computation\n",
    "\n",
    "You can check for CUDA/GPU capabilities, using the line below. If the CUDA has not been properly installed or the GPU is not compatible, you will be using a CPU instead.\n",
    "\n",
    "We strongly advise to take a moment to make sure your machine is CUDA enabled, assuming your GPU is compatible. When CUDA is properly installed on a compatible GPU, the line below should display *cuda*, otherwise it will print *cpu*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tensor object\n",
    "\n",
    "Tensors are a data structure that are very similar to arrays and matrices. The tensor is PyTorch's basic building block and similar to NumPy arrays, which is why most of the concepts and methods will look similar. However, these come with additional features, which will be useful later on when building Neural Networks with these tensors.\n",
    "\n",
    "They can be initialized as in NumPy, by using **zeros()** or **ones()** functions, specifying dimensions with tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D Numpy array and a PyTorch tensor,\n",
    "# both of size 2 by 5, filled with ones.\n",
    "ones_array = np.ones((2, 5))\n",
    "print(ones_array)\n",
    "ones_tensor = torch.ones(size = (2, 5))\n",
    "print(ones_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D tensor of size 3, filled zeros.\n",
    "# (Pay attention to the extra comma in the tuple.)\n",
    "zeros_tensor = torch.zeros(size = (3, ))\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also create a tensor directly from a list (or a list of lists), as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor from a list, directly\n",
    "l = [1, 2, 3, 4]\n",
    "list_tensor = torch.tensor(l)\n",
    "print(list_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also transform a NumPy array into a tensor, using the **from_numpy()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a numPy array\n",
    "numpy_array = np.array([0.1, 0.2, 0.3])\n",
    "numpy_tensor = torch.from_numpy(numpy_array)\n",
    "print(numpy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors have an attribute called **dtype**, which tracks the types of values stored in the tensor. The most common one is *torch.float64*, but other dtypes exist. See https://www.tensorflow.org/api_docs/python/tf/dtypes for more details on the possible dtypes.\n",
    "\n",
    "It is possible to change the **dtype** of a tensor\n",
    "- by either specifying it during its creation;\n",
    "- or by using the **type()** method on the tensor, specifying a new dtype to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor from a list, directly\n",
    "# Forcing dtype to be integers on 32bits.\n",
    "l = [1, 2, 3, 4]\n",
    "list_tensor = torch.tensor(l, dtype = torch.int32)\n",
    "print(list_tensor)\n",
    "# Changing to float 64bits\n",
    "list_tensor2 = list_tensor.type(torch.float64)\n",
    "print(list_tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, operations between tensors require compatible, and sometimes **identical** dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor from a list, directly\n",
    "# Forcing dtype to be integers on 32bits.\n",
    "l = [1, 2, 3, 4]\n",
    "list_tensor = torch.tensor(l, dtype = torch.int32)\n",
    "print(list_tensor)\n",
    "# Changing to float 64bits\n",
    "l2 = [2, 4, 6, 8]\n",
    "list_tensor2 = torch.tensor(l2, dtype = torch.double)\n",
    "print(list_tensor2)\n",
    "# SOme operations on tensors with different datatypes might be problematic\n",
    "list_tensor3 = torch.dot(list_tensor, list_tensor2)\n",
    "print(list_tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can also be initialized using random generators, as in NumPy. For instance we can use **rand()** for drawing random values in a $ [0, 1] $ uniform distribution, or use **randn()** to draw values from a normal distribution with zero mean and variance one.\n",
    "\n",
    "Functions and methods both exist for calculating mean values of a tensor, its standard deviation/variance, etc.\n",
    "\n",
    "Seeding is done with **torch.manual_seed()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor, of size 3 by 2 by 2, filled with random values\n",
    "# drawn from a uniform [0, 1] distribution.\n",
    "rand_unif_tensor = torch.rand(size = (3, 2, 2))\n",
    "print(rand_unif_tensor)\n",
    "# Calculate mean with function (should be close to 0.5)\n",
    "val = torch.mean(rand_unif_tensor)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding\n",
    "torch.manual_seed(17)\n",
    "\n",
    "# Create a 4D tensor, of size 4 by 2 by 3 by 7, filled with random values\n",
    "# drawn from a normal distribution with zero mean and variance one.\n",
    "rand_normal_tensor = torch.randn(size = (4, 2, 3, 7))\n",
    "print(rand_normal_tensor.shape)\n",
    "\n",
    "# Calculate mean with method (should be close to 0)\n",
    "# (With see 17, should be 0.0865)\n",
    "val = rand_normal_tensor.mean()\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can ask the shape of a tensor, like in NumPy, using the **shape** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand_normal_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have **(way too) many** functions and methods you could use, just like the NumPy arrays.\n",
    "\n",
    "You know the drill, RTFM! (https://pytorch.org/docs/stable/torch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(rand_normal_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accesing, slicing, updating, browsing\n",
    "\n",
    "As we said, tensors are very similar to NumPy arrays. All the typical element-wise operations therefore work on tensors as well. For instance, we can:\n",
    "- access elements using the square bracket notation, multiple square bracket notations and multiple indexes in a single square bracket;\n",
    "- slice a tensor using the square bracket notation and colon symbol;\n",
    "- update elements of a tensor using the square bracket notation;\n",
    "- browse thropugh elements of a tensor using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor, of size 3 by 2 by 2, filled with random values\n",
    "# drawn from a uniform [0, 1] distribution.\n",
    "torch.manual_seed(17)\n",
    "rand_unif_tensor = torch.rand(size = (3, 2, 2))\n",
    "print(rand_unif_tensor)\n",
    "\n",
    "# Indexing\n",
    "element1 = rand_unif_tensor[2]\n",
    "print(\"Element1: \", element1)\n",
    "element2 = rand_unif_tensor[2][0]\n",
    "print(\"Element2: \", element2)\n",
    "element3 = rand_unif_tensor[2, 0, 1]\n",
    "print(\"Element3: \", element3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor, of size 3 by 2 by 2, filled with random values\n",
    "# drawn from a uniform [0, 1] distribution.\n",
    "torch.manual_seed(17)\n",
    "rand_unif_tensor = torch.rand(size = (3, 2, 2))\n",
    "print(rand_unif_tensor)\n",
    "\n",
    "# Slicing\n",
    "slice1 = rand_unif_tensor[0:2]\n",
    "print(\"Slice1: \", slice1)\n",
    "slice2 = rand_unif_tensor[:2]\n",
    "print(\"Slice2: \", slice2)\n",
    "slice3 = rand_unif_tensor[1:]\n",
    "print(\"Slice3: \", slice3)\n",
    "slice4 = rand_unif_tensor[0, :, :]\n",
    "print(\"Slice4: \", slice4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor, of size 3 by 2 by 2, filled with random values\n",
    "# drawn from a uniform [0, 1] distribution.\n",
    "torch.manual_seed(17)\n",
    "rand_unif_tensor = torch.rand(size = (3, 2, 2))\n",
    "print(rand_unif_tensor)\n",
    "\n",
    "# Before\n",
    "element4 = rand_unif_tensor[2, 1, 1]\n",
    "print(\"Element4: \", element4)\n",
    "# Updating\n",
    "rand_unif_tensor[2, 1, 1] = 0.5\n",
    "# After\n",
    "element4 = rand_unif_tensor[2, 1, 1]\n",
    "print(\"New Element4: \", element4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor, of size 3 by 2 by 2, filled with random values\n",
    "# drawn from a uniform [0, 1] distribution.\n",
    "torch.manual_seed(17)\n",
    "rand_unif_tensor = torch.rand(size = (3, 2, 2))\n",
    "print(rand_unif_tensor)\n",
    "\n",
    "# Browsing\n",
    "for sub_tensor in rand_unif_tensor:\n",
    "    print(\"---\")\n",
    "    print(sub_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Tensors\n",
    "\n",
    "All NumPy array operations work on tensors and equivalent methods have been writen in torch as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two simple 2D tensors\n",
    "a = torch.tensor([[1, 2, 3], [1, 2, 3]])\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise addition\n",
    "c = a + b\n",
    "print(c)\n",
    "c = torch.add(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise substraction\n",
    "c = a - b\n",
    "print(c)\n",
    "c = torch.sub(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise multiplication\n",
    "c = a * b\n",
    "print(c)\n",
    "c = torch.mul(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise division\n",
    "c = a / b\n",
    "print(c)\n",
    "c = torch.div(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposition \n",
    "c = a.T\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose and swap dimensions 0 and 1\n",
    "# (could specify other dimensions if ND tensor)\n",
    "d = b.transpose(0, 1)\n",
    "print(b)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix multiplication, not to be confused with the element-wise multiplication, is performed using the **matmul()** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product, on the other hand, is typically performed using the **dot()** function. When used on two 1D tensors, you obtain the inner product. When used on two 2D tensors, it is equivalent to matmul()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "e = torch.matmul(a, d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two simple 1D tensors\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Dot operation, used for computing\n",
    "# the dot product of two 1D tensors.\n",
    "f = torch.dot(a, b)\n",
    "print(f)\n",
    "g = torch.matmul(a, b.T)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on broadcasting\n",
    "\n",
    "Tensors, just like NumPy arrays, support broadcasting. Two tensors are “broadcastable” if the following rules hold:\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "If two tensors x, y are “broadcastable”, the resulting tensor size is calculated as follows:\n",
    "- If the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n",
    "- Then, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same shapes are always broadcastable\n",
    "# (i.e. the above rules always hold)\n",
    "x = torch.ones(5, 7, 3)\n",
    "y = torch.ones(5, 7, 3)\n",
    "z = (x+y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors x and y are not broadcastable,\n",
    "# because x does not have at least 1 dimension\n",
    "x = torch.ones((0,))\n",
    "y = torch.ones(2,2)\n",
    "z = (x+y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can line up trailing dimensions\n",
    "# Tensors x and y are broadcastable.\n",
    "# 1st trailing dimension: both have size 1\n",
    "# 2nd trailing dimension: y has size 1, using size of x and broadcasting\n",
    "# 3rd trailing dimension: x size is same as y size\n",
    "# 4th trailing dimension: y dimension doesn't exist, using x only\n",
    "x = torch.ones(5, 3, 4, 1)\n",
    "y = torch.ones(3, 1, 1)\n",
    "z = (x+y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, x and y are not broadcastable, \n",
    "# because of third trailing dimension (2 != 3).\n",
    "x = torch.ones(5, 2, 4, 1)\n",
    "y = torch.ones(3, 1, 1)\n",
    "z = (x+y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on tensor locations\n",
    "\n",
    "By default, all tensors are used by the CPU when performing calculations on them. If your device has been enabled for GPU/CUDA computation, you will have to transfer the tensor to the GPU for faster computation. This is done in three ways:\n",
    "- Using **.to(device)** method will transfer to the best device available for computation (we defined the value of the device variable earlier, when we checked for cuda/cpu).\n",
    "- Using **.cpu()** or **.cuda()** will force transfer to the cpu or cuda respectively. Note that it might fail if you machine is not CUDA compatible.\n",
    "\n",
    "In doubt, you can check the device attribute of your tensors to find where their computations will occur. In general, two tensors with different devices cannot be used for the same computation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tensor will by default be hosted on CPU\n",
    "a = torch.ones(2, 3)\n",
    "print(a)\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best option, use GPU/CUDA if available, else use CPU\n",
    "b = torch.ones(2, 3).to(device)\n",
    "print(b)\n",
    "print(b.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force tensor to CPU\n",
    "c = torch.ones(2, 3).cpu()\n",
    "print(c)\n",
    "print(c.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force tensor to GPU/CUDA\n",
    "# (will fail if not CUDA compatible)\n",
    "d = torch.ones(2, 3).cuda()\n",
    "print(d)\n",
    "print(d.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations require tensors\n",
    "# to be on same device\n",
    "c = torch.ones(2, 3).cpu()\n",
    "print(c)\n",
    "print(c.device)\n",
    "d = torch.ones(2, 3).cuda()\n",
    "print(d)\n",
    "print(d.device)\n",
    "f = c + d\n",
    "print(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few practice activities\n",
    "\n",
    "In order to practice your PyTorch Tensor skills, you may try to manually implement your own version of typical algorithms we ran on lists/Numpy arrays in previous classes, using the basic operations on PyTorch tensors.\n",
    "\n",
    "For instance, try writing algorithms:\n",
    "- Finding the maximum, minimum, average, median values of a given 1D tensor,\n",
    "- Transposing a given 2D tensor,\n",
    "- Sorting a given 1D tensor (bubble sort, insertion sort, selection sort, quick sort, merge sort),\n",
    "- Generating a 1D array containing the first K Fibonacci numbers with K given,\n",
    "- Etc.\n",
    "\n",
    "Later, you can check their performance times compared to their numpy/pytorch implementations when running them on both CPU and CUDA (if available).\n",
    "\n",
    "In which scenarios is it slower to implement said functions and run them on GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "In the next notebook, we will investigate how to use the PyTorch framework, and start implementing Neural Networks more efficiently, starting with the init, forward propagation and loss for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
