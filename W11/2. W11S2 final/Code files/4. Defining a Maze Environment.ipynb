{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671ac3d9",
   "metadata": {},
   "source": [
    "# 4. Defining a Maze Environment\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (04/02/2025)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.13.1)\n",
    "- Matplotlib (tested on v3.10.0)\n",
    "- Numpy (tested on v2.2.1)\n",
    "- enum (Standard libraries for Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4f22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, IntEnum\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3ff17",
   "metadata": {},
   "source": [
    "### Some Enums to begin\n",
    "\n",
    "As you can see, these enums will be used to encode the state of a cell in the maze, the actions and the status of the game. Please self-study the code below, it should be rather self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61393aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(IntEnum):\n",
    "    # Using an Enum for simplicity\n",
    "    # 0 = Empty cell where the agent can move\n",
    "    # 1 = Cell containing a wall, not accessible\n",
    "    EMPTY = 0\n",
    "    OCCUPIED = 1\n",
    "\n",
    "class Action(IntEnum):\n",
    "    # Using an Enum for simplicity\n",
    "    # Self explanatory\n",
    "    MOVE_LEFT = 0\n",
    "    MOVE_RIGHT = 1\n",
    "    MOVE_UP = 2\n",
    "    MOVE_DOWN = 3\n",
    "\n",
    "class Status(Enum):\n",
    "    # Using an Enum for simplicity\n",
    "    # Self explanatory\n",
    "    WIN = 0\n",
    "    LOSE = 1\n",
    "    PLAYING = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863aa09",
   "metadata": {},
   "source": [
    "### Our Maze object\n",
    "\n",
    "Next, we will define our environment, using a Maze object. Please self-study the code, but do not worry too much about the rendering functions.\n",
    "\n",
    "Our Maze object will have the following attributes:\n",
    "- maze: Stores the layout of the maze (2D numpy array). The values 0 represents empty cells, 1 represents walls, and 2 represents the agent's current position.\n",
    "- cells: A list of all possible cell coordinates (row, column) in the maze.\n",
    "- empty: A list of all valid (non-wall) cells where the agent can move.\n",
    "- exit_cell:The coordinates of the exit cell in the maze.\n",
    "- actions: A list of all possible agent actions (MOVE_LEFT, MOVE_RIGHT, MOVE_UP, MOVE_DOWN).\n",
    "\n",
    "In terms of rewards and Penalties:\n",
    "- reward_exit: The reward for reaching the exit cell.\n",
    "- penalty_move: A penalty applied for each move (encourages efficiency).\n",
    "- penalty_visited: A penalty for revisiting a cell (discourages loops).\n",
    "- penalty_impossible_move: A penalty for attempting an invalid move (e.g., into a wall).\n",
    "- minimum_reward: Threshold for ending the game if the cumulative reward falls below this value.\n",
    "\n",
    "Regarding the agent state:\n",
    "- current_cell: The agent's current position in the maze.\n",
    "- previous_cell: The agent's position before the last move.\n",
    "- total_reward: The cumulative reward the agent has accumulated.\n",
    "- visited: A set of cells the agent has already visited.\n",
    "\n",
    "In terms of methods, we have methods related to initialization and setup, namely\n",
    "- init(maze, start_cell, exit_cell): Initializes the maze environment, defines RL parameters.\n",
    "- create_maze(maze, start_cell, exit_cell): Defines the maze layout, validates the exit cell, and populates the list of valid (empty) cells.\n",
    "- define_rl_params(): Initializes RL parameters (rewards and penalties).\n",
    "\n",
    "Next, there are methods related to the agent actions and state, these are important as they will later be used to play the game:\n",
    "- reset(start_cell): Resets the maze to its initial state, placing the agent at the starting position and clearing rewards and visited cells.\n",
    "- step(action): Performs the specified action (e.g., move left or right) and returns the new state, reward, and game status.\n",
    "- execute(action): Handles the agent's movement logic, computes rewards, and updates the agent's position.\n",
    "- possible_actions(cell): Returns a list of valid actions (based on the maze layout) that the agent can take from the specified cell.\n",
    "- status(): Determines the game's status (WIN, LOSE, or PLAYING) based on the agent's position and cumulative reward.\n",
    "- observe(): Returns the agent's current position as a numpy array.\n",
    "- play(model, start_cell): Simulates a game where the provided model predicts the agent's actions.\n",
    "\n",
    "Some additional attributes and methods regarding the rendering/visualization exist, but you do not have to worry about them too much, they are just some heavy matplotlib stuff to display the maze and wil be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07704f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"\n",
    "    A Maze environment where an agent must navigate from a start cell to an exit cell.\n",
    "    \"\"\"\n",
    "    def __init__(self, maze, start_cell, exit_cell):\n",
    "        # Initialize maze\n",
    "        self.create_maze(maze, start_cell, exit_cell)\n",
    "        # Define the RL parameters for agent (rewards, etc.)\n",
    "        self.define_rl_params()\n",
    "        # Reset\n",
    "        self.reset(start_cell)\n",
    "\n",
    "    def create_maze(self, maze, start_cell, exit_cell):\n",
    "        # Maze definition\n",
    "        self.maze = maze\n",
    "        nrows, ncols = self.maze.shape\n",
    "        self.cells = [(col, row) for col in range(ncols) for row in range(nrows)]\n",
    "        self.empty = [(col, row) for col in range(ncols) for row in range(nrows) if self.maze[row, col] == Cell.EMPTY]\n",
    "        self.exit_cell = exit_cell\n",
    "        self.empty.remove(self.exit_cell)\n",
    "        self.start_cell = start_cell\n",
    "        \n",
    "    def define_rl_params(self):\n",
    "        # List of all 4 possible actions\n",
    "        self.actions = [Action.MOVE_LEFT, Action.MOVE_RIGHT, Action.MOVE_UP, Action.MOVE_DOWN]\n",
    "        # Reward for reaching the exit\n",
    "        self.reward_exit = 10.0\n",
    "        # Penalty applied everytime a move is taken (encourages the agent to find the exit ASAP)\n",
    "        self.penalty_move = -0.05\n",
    "        # Penalty for revisiting a cell (encourages the agent not to revisit cells it has already gone through)\n",
    "        self.penalty_visited = -0.25\n",
    "        # Penalty for invalid moves (encourages agent not to bump into walls)\n",
    "        self.penalty_impossible_move = -0.75\n",
    "        # Threshold to force game end, if cumulated reward falls below this value\n",
    "        self.minimum_reward = -10\n",
    "        \n",
    "    def reset(self, start_cell):\n",
    "        \"\"\"\n",
    "        Reset the maze to its initial state.\n",
    "        \"\"\"\n",
    "        self.previous_cell = self.current_cell = start_cell\n",
    "        self.total_reward = 0.0\n",
    "        self.visited = set()\n",
    "        return self.observe()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform an action in the maze.\n",
    "        \"\"\"\n",
    "        reward = self.execute(action)\n",
    "        self.total_reward += reward\n",
    "        status = self.status()\n",
    "        state = self.observe()\n",
    "        return state, reward, status\n",
    "\n",
    "    def execute(self, action):\n",
    "        \"\"\"\n",
    "        Execute an action and return the reward of this action.\n",
    "        \"\"\"\n",
    "        # Compute possible actions in the current location\n",
    "        possible_actions = self.possible_actions(self.current_cell)\n",
    "        if not possible_actions:\n",
    "            # No valid moves, force game over\n",
    "            return self.minimum_reward - 1\n",
    "        # Tried going into a wall, apply penalty and do nothing in terms of movement\n",
    "        if action not in possible_actions:\n",
    "            return self.penalty_impossible_move\n",
    "        # Otherwise, move the agent based on the selected action\n",
    "        col, row = self.current_cell\n",
    "        if action == Action.MOVE_LEFT:\n",
    "            col -= 1\n",
    "        elif action == Action.MOVE_UP:\n",
    "            row -= 1\n",
    "        elif action == Action.MOVE_RIGHT:\n",
    "            col += 1\n",
    "        elif action == Action.MOVE_DOWN:\n",
    "            row += 1\n",
    "        # Update cell position\n",
    "        self.previous_cell = self.current_cell\n",
    "        self.current_cell = (col, row)\n",
    "        # Check for penalties or rewards based on the new cell\n",
    "        if self.current_cell == self.exit_cell:\n",
    "            return self.reward_exit\n",
    "        elif self.current_cell in self.visited:\n",
    "            # Apply penalty for revisiting\n",
    "            return self.penalty_visited\n",
    "        else:\n",
    "            # Mark as visited\n",
    "            self.visited.add(self.current_cell)\n",
    "            return self.penalty_move\n",
    "\n",
    "    def possible_actions(self, cell = None):\n",
    "        \"\"\"\n",
    "        Determine all valid actions from the current cell.\n",
    "        \"\"\"\n",
    "        if cell is None:\n",
    "            col, row = self.current_cell\n",
    "        else:\n",
    "            col, row = cell\n",
    "        possible_actions = self.actions.copy()\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # Update list of moves\n",
    "        # For each direction, remove from list of actions if it leads into a wall or out of the maze\n",
    "        if row == 0 or self.maze[row - 1, col] == Cell.OCCUPIED:\n",
    "            possible_actions.remove(Action.MOVE_UP)\n",
    "        if row == nrows - 1 or self.maze[row + 1, col] == Cell.OCCUPIED:\n",
    "            possible_actions.remove(Action.MOVE_DOWN)\n",
    "        if col == 0 or self.maze[row, col - 1] == Cell.OCCUPIED:\n",
    "            possible_actions.remove(Action.MOVE_LEFT)\n",
    "        if col == ncols - 1 or self.maze[row, col + 1] == Cell.OCCUPIED:\n",
    "            possible_actions.remove(Action.MOVE_RIGHT)\n",
    "        return possible_actions\n",
    "\n",
    "    def status(self):\n",
    "        \"\"\"\n",
    "        Determine the current status of the game.\n",
    "        \"\"\"\n",
    "        if self.current_cell == self.exit_cell:\n",
    "            # Reached the exit\n",
    "            return Status.WIN\n",
    "        elif self.total_reward < self.minimum_reward:\n",
    "            # Cumulative reward going below minimum_reward means\n",
    "            # too many moves taken without finding the exit\n",
    "            return Status.LOSE\n",
    "        else:\n",
    "            # Otherwise, still playing\n",
    "            return Status.PLAYING\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"\n",
    "        Helper function that returns the current state of the agent.\n",
    "        \"\"\"\n",
    "        return np.array([[*self.current_cell]])\n",
    "\n",
    "    def play(self, model, start_cell = (0, 0)):\n",
    "        \"\"\"\n",
    "        Play a game using the provided model to predict actions.\n",
    "        \"\"\"\n",
    "        self.reset(start_cell)\n",
    "        state = self.observe()\n",
    "        while True:\n",
    "            # Decide on action to use in state\n",
    "            action = model.predict(state)\n",
    "            # Update environment by applying action\n",
    "            state, reward, status = self.step(action)\n",
    "            # When game ends (exit reach or too many moves), return outcome (win or lose)\n",
    "            if status in (Status.WIN, Status.LOSE):\n",
    "                return status\n",
    "    \n",
    "    def draw_full_maze(self):\n",
    "        \"\"\"\n",
    "        Draw the entire maze with walls, valid cells, start, and exit positions.\n",
    "        Ensures (0,0) is at the top-left and (nrows-1, ncols-1) at the bottom-right.\n",
    "        \"\"\"\n",
    "        nrows, ncols = self.maze.shape\n",
    "        fig, ax = plt.subplots(figsize = (6, 6), tight_layout = True)\n",
    "        # Display the maze\n",
    "        ax.imshow(self.maze, cmap = \"Greys\", origin = \"upper\")\n",
    "        # Set major ticks to match correct grid positions\n",
    "        ax.set_xticks(np.arange(ncols))\n",
    "        ax.set_yticks(np.arange(nrows))\n",
    "        # Label axes correctly with 0 at top-left\n",
    "        ax.set_xticklabels(np.arange(ncols))\n",
    "        ax.set_yticklabels(np.arange(nrows))\n",
    "        # Draw grid lines\n",
    "        ax.set_xticks(np.arange(-0.5, ncols, 1), minor = True)\n",
    "        ax.set_yticks(np.arange(-0.5, nrows, 1), minor = True)\n",
    "        ax.grid(which = \"minor\", color = \"black\", linestyle = \"-\", linewidth = 0.5)\n",
    "        # Mark the starting position\n",
    "        start_col, start_row = self.start_cell\n",
    "        ax.plot(start_col, start_row, \"rs\", markersize = 20)\n",
    "        ax.text(start_col, start_row, \"Start\", ha = \"center\", va = \"center\", color = \"white\")\n",
    "        # Mark the exit position\n",
    "        exit_col, exit_row = self.exit_cell\n",
    "        ax.plot(exit_col, exit_row, \"gs\", markersize = 20)\n",
    "        ax.text(exit_col, exit_row, \"Exit\", ha = \"center\", va = \"center\", color = \"white\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079bd24",
   "metadata": {},
   "source": [
    "### Explanations on how to use the Maze object\n",
    "\n",
    "In the code below, we show the important ways to create and use the Maze environment. It start with a layout for the maze, which will be a 4 by 4 grid, shown below. We will then decide that the start cell is top left (0, 0) and the exit cell is bottom right (3, 3).\n",
    "\n",
    "Then, we will reset the game (efficiently starting the game for the first time) and show the maze. All moves are shown on each cell as no agent has currently been trained to recognize the best move to play in each cell yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83170546-0f35-446e-b106-e41084f8714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze initialized.\n",
      "\n",
      "Initial state: [[0 0]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a simple maze layout\n",
    "maze_layout = np.array([[0, 1, 0, 0],\n",
    "                        [0, 0, 1, 0],\n",
    "                        [1, 0, 0, 0],\n",
    "                        [0, 1, 1, 0]])\n",
    "# Initialize the Maze environment\n",
    "maze = Maze(maze = maze_layout, start_cell = (0, 0), exit_cell = (3, 3))\n",
    "print(\"Maze initialized.\\n\")\n",
    "# Reset the maze\n",
    "state = maze.reset(start_cell = (0, 0))\n",
    "print(f\"Initial state: {state}\\n\")\n",
    "# Display the maze\n",
    "maze.draw_full_maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb9896-4cec-475c-88e5-0272c25d51bc",
   "metadata": {},
   "source": [
    "The functions below can be used to check the current agent state and the possible actions in any cell (meaning actions that will not lead into walls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec2fe83-9990-4609-bf8b-6d5304efb606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible actions from cell (2, 2): ['MOVE_LEFT', 'MOVE_RIGHT']\n",
      "Current position for agent: [[0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Show possible actions from a specific cell\n",
    "specific_cell = (2, 2)\n",
    "possible_actions = maze.possible_actions(cell = specific_cell)\n",
    "act = [action.name for action in possible_actions]\n",
    "print(f\"Possible actions from cell {specific_cell}: {act}\")\n",
    "\n",
    "# Observe the current state\n",
    "observation = maze.observe()\n",
    "print(f\"Current position for agent: {observation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ef28b-323a-4769-b306-d855c03c5797",
   "metadata": {},
   "source": [
    "We will then play a sequence of 5 actions: down, down, right, right, down, right, right, up, down, down.\n",
    "\n",
    "Some of these actions will change the agent state, moving it to a new location. Some of these actions will, however, bump into walls and lead to penalties and no changes of state. You can refer to the prints below. Notice how we use the step() method each time to update the game with a new action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0d9a99-b3c9-485d-b0d6-a4f3cd7d49ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing actions in the maze:\n",
      "\n",
      "-----\n",
      "Current state: [[0 0]]\n",
      "Step 1: Executing action MOVE_DOWN\n",
      "New state: [[0 1]]\n",
      "Reward: -0.05\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[0 1]]\n",
      "Step 2: Executing action MOVE_DOWN\n",
      "New state: [[0 1]]\n",
      "Reward: -0.75\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[0 1]]\n",
      "Step 3: Executing action MOVE_RIGHT\n",
      "New state: [[1 1]]\n",
      "Reward: -0.05\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[1 1]]\n",
      "Step 4: Executing action MOVE_RIGHT\n",
      "New state: [[1 1]]\n",
      "Reward: -0.75\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[1 1]]\n",
      "Step 5: Executing action MOVE_DOWN\n",
      "New state: [[1 2]]\n",
      "Reward: -0.05\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[1 2]]\n",
      "Step 6: Executing action MOVE_RIGHT\n",
      "New state: [[2 2]]\n",
      "Reward: -0.05\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[2 2]]\n",
      "Step 7: Executing action MOVE_RIGHT\n",
      "New state: [[3 2]]\n",
      "Reward: -0.05\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[3 2]]\n",
      "Step 8: Executing action MOVE_UP\n",
      "New state: [[3 1]]\n",
      "Reward: -0.05\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[3 1]]\n",
      "Step 9: Executing action MOVE_DOWN\n",
      "New state: [[3 2]]\n",
      "Reward: -0.25\n",
      "Status: PLAYING\n",
      "\n",
      "-----\n",
      "Current state: [[3 2]]\n",
      "Step 10: Executing action MOVE_DOWN\n",
      "New state: [[3 3]]\n",
      "Reward: 10.0\n",
      "Status: WIN\n",
      "\n",
      "Game ended with status: WIN\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate taking a few actions\n",
    "actions = [Action.MOVE_DOWN, Action.MOVE_DOWN, Action.MOVE_RIGHT, Action.MOVE_RIGHT, Action.MOVE_DOWN, \\\n",
    "           Action.MOVE_RIGHT, Action.MOVE_RIGHT, Action.MOVE_UP, Action.MOVE_DOWN, Action.MOVE_DOWN]\n",
    "\n",
    "# Use the sequence of actions\n",
    "print(\"Performing actions in the maze:\\n\")\n",
    "for i, action in enumerate(actions):\n",
    "    print(\"-----\")\n",
    "    print(f\"Current state: {state}\")\n",
    "    print(f\"Step {i + 1}: Executing action {action.name}\")\n",
    "    state, reward, status = maze.step(action)\n",
    "    print(f\"New state: {state}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print(f\"Status: {status.name}\\n\")\n",
    "\n",
    "    if status != Status.PLAYING:\n",
    "        print(f\"Game ended with status: {status.name}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc0057-9c71-42a3-b1f8-f5b7590b51b0",
   "metadata": {},
   "source": [
    "We can then confirm the visited cells and the final cumulative reward for this sequence of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9747dc5-9547-4410-8fd2-4c9016605287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent has visited the following cells: {(0, 1), (1, 2), (3, 1), (1, 1), (2, 2), (3, 2)}\n",
      "Total reward for sequence of moves 7.95\n"
     ]
    }
   ],
   "source": [
    "# Display visited cells\n",
    "print(\"Agent has visited the following cells:\", maze.visited)\n",
    "\n",
    "# Display cumulated reward\n",
    "print(\"Total reward for sequence of moves\", round(maze.total_reward, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbd7d2-0244-4266-82c2-407e365c8951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
