{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89a4d40-b766-4b39-97ee-4eeb379af5d2",
   "metadata": {},
   "source": [
    "# 2. Implementing a basic ViT\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (23/01/2025)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.13.1)\n",
    "- Numpy (tested on v2.2.1)\n",
    "- Sklearn (tested on v1.6.1)\n",
    "- Torch (tested on v2.7.0+cu124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77030e0d-50f2-405a-b27f-b76b85445e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0f8b86-a586-4f17-93b1-a7ac44e55c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13367f4d-8bde-4fd0-a2d6-7d3473145e0b",
   "metadata": {},
   "source": [
    "### Imports from the previous notebook (patches embeddings and 2D sinusoidal positional encodings)\n",
    "\n",
    "For simplicity, we will include everything in a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5f1c2d-79a1-485f-b26a-f12f301d51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerProcessor:\n",
    "    def __init__(self, img_size, patch_size, embed_dim, device):\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size//patch_size)**2\n",
    "        self.embed_dim = embed_dim\n",
    "        self.grid_size = (img_size//patch_size, img_size//patch_size)\n",
    "        self.device = device\n",
    "        self.linear_proj = nn.Conv2d(in_channels = 3, out_channels = embed_dim, \n",
    "                                     kernel_size = patch_size, stride = patch_size).to(device)\n",
    "        # Generate 2D positional encodings, do it once, instead of every single batch of images\n",
    "        self.positional_encodings = self.generate_2d_positional_encoding()\n",
    "\n",
    "    def generate_2d_positional_encoding(self):\n",
    "        # Create row and column positional encodings\n",
    "        num_rows, num_cols = self.grid_size\n",
    "        row_pos = np.arange(num_rows).reshape(-1, 1)\n",
    "        col_pos = np.arange(num_cols).reshape(-1, 1)\n",
    "        d = np.arange(self.embed_dim//2).reshape(1, -1)\n",
    "        # Sinusoidal encodings for rows and columns\n",
    "        angle_rates = 1/np.power(10000, (2*d)/self.embed_dim)\n",
    "        row_encoding = np.concatenate([np.sin(row_pos*angle_rates), np.cos(row_pos*angle_rates)], axis = -1)\n",
    "        col_encoding = np.concatenate([np.sin(col_pos*angle_rates), np.cos(col_pos*angle_rates)], axis = -1)\n",
    "        # Combine row and column encodings\n",
    "        row_encoding = np.tile(row_encoding[:, np.newaxis, :], (1, num_cols, 1))\n",
    "        col_encoding = np.tile(col_encoding[np.newaxis, :, :], (num_rows, 1, 1))\n",
    "        pos_encoding = row_encoding + col_encoding\n",
    "        return pos_encoding.reshape(-1, self.embed_dim)\n",
    "\n",
    "    def process_images(self, images):\n",
    "        # Process a batch of images to generate patch embeddings and apply positional encodings.\n",
    "        batch_size, channels, height, width = images.shape\n",
    "        # Generate patch embeddings\n",
    "        patches = self.linear_proj(images)  # Shape: (batch_size, embed_dim, grid_h, grid_w)\n",
    "        patches = patches.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "        # Add positional encodings\n",
    "        pos_encodings = torch.tensor(self.positional_encodings, dtype = torch.float32).to(images.device)\n",
    "        patches_with_pos = patches + pos_encodings.unsqueeze(0)  # Add positional encodings\n",
    "        return patches_with_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31477b2-1af2-4964-bdd9-2008aef081f4",
   "metadata": {},
   "source": [
    "Let us show how it works on one batch of images coming from CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e1a817-3abf-450f-b0a6-01f983fa6b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch Shape: torch.Size([16, 16, 64])\n",
      "Example Patch Embedding for First Image: tensor([-0.0376, -0.3324,  0.0313,  0.3214, -0.0649, -0.1658, -0.1404,  0.1826,\n",
      "         0.5735, -0.1980,  0.3530,  0.1537, -0.1557, -0.1961, -0.5471, -0.6755,\n",
      "        -0.0901,  0.5010, -0.1805, -0.1357,  0.5971,  0.0898, -0.3910, -0.2648,\n",
      "        -0.0728, -0.0758, -0.1184,  0.0698, -0.1251, -0.9450, -0.0135,  0.5654,\n",
      "         2.1114,  2.0324,  2.1717,  1.7041,  1.9677,  2.0746,  1.7042,  2.2752,\n",
      "         2.0074,  2.4681,  1.8811,  2.4472,  1.9353,  2.0653,  1.6227,  1.6465,\n",
      "         1.8564,  1.8609,  2.8056,  1.9274,  1.8273,  1.9682,  1.8559,  1.9932,\n",
      "         2.1965,  1.7839,  1.6668,  1.9211,  2.3594,  1.5442,  2.1772,  2.6053],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage with CIFAR-10\n",
    "# Example image size for CIFAR 10 is 32x32 pixels\n",
    "img_size = 32\n",
    "# Each patch is 8x8\n",
    "patch_size = 8\n",
    "# Embedding dimension set to 64\n",
    "embed_dim = 64\n",
    "\n",
    "# Create an instance of the processor\n",
    "vi_processor = VisionTransformerProcessor(img_size = img_size, patch_size = patch_size, embed_dim = embed_dim, device = device)\n",
    "\n",
    "# CIFAR-10 Dataset and DataLoader, uses batch size 16\n",
    "transform = transforms.Compose([transforms.Resize((img_size, img_size)), transforms.ToTensor()])\n",
    "cifar10_dataset = datasets.CIFAR10(root = \"./data\", train = True, download = True, transform = transform)\n",
    "dataloader = DataLoader(cifar10_dataset, batch_size = 16, shuffle = True)\n",
    "\n",
    "# Process a batch of images\n",
    "for images, labels in dataloader:\n",
    "    patch_embeddings = vi_processor.process_images(images.to(device))\n",
    "    # We expect (batch_size = 16, patches_number = 16, embed_dim = 64)\n",
    "    print(f\"Processed Batch Shape: {patch_embeddings.shape}\")\n",
    "    print(f\"Example Patch Embedding for First Image: {patch_embeddings[0, 0, :]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c875af-bb17-4419-b3c2-bccf91d84ab9",
   "metadata": {},
   "source": [
    "### Implementing a basic ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d91e7c-1906-4a02-a677-ed02f236d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, num_classes, num_heads, num_layers, mlp_dim, dropout = 0.1):\n",
    "        super(ViT, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        # Transformer Encoder, using PyTorch prototypes for transformers\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model = embed_dim,\n",
    "                                                                        nhead = num_heads,\n",
    "                                                                        dim_feedforward = mlp_dim,\n",
    "                                                                        dropout = dropout,\n",
    "                                                                        activation = 'gelu',\n",
    "                                                                        batch_first = True),\n",
    "                                             num_layers=num_layers)\n",
    "        # Classification head\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(embed_dim),\n",
    "                                      nn.Linear(embed_dim, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_patches, embed_dim = x.shape\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim = 1)\n",
    "        x = self.encoder(x)\n",
    "        cls_output = x[:, 0]\n",
    "        logits = self.mlp_head(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c86630-ebae-4378-aca5-043f5e800c5e",
   "metadata": {},
   "source": [
    "### Training loop for ViT\n",
    "\n",
    "Finally, we will write a simple training loop for our model and the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17883e6a-2dad-407a-b4ed-d15b42e4b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 Dataset and DataLoader\n",
    "transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train = True, download = True, transform = transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train = False, download = True, transform = transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8718b95f-51e8-437c-a58d-69b55c05bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model\n",
    "img_size = 32\n",
    "patch_size = 8\n",
    "embed_dim = 64\n",
    "num_patches = (img_size//patch_size)**2\n",
    "num_classes = 10\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "mlp_dim = 128\n",
    "\n",
    "# Vision Transformer Processor for Patches and Encodings\n",
    "vi_processor = VisionTransformerProcessor(img_size = img_size, patch_size = patch_size, embed_dim = embed_dim, device = device)\n",
    "\n",
    "# Vision Transformer Model\n",
    "model = ViT(embed_dim = embed_dim, num_patches = num_patches, num_classes = num_classes, \n",
    "            num_heads = num_heads, num_layers = num_layers, mlp_dim = mlp_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f47fad4-d284-4127-b830-62b6c9cbc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 100\n",
    "lr = 5e-3\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f87b4e-642a-43a8-8b75-fd5d2e06d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Generate patch embeddings with positional encodings\n",
    "            patch_embeddings = vi_processor.process_images(images)\n",
    "            # Forward pass\n",
    "            outputs = model(patch_embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4454e0ca-e883-42ab-9af7-f9db4a86f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.0111\n",
      "Epoch [2/100], Loss: 1.8319\n",
      "Epoch [3/100], Loss: 1.7671\n",
      "Epoch [4/100], Loss: 1.7204\n",
      "Epoch [5/100], Loss: 1.6859\n",
      "Epoch [6/100], Loss: 1.6555\n",
      "Epoch [7/100], Loss: 1.6378\n",
      "Epoch [8/100], Loss: 1.6034\n",
      "Epoch [9/100], Loss: 1.5809\n",
      "Epoch [10/100], Loss: 1.5567\n",
      "Epoch [11/100], Loss: 1.5376\n",
      "Epoch [12/100], Loss: 1.5346\n",
      "Epoch [13/100], Loss: 1.5146\n",
      "Epoch [14/100], Loss: 1.5023\n",
      "Epoch [15/100], Loss: 1.5007\n",
      "Epoch [16/100], Loss: 1.4896\n",
      "Epoch [17/100], Loss: 1.4797\n",
      "Epoch [18/100], Loss: 1.4680\n",
      "Epoch [19/100], Loss: 1.4597\n",
      "Epoch [20/100], Loss: 1.4520\n",
      "Epoch [21/100], Loss: 1.4479\n",
      "Epoch [22/100], Loss: 1.4394\n",
      "Epoch [23/100], Loss: 1.4260\n",
      "Epoch [24/100], Loss: 1.4214\n",
      "Epoch [25/100], Loss: 1.4110\n",
      "Epoch [26/100], Loss: 1.4086\n",
      "Epoch [27/100], Loss: 1.3965\n",
      "Epoch [28/100], Loss: 1.3892\n",
      "Epoch [29/100], Loss: 1.3851\n",
      "Epoch [30/100], Loss: 1.3809\n",
      "Epoch [31/100], Loss: 1.3760\n",
      "Epoch [32/100], Loss: 1.3689\n",
      "Epoch [33/100], Loss: 1.3628\n",
      "Epoch [34/100], Loss: 1.3596\n",
      "Epoch [35/100], Loss: 1.3586\n",
      "Epoch [36/100], Loss: 1.3528\n",
      "Epoch [37/100], Loss: 1.3478\n",
      "Epoch [38/100], Loss: 1.3363\n",
      "Epoch [39/100], Loss: 1.3350\n",
      "Epoch [40/100], Loss: 1.3366\n",
      "Epoch [41/100], Loss: 1.3254\n",
      "Epoch [42/100], Loss: 1.3152\n",
      "Epoch [43/100], Loss: 1.3215\n",
      "Epoch [44/100], Loss: 1.3110\n",
      "Epoch [45/100], Loss: 1.3098\n",
      "Epoch [46/100], Loss: 1.3024\n",
      "Epoch [47/100], Loss: 1.3031\n",
      "Epoch [48/100], Loss: 1.3003\n",
      "Epoch [49/100], Loss: 1.2915\n",
      "Epoch [50/100], Loss: 1.2914\n",
      "Epoch [51/100], Loss: 1.2837\n",
      "Epoch [52/100], Loss: 1.2818\n",
      "Epoch [53/100], Loss: 1.2784\n",
      "Epoch [54/100], Loss: 1.2783\n",
      "Epoch [55/100], Loss: 1.2751\n",
      "Epoch [56/100], Loss: 1.2711\n",
      "Epoch [57/100], Loss: 1.2604\n",
      "Epoch [58/100], Loss: 1.2654\n",
      "Epoch [59/100], Loss: 1.2631\n",
      "Epoch [60/100], Loss: 1.2558\n",
      "Epoch [61/100], Loss: 1.2551\n",
      "Epoch [62/100], Loss: 1.2555\n",
      "Epoch [63/100], Loss: 1.2457\n",
      "Epoch [64/100], Loss: 1.2456\n",
      "Epoch [65/100], Loss: 1.2469\n",
      "Epoch [66/100], Loss: 1.2397\n",
      "Epoch [67/100], Loss: 1.2441\n",
      "Epoch [68/100], Loss: 1.2421\n",
      "Epoch [69/100], Loss: 1.2360\n",
      "Epoch [70/100], Loss: 1.2386\n",
      "Epoch [71/100], Loss: 1.2375\n",
      "Epoch [72/100], Loss: 1.2344\n",
      "Epoch [73/100], Loss: 1.2317\n",
      "Epoch [74/100], Loss: 1.2231\n",
      "Epoch [75/100], Loss: 1.2241\n",
      "Epoch [76/100], Loss: 1.2273\n",
      "Epoch [77/100], Loss: 1.2210\n",
      "Epoch [78/100], Loss: 1.2187\n",
      "Epoch [79/100], Loss: 1.2214\n",
      "Epoch [80/100], Loss: 1.2156\n",
      "Epoch [81/100], Loss: 1.2125\n",
      "Epoch [82/100], Loss: 1.2101\n",
      "Epoch [83/100], Loss: 1.2120\n",
      "Epoch [84/100], Loss: 1.2123\n",
      "Epoch [85/100], Loss: 1.2130\n",
      "Epoch [86/100], Loss: 1.2095\n",
      "Epoch [87/100], Loss: 1.2042\n",
      "Epoch [88/100], Loss: 1.2019\n",
      "Epoch [89/100], Loss: 1.2037\n",
      "Epoch [90/100], Loss: 1.2041\n",
      "Epoch [91/100], Loss: 1.1954\n",
      "Epoch [92/100], Loss: 1.1945\n",
      "Epoch [93/100], Loss: 1.1980\n",
      "Epoch [94/100], Loss: 1.1920\n",
      "Epoch [95/100], Loss: 1.1917\n",
      "Epoch [96/100], Loss: 1.1936\n",
      "Epoch [97/100], Loss: 1.1916\n",
      "Epoch [98/100], Loss: 1.1942\n",
      "Epoch [99/100], Loss: 1.1907\n",
      "Epoch [100/100], Loss: 1.1819\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# (Takes a while as transformers have lots of trainable parameters!)\n",
    "train_model(model, train_loader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de9e71d-b423-4ba9-b39e-2b88bea82b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Generate patch embeddings with positional encodings\n",
    "            patch_embeddings = vi_processor.process_images(images)\n",
    "            # Forward pass\n",
    "            outputs = model(patch_embeddings)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0704de10-b856-456e-b854-bf221cec187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e3bec-36ea-4fd6-a693-01e9f787652f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
