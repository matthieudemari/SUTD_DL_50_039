{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679ef0ad",
   "metadata": {},
   "source": [
    "# 2. FastText demo\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.1 (22/03/2022)\n",
    "\n",
    "This notebook demonstrates how you may reuse a pre-trained language model from a Python library (e.g. the fasttext one).\n",
    "\n",
    "Most language models already have pre-trained version online, along with a few basic method giving the closest 10 words to a given word or vector representation, giving analogies, etc.\n",
    "\n",
    "This is based on the paper P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, \"Enriching Word Vectors with Subword Information\", 2017 (https://arxiv.org/abs/1607.04606).\n",
    "\n",
    "And it follows the (very nice) documentation provided on https://fasttext.cc/docs/en/unsupervised-tutorial.html\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)\n",
    "- Torch (tested on v1.10.1)\n",
    "- Torchvision (tested on v0.11.2)\n",
    "- Fasttext (tested on v0.9.2)\n",
    "- We also strongly recommend setting up CUDA on your machine!\n",
    "\n",
    "Important: You might have to pip install the **fasttext** package.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd656af7",
   "metadata": {},
   "source": [
    "### Download the model\n",
    "\n",
    "This command will download a pre-trained english language model and save it to file.\n",
    "\n",
    "Note: heavy model, takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "lang = 'en'\n",
    "fasttext.util.download_model(lang, if_exists = 'ignore')\n",
    "model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55fb52",
   "metadata": {},
   "source": [
    "### Getting a vector embedding for word\n",
    "\n",
    "The command below can be used to get the word embedding for any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722569ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector embedding for word\n",
    "word = \"hello\"\n",
    "v = model.get_word_vector(word)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24935c82",
   "metadata": {},
   "source": [
    "### Getting the closest words to a given word or vector\n",
    "\n",
    "You may use the get_nearest_neighbors() method to get the 10 closest word in vocabulary to another given word, or a given word vector (assuming this word vector is produced by another model, e.g. an autocompletion AI trying to predict the next word in a given sentence).\n",
    "\n",
    "In order to find nearest neighbors, we need to compute a similarity score between words. Our words are represented by continuous word vectors and we can thus apply simple similarities to them. In particular we use the cosine of the angles between two vectors (to be discussed in W9S3).\n",
    "\n",
    "This similarity is computed for all words in the vocabulary, and the 10 most similar words are shown. Of course, if the word appears in the vocabulary, it will appear on top, with a similarity of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd29640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show closest words and their similarity scores for a given word\n",
    "l = model.get_nearest_neighbors('university')\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show closest words and their similarity scores for a given word vector\n",
    "vec = model.get_word_vector(\"asparagus\")\n",
    "l2 = model.get_nearest_neighbors(vec)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20895e40",
   "metadata": {},
   "source": [
    "### Word analogies\n",
    "\n",
    "You may even use word analogies, e.g.: Following the analogy between Paris and France, what are the top 10 words having the same analogy with Spain? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d93822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with analogies\n",
    "# Following the analogy between Paris and France, which words have the same analogy with Spain?\n",
    "l3 = model.get_analogies(\"paris\", \"france\", \"spain\")\n",
    "print(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b8de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
