{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.b. Implementing our first Shallow Neural Network in Numpy - Back Propagation\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (16/12/2022)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.5.1)\n",
    "- Numpy (tested on v1.22.1)\n",
    "- Sklearn (tested on v0.0.post1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Removing unecessary warnings (optional, just makes notebook outputs more readable)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock dataset generation\n",
    "\n",
    "As in Notebook 7.a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All helper functions\n",
    "min_surf = 40\n",
    "max_surf = 200\n",
    "def surface(min_surf, max_surf):\n",
    "    return round(np.random.uniform(min_surf, max_surf), 2)\n",
    "min_dist = 50\n",
    "max_dist = 1000\n",
    "def distance(min_dist, max_dist):\n",
    "    return round(np.random.uniform(min_dist, max_dist), 2)\n",
    "def price(surface, distance):\n",
    "    return round((100000 + 14373*surface + (1000 - distance)*1286)*(1 + np.random.uniform(-0.1, 0.1)))/1000000\n",
    "n_points = 100\n",
    "def create_dataset(n_points, min_surf, max_surf, min_dist, max_dist):\n",
    "    surfaces_list = np.array([surface(min_surf, max_surf) for _ in range(n_points)])\n",
    "    distances_list = np.array([distance(min_dist, max_dist) for _ in range(n_points)])\n",
    "    inputs = np.array([[s, d] for s, d in zip(surfaces_list, distances_list)])\n",
    "    outputs = np.array([price(s, d) for s, d in zip(surfaces_list, distances_list)]).reshape(n_points, 1)\n",
    "    return surfaces_list, distances_list, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n",
      "(100, 2)\n",
      "(100, 1)\n",
      "[[ 58.16 572.97]\n",
      " [195.92 809.8 ]\n",
      " [156.6  349.04]\n",
      " [ 96.23  86.82]\n",
      " [153.22 817.92]\n",
      " [167.94 806.25]\n",
      " [143.29 315.92]\n",
      " [106.34 482.67]\n",
      " [152.96 427.77]\n",
      " [ 79.46 955.76]]\n",
      "[[1.581913]\n",
      " [3.450274]\n",
      " [2.978769]\n",
      " [2.808258]\n",
      " [2.556398]\n",
      " [3.023983]\n",
      " [3.099523]\n",
      " [2.121069]\n",
      " [3.136544]\n",
      " [1.273443]]\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(47)\n",
    "surfaces_list, distances_list, inputs, outputs = create_dataset(n_points, min_surf, max_surf, min_dist, max_dist)\n",
    "# Check a few entries of the dataset\n",
    "print(surfaces_list.shape)\n",
    "print(distances_list.shape)\n",
    "print(inputs.shape)\n",
    "print(outputs.shape)\n",
    "print(inputs[0:10, :])\n",
    "print(outputs[0:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Neural Network class, with a single hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Notebook 7.a., we will reuse the ShallowNeuralNet class we defined earlier, along with its forward and loss methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNet():\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.W1 = np.random.randn(n_x, n_h)*0.1\n",
    "        self.b1 = np.random.randn(1, n_h)*0.1\n",
    "        self.W2 = np.random.randn(n_h, n_y)*0.1\n",
    "        self.b2 = np.random.randn(1, n_y)*0.1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Z1 = np.matmul(x, self.W1)\n",
    "        Z1_b = Z1 + self.b1\n",
    "        Z2 = np.matmul(Z1_b, self.W2)\n",
    "        Z2_b = Z2 + self.b2\n",
    "        return Z2_b\n",
    "    \n",
    "    def MSE_loss(self, inputs, outputs):\n",
    "        outputs_re = outputs.reshape(-1, 1)\n",
    "        pred = self.forward(inputs)\n",
    "        losses = (pred - outputs_re)**2\n",
    "        loss = np.sum(losses)/outputs.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can try it on our dataset, by using $ n_x = 2 $ and $ n_y = 1 $ to match the dimensionality of our dataset. The hidden layer size $ n_h $ is free for us to choose, and we will arbitrarily fix it to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_x': 2, 'n_h': 4, 'n_y': 1, 'W1': array([[-0.10476816,  0.18570216,  0.03204007, -0.10951262],\n",
      "       [-0.13867874, -0.03539496, -0.02856421,  0.20592501]]), 'b1': array([[ 0.0232776 , -0.16122469,  0.00718537,  0.06663351]]), 'W2': array([[ 0.03321156],\n",
      "       [-0.0336505 ],\n",
      "       [ 0.04977554],\n",
      "       [-0.1794089 ]]), 'b2': array([[0.03460341]])}\n"
     ]
    }
   ],
   "source": [
    "# Define neural network structure\n",
    "n_x = 2\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "shallow_neural_net = ShallowNeuralNet(n_x, n_h, n_y)\n",
    "print(shallow_neural_net.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the neural network is poorly predicting, as the values used in the $ W $ and $ b $ matrices of each layer have been randomly decided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "(100, 1)\n",
      "[[-23.24055489]\n",
      " [-31.54945952]\n",
      " [-12.75105332]\n",
      " [ -2.49026451]\n",
      " [-32.3803654 ]]\n",
      "[[1.581913]\n",
      " [3.450274]\n",
      " [2.978769]\n",
      " [2.808258]\n",
      " [2.556398]]\n"
     ]
    }
   ],
   "source": [
    "pred = shallow_neural_net.forward(inputs)\n",
    "print(pred.shape)\n",
    "print(outputs.shape)\n",
    "print(pred[0:5])\n",
    "print(outputs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also becomes apparent when checking the loss function for this model. As a comparison, our first simple model has a loss of roughly 0.025!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677.625448852107\n"
     ]
    }
   ],
   "source": [
    "loss = shallow_neural_net.MSE_loss(inputs, outputs)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a back propagation mechanism\n",
    "\n",
    "Of course, the poor prediction of the model above has to do with the fact that we could not smartly set the weights like before, and that the random initialization for our shallow neural network was incorrect.\n",
    "\n",
    "Now, of course, we are not going to randomly try our luck and pray to the RNG (Random Number Generation) gods for a lucky initialization on the $ W $ and $ b $ matrices... Instead, we will tell our model how to adjust the said matrices to improve its loss and prediction capabilities.\n",
    "\n",
    "This process is called, **back propagation** and will be implemented in the **backward()** method of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
