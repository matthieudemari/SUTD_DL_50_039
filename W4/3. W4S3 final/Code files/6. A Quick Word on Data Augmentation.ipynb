{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. A Quick Word on Data Augmentation\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (10/02/2023)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Torch (tested on v1.12.1)\n",
    "- Torchvision (tested on v0.13.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize, RandomHorizontalFlip\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "This is the same original MNIST dataset as before, with no data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to convert images to tensors and normalize them\n",
    "transform_data = Compose([ToTensor(),\n",
    "                          Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the data\n",
    "batch_size = 256\n",
    "train_dataset = MNIST(root='./mnist/', train = True, download = True, transform = transform_data)\n",
    "test_dataset = MNIST(root='./mnist/', train = False, download = True, transform = transform_data)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Data augmentation on the MNIST Dataset\n",
    "\n",
    "Data augmentation is a technique used to artificially increase the size of the training dataset by applying various transformations to the existing data. This helps to expose the model to more variations of the data, making it more robust and less prone to overfitting.\n",
    "\n",
    "By applying data augmentation, the model can learn to recognize patterns and features that are supposed to be invariant to certain changes in the data, such as translations, rotations, and scaling. This allows the model to generalize better to new data, which is particularly useful when the amount of available training data is limited.\n",
    "\n",
    "Data augmentation can also be used to balance the distribution of classes in the dataset by artificially increasing the number of examples of the under-represented classes. This can help to improve the performance of the model on these classes.\n",
    "\n",
    "In summary, data augmentation is a powerful technique that can help to improve the generalization ability of a model by increasing the diversity of the training data.\n",
    "\n",
    "Some possible ways to perform data augmentation on this dataset include:\n",
    "- Randomly translating images by a certain number of pixels,\n",
    "- Randomly rotating the images by a small angle,\n",
    "- Randomly scaling the images by a small factor,\n",
    "- Randomly cropping the images,\n",
    "- Randomly flipping the images horizontally,\n",
    "- Adding random noise to the images,\n",
    "- Randomly changing the brightness or contrast of the images,\n",
    "- Etc.\n",
    "\n",
    "These can be accomplished by using pre-built data augmentation transforms such as RandomRotation, RandomAffine, RandomCrop, RandomHorizontalFlip, RandomGrayscale from the torchvision.transforms library or using other libraries like imgaug, albumentations, etc. Feel free to have a look at: https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "Additionally, you can also chain multiple transforms together using the Compose class from the torchvision.transforms library to apply multiple data augmentations at once.\n",
    "\n",
    "For instance, if we want to flip images horizontally, we will simply amend the code below, adding RandomHorizontalFlip() to the list of transforms.\n",
    "\n",
    "**Quick question:** Does it makes sense to perform this data augmentation on all pictures? Can all digits be flipped horizontally to generate new valid images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to convert images to tensors and normalize them\n",
    "transform_data = Compose([RandomHorizontalFlip(), ToTensor(),\n",
    "                          Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the data\n",
    "batch_size = 256\n",
    "train_dataset_aug = MNIST(root = './mnist/', train = True, download = True, transform = transform_data)\n",
    "test_dataset_aug = MNIST(root = './mnist/', train = False, download = True, transform = transform_data)\n",
    "train_loader_aug = torch.utils.data.DataLoader(train_dataset_aug, batch_size = batch_size, shuffle = True)\n",
    "test_loader_aug = torch.utils.data.DataLoader(test_dataset_aug, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "In the next notebook, we will discuss about some important pre-trained models that were state-of-the-art in the past and their underlying techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
