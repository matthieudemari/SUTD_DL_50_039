{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using the Convolution Operation in Neural Networks\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.1 (06/07/2023)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.11.4)\n",
    "- Scipy (tested on v1.9.3)\n",
    "- Torch (tested on v2.0.1+cu118)\n",
    "- Torchvision (tested on v0.15.2+cu118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "As in the notebooks from the previous week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to convert images to tensors and normalize them\n",
    "transform_data = Compose([ToTensor(),\n",
    "                          Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the data\n",
    "batch_size = 256\n",
    "train_dataset = MNIST(root='./mnist/', train = True, download = True, transform = transform_data)\n",
    "test_dataset = MNIST(root='./mnist/', train = False, download = True, transform = transform_data)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Conv2d in our model\n",
    "\n",
    "As we have seen, this convolution operation is key when processing images. We could typically use it to detect edges, sharpen the image so it can be processed better, etc. Eventually, we hope that it will allow the neural network to recognize shapes and features in the images, and eventuall use this information to perform classification on our images.\n",
    "\n",
    "We will therefore build our first Convolutional Neural Network (CNN). This network will have the same logic as the previous DNN from last week, but it will replace the first few fully connected layers with Conv2d operations instead. We are hoping that the neural network will then be able to figure out which kernel values to use, to eventually detect appropriate and useful features for classification, using backpropagation. The final layers (here two of them), will consist of fully connected layers like before, which will eventually produce a vector of 10 values, one for each class.\n",
    "\n",
    "Between the last Conv2d (which produces a 2D image) and the first fully connected vector (which expects a 1D vector), we will implement a **flattening** of the image, basically reshaping the 2D image into a 1D vector, using ```x = x.view(-1, 64*28*28)```.\n",
    "\n",
    "Finally, you will have probably noticed that the number of channels in the ouput images of our convolutions has changed. Our original images only had one channel (greyscale), but the output of the first convolution will have 32 channels, as shown in the Conv2d call ```self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1)```.\n",
    "\n",
    "This typically is a way for us to have 32 convolution operations in parallel, each with their own kernels.\n",
    "\n",
    "Having a larger number of output channels in a Conv2D layer allows for the model to learn more complex and diverse features from the input data. With more channels, the model can learn a wider range of filters, which can in turn detect more nuanced features in the input. This can lead to improved performance on tasks such as image classification or object detection. However, it's important to note that increasing the number of channels also increases the number of parameters in the model, which can make it more computationally expensive and may also lead to overfitting if not properly regularized.\n",
    "\n",
    "To summarize, this will be our CNN.\n",
    "\n",
    "Input $ \\rightarrow $ Conv2D(1, 32, 3, 1, 1) $ \\rightarrow $ Cond2D(32, 34, 3, 1, 1) $ \\rightarrow $ Flatten $ \\rightarrow $ Linear(50176, 128) $ \\rightarrow $ Linear(128, 10) $ \\rightarrow $ Output\n",
    "\n",
    "The full network implementation is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        \n",
    "        # Two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "\n",
    "        # Two fully connected layers\n",
    "        self.fc1 = nn.Linear(64*28*28, 128) # 64*28*28 = 50176\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Display initial shape\n",
    "        print(\"Initial: \", x.shape)\n",
    "        \n",
    "        # Pass input through first convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        print(\"After conv1: \", x.shape)\n",
    "\n",
    "        # Pass output of first conv layer through\n",
    "        # second convolutional layer\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        print(\"After conv2: \", x.shape)\n",
    "\n",
    "        # Flatten output of second conv layer\n",
    "        x = x.view(-1, 64*28*28)\n",
    "        print(\"After flatten: \", x.shape)\n",
    "\n",
    "        # Pass flattened output through first Linear layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        print(\"After FC1: \", x.shape)\n",
    "\n",
    "        # Pass output of first Linear layer to second linear layer\n",
    "        x = self.fc2(x)\n",
    "        print(\"After FC2: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create and use our CNN model, and observe how our input will be processed by the network, having its size eventually changing after each operation. When designing a CNN, it is very important to spend some time checking the sizes of the different outputs produced by each layer, making sure it will connect smoothly with what the next layer expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of MNIST_CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=50176, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = MNIST_CNN()\n",
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial:  torch.Size([256, 1, 28, 28])\n",
      "After conv1:  torch.Size([256, 32, 28, 28])\n",
      "After conv2:  torch.Size([256, 64, 28, 28])\n",
      "After flatten:  torch.Size([256, 50176])\n",
      "After FC1:  torch.Size([256, 128])\n",
      "After FC2:  torch.Size([256, 10])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    out = model(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "We have implemented a simple CNN and a forward method for it. In the next notebook, we will implement a backpropagation and training function for this model and will compare its performance with a full DNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
